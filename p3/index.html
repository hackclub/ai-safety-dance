<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>

    <!-- Title -->
    <title>Part 3: The Proposed Solutions</title>

    <!-- UTF-8 & Mobile -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- Links are external by default -->
    <base target="_blank">

	<!-- Favicon -->
	<link rel="icon" type="image/png" href="favicon.png">

    <!-- Social Share Nonsense -->
	<meta itemprop="name" content="Part 3: The Proposed Solutions">
	<meta itemprop="description" content="Chapter Three of ‚ÄúAI Safety for Fleshy Humans: a whirlwind tour‚Äù">
	<meta itemprop="image" content="https://aisafety.dance/thumbs/thumb-p3.png">
	<meta property="og:title" content="Part 3: The Proposed Solutions">
	<meta property="og:type" content="website">
	<meta property="og:image" content="https://aisafety.dance/thumbs/thumb-p3.png">
	<meta property="og:description" content="Chapter Three of ‚ÄúAI Safety for Fleshy Humans: a whirlwind tour‚Äù">
    <meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Part 3: The Proposed Solutions">
	<meta name="twitter:description" content="Chapter Three of ‚ÄúAI Safety for Fleshy Humans: a whirlwind tour‚Äù">
	<meta name="twitter:image" content="https://aisafety.dance/thumbs/thumb-p3.png">

	<!-- STYLES -->
	<link rel="stylesheet" href="../styles/Merriweather/merriweather.css">
    <link rel="stylesheet" href="../styles/Open_Sans/opensans.css">
    <link rel="stylesheet" href="../styles/littlefoot.css"/> <!-- before page.css, so page can override it -->
	<link rel="stylesheet" href="../styles/page.css">

	<!-- SCRIPTS -->
    <!-- Littlefoot: for my feetnotes -->
    <script src="../scripts/littlefoot.js" ></script>
    <!-- Nutshell: expandable explanations -->
    <script src="../scripts/nutshell-v1.0.5.js"></script>
    <script> Nutshell.setOptions({ startOnLoad: false, /* Start AFTER footnotes loaded */ }); </script>
    <!-- MathJAX: for nice math -->
    
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['\\(', '\\)'], ['$', '$']],
            displayMath: [['\\[', '\\]'], ['$$', '$$']],
            processEscapes: true
        }
    };
    </script>
    
    <script src="../scripts/tex-mml-chtml.js"></script>
	<!-- This website's own scripts -->
    <script src="../scripts/page.js"></script>
    <!-- Hack Club's no-cookies, GDPR-compliant analytics -->
    <script defer data-domain="aisafety.dance" src="https://plausible.io/js/script.js"></script>

</head>
<body>

<!-- HACKBRAND -->
<a class="orpheus-flag" target="_blank" href="https://hackclub.com/">
	<img src="../styles/orpheus-flag.svg" width="560" height="315" alt="A project by Hack Club" aria-label="A project by Hack Club">
</a>

<!-- The Sidebar UI -->
<div id="return_to_content"></div>
<div id="sidebar">
	<div id="panel_toc"></div>

    <!-- STYLE CHANGER -->
	<div id="panel_style">

        <div id="style_dark_mode_container" style="cursor:pointer;">
            <input type="checkbox" id="style_dark_mode" style="pointer-events: none;">
            Dark Mode
        </div>
        <br>

        Font size:
        <span id="style_fontsize"></span>
        <br>
        <input type="range" id="style_fontsize_slider" min="10" value="19" max="40">
        <br>

        Font type:
        <br>
        <label>
            <input type="radio" name="style_font_family" value="serif" checked>
            <span style="font-family:'Merriweather'">Serif</span>
        </label>
        <br>
        <label>
            <input type="radio" name="style_font_family" value="sans_serif">
            <span style="font-family:'Open Sans'">Sans Serif</span>
        </label>
        <br><br>

        <button id="style_reset">Reset</button>

    </div>

    <!-- TRANSLATIONS -->
	<div id="panel_translations">
        <!-- none... sorry -->
    </div>
	<div id="panel_share">share on... w/e</div>
    <!-- SHILLING FOR BIG NICKY -->
	<div id="panel_sub">
    </div>
    <div id="panel_support"></div>

</div>

<!-- Reading Time Clock! -->
<div id="reading_time">
	<div id="clock_icon"></div>
	<div id="clock_label"></div>
</div>

<!-- EVERYTHING TO THE LEFT of the sidebar... -->
<div id="everything_container">

    <!-- A big cute header -->
    <div id="header" class="chapter">
        <div id="splash_image">

            
            
            
            <img id="p3-banner" src="../media/p3/P3Cat.png"/>
            

            <div id="crt_lines"></div>
            <div id="static"></div>

            

        </div>
        

        <div id="header_words">
            <div id="title">
                P<span style="position: relative;left: -7px;">A</span>RT
                THREE
            </div>
            <div id="subtitle">
                The Proposed Solutions
            </div>
        </div>

        
	</div>

    <!-- Chapter Navigation -->
    <div id="chapter_nav">
        <div id="chapter_nav_centered">
            <a target="_self" href="../"
                class="live">
                <div >
                    <span class='chapter-nav-desktop'>
                        introduction
                    </span>
                    <span class='chapter-nav-phone'>
                        intro
                    </span>
                </div>
            </a>
            <a target="_self" href="../p1"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        part 1<br>past & future
                    </span>
                    <span class='chapter-nav-phone'>
                        part 1
                    </span>
                </div>
            </a>
            <a target="_self" href="../p2"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        part 2<br>problems
                    </span>
                    <span class='chapter-nav-phone'>
                        part 2
                    </span>
                </div>
            </a>
            <a target="_self" href="../p3"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        part 3<br>solutions?
                    </span>
                    <span class='chapter-nav-phone'>
                        part 3
                    </span>
                </div>
            </a>
            <a target="_self" href="../conclusion"
                class="live">
                <div style="border-right:1px solid rgba(128,128,128,0.8);">
                    <span class='chapter-nav-desktop'>
                        conclusion
                    </span>
                    <span class='chapter-nav-phone'>
                        outro
                    </span>
                </div>
            </a>
        </div>
    </div>

    <!-- The lil' tabs for sidebar UI -->
    <div id="sidebar_tabs">
		<div id="tab_toc">
			<div></div>
			table of contents
		</div>
		<div id="tab_style">
			<div></div>
			change style üòé
		</div>
        <!--
		<div id="tab_sub">
            CREDITS & Signup for notifications
			<div></div>
			subscribe üíñ
		</div>
        -->
	</div>

    <!-- BEHOLD! CONTENT!!!!! -->
	<article id="content">
<div style="text-align:center"><i>(This is Part 3 of a series on AI Safety! You don't</i> have <i>to read the previous parts ‚Äî <a href="https://aisafety.dance/">Intro</a>, <a href="https://aisafety.dance/p1">Part 1</a>, <a href="https://aisafety.dance/p2">Part 2</a> ‚Äî but they'll help!)</i></div>
<p>So, after writing 40,000+ words on how weird &amp; difficult AI Safety is... how am I feeling about the chances of humanity solving this problem?</p>
<p>...pretty optimistic, actually!</p>
<p>No, really!</p>
<p>Maybe it's just cope. But in my opinion, if <em>this</em> is the space of all the problems:</p>
<p><img src="../media/p3/intro/problems.png" alt="Drawing of a faintly outlined blob, labelled &quot;the whole problem space&quot;"></p>
<p>Then: although no <em>one</em> solution covers the whole space, <em>the entire problem space</em> is covered by one (or more) promising solutions:</p>
<p><img src="../media/p3/intro/solutions.png" alt="Same outline, but entirely overlapped by small colorful circles, each one representing a different solution"></p>
<p><strong>We don't need One Perfect Solution; we can stack several imperfect solutions!</strong> This is similar to the <a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">Swiss Cheese Model</a> in Risk Analysis ‚Äî each layer of defense has holes, but if you have enough layers with holes in <em>different</em> places, risks can't go all the way through:</p>
<p><img src="../media/p3/intro/cheese.png" alt="Rays going through layers of swiss cheese, which have holes in them. The rays can easily go through the holes of one layer of cheese, but not all of them."></p>
<p>( <a href="#SwissCheese">: üßÄ Bonus section: counter-arguments &amp; counter-counter-arguments on the Swiss Cheese Model</a> ‚Üê <em>optional: whenever you see a dotted-underlined section, you can click to expand it!</em> )</p>
<p><em>This does not mean AI Safety is 100% solved yet</em> ‚Äî we still need to triple-check these proposals, and get engineers/policymakers to even <em>know</em> about these solutions, let alone implement them. But for now, I'd say: &quot;lots of work to do, but lots of promising starts&quot;!</p>
<p>As a reminder, here's how we can break down the main problems in AI &amp; AI Safety:</p>
<p><img src="../media/p3/intro/breakdown.png" alt="Breakdown: &quot;How can we align AI to humane values?&quot; breaks down into Problems in the AI (technical alignment, game theory, deep learning) and Problems in the Humans (whose values?, coordinating around ai)"></p>
<p>So in this Part 3, we'll learn about the most-promising solution(s) for each part of the problem, while being honest about their pros, cons, and unknowns:</p>
<p><strong>ü§ñ Problems in the AI:</strong></p>
<ul>
<li><u>Scalable Oversight</u>: How can we safely check AIs, even when they're <em>far</em> more advanced than us? <a href="#oversight">‚Ü™</a></li>
<li><u>Solving AI Logic</u>: AI should aim for our &quot;future lives&quot; <a href="#future">‚Ü™</a>, and learn our values with uncertainty <a href="#uncertain">‚Ü™</a>.</li>
<li><u>Solving AI &quot;Intuition&quot;</u>: AI should be easy to &quot;read &amp; write&quot; <a href="#interpretable">‚Ü™</a>, be robust <a href="#robust">‚Ü™</a>, and think in cause-and-effect. <a href="#causality">‚Ü™</a></li>
</ul>
<p><strong>üò¨ Problems in the Humans</strong>:</p>
<ul>
<li><u>Humane Values</u>: Which values, <em>whose</em> values, should we put into AI, and how? <a href="#humane">‚Ü™</a></li>
<li><u>AI Governance</u>: How can we coordinate humans to manage AI, from the top-down and/or bottom-up? <a href="#governance">‚Ü™</a></li>
</ul>
<p><strong>üåÄ Working <em>around</em> the problems</strong>:</p>
<ul>
<li><u>Alternatives to AGI</u>: How about we just don't make the Torment Nexus? <a href="#alt">‚Ü™</a></li>
<li><u>Cyborgism</u>: If you can't beat 'em, join 'em! <a href="#cyborg">‚Ü™</a></li>
</ul>
<p>( If you'd like to skip around, the <img src="../media/intro/icon1.png" class="inline-icon"> Table of Contents are to your right! üëâ You can also <img src="../media/intro/icon2.png?v=3" class="inline-icon"> change this page's style, and <img src="../media/intro/icon3.png?v=2" class="inline-icon"> see how much reading is left. )</p></p>
<p>Quick aside: <strong>this final Part 3, published on December 2025,</strong> was supposed to be posted 12 months ago. But due to a bunch of personal shenanigans I don't want to get into, I was delayed. Sorry you've waited a year for this finale! On the upside, there's been lots of progress &amp; research in this field since then, so I'm excited to share all that with you, too.</p>
<p>Alright, let's dive in! No need for more introduction, or weird stories about cowboy catboys, let's just‚Äî</p>
<h4>:x Swiss Cheese</h4>
<p><a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">The famous Swiss Cheese Model</a> from Risk Analysis tells us: <strong>you don't need <em>one</em> perfect solution, you can stack <em>several</em> imperfect solutions.</strong></p>
<p>This model's been used <em>everywhere</em>, from aviation to cybersecurity to pandemic resilience. An imperfect solution has &quot;holes&quot;, which are easy to get through. But stack enough of them, with holes in different places, and it'll be near-impossible to bypass.</p>
<p>But, let's address two critiques of the Swiss Cheese model:</p>
<p><a href="https://www.eurocontrol.int/publication/revisiting-swiss-cheese-model-accidents">Critique One</a> ‚Äî this assumes the &quot;holes&quot; in the solutions are <em>independent</em>. If there's any <em>one</em> hole that all solutions share, then a problem can slip through all of them. Fair enough. <strong>This is why the Proposed Solutions on this page try to be as diverse as possible.</strong> (See below section: Robustness &gt; Diversity)</p>
<p>Critique Two ‚Äî more relevant to AI Safety ‚Äî is it assumes the problem is not an intelligent agent. <a href="https://archive.is/20250921161137/https://www.vox.com/future-perfect/461680/if-anyone-builds-it-yudkowsky-soares-ai-risk">A quote from Nate Soares</a>:</p>
<blockquote>
<p>‚ÄúIf you ever make something that is trying to get to the stuff on the other side of all your Swiss cheese, it‚Äôs not that hard for it to just route through the holes.‚Äù</p>
</blockquote>
<p>I accept this counterargument when it comes to defending against an <em>already</em>-misaligned superintelligence. But if we're talking about <em>growing a trusted intelligence from scratch</em>, then the Swiss Cheese Model still makes sense at each step, <em>and</em>, you can use each iteration of a trusted AI as an extra &quot;layer of cheese&quot; for training better iterations of that AI. (See below section: Scalable Oversight)</p>
<p>As AI researcher Jan Leike <a href="https://aligned.substack.com/p/should-we-control-ai">puts it</a>:</p>
<blockquote>
<p>More generally, we should actually solve alignment instead of just trying to control misaligned AI. [...] Don‚Äôt try to imprison a monster, build something that you can actually trust!</p>
</blockquote>
<hr>
<p><a id="oversight"></a></p>
<h2>Scalable Oversight</h2>
<p>This is Sheriff Meowdy, the cowboy catboy:</p>
<p><img src="../media/p3/so/meowdy0001.png" alt="Drawing of Sheriff Meowdy"></p>
<p>One day, the Varmin strode into town:</p>
<p><img src="../media/p3/so/meowdy0002.png" alt="Sheriff Meowdy staring down a bunch of Jerma rats strolling towards him"></p>
<p>Sharpshootin' as the Sheriff was, he's man enough (catman enough) to admit when he needs backup. So, he makes a robot helper ‚Äî Meowdy 2.0 ‚Äî to help fend off the Varmin:</p>
<p><img src="../media/p3/so/meowdy0003.png" alt="Robot version of Sheriff Meowdy, labelled &quot;Meowdy 2.0&quot;"></p>
<p>Meowdy 2.0 can shoot twice as fast as the Sheriff, but there's a catch: Meowdy 2.0 <em>might</em> betray the Sheriff. Thankfully, it takes time to turn around &amp; betray the Sheriff, and the Sheriff is still fast enough to stop Meowdy 2.0 if it does that.</p>
<p>This is <strong>oversight.</strong></p>
<p><img src="../media/p3/so/meowdy0004.png" alt="Sheriff Meowdy watching 2.0, with a gun to its head. 2.0 can turn around in 500ms, Meowdy can react &amp; shoot in 200ms."></p>
<p>Alas, even Meowdy 2.0 <em>still</em> ain't fast enough to stop the millions of Varmin. So Sheriff makes Meowdy 3.0, which is twice as fast as 2.0, or <em>four times</em> as fast as the Sheriff.</p>
<p>This time, the Sheriff has a harder time overseeing it:</p>
<p><img src="../media/p3/so/meowdy0005.png" alt="3.0 can turn around in 250ms, Meowdy can only still react in 200ms. Meowdy is sweating."></p>
<p>But Meowdy 3.0 <em>still</em> ain't fast enough. So the Sheriff makes Meowdy 4.0, who's twice as fast as 3.0...</p>
<p>...and this time, it's so fast, the Sheriff can't react if 4.0 betrays him:</p>
<p><img src="../media/p3/so/meowdy0006.png" alt="4.0 can turn around in 125ms, which is fast enough to betray Meowdy. 4.0 shoots Meowdy dead."></p>
<p>So, what to do? The Sheriff strains all two of his orange-cat brain cells, and comes up with a plan: <strong><em>scalable</em> oversight!</strong></p>
<p>He'll oversee 2.0, which can oversee 3.0, which <em>can</em> oversee 4.0!</p>
<p><img src="../media/p3/so/meowdy0007.png" alt="Meowdy oversees 2.0 oversees 3.0 oversees 4.0"></p>
<p>In fact, why stop there? This harebrained &quot;scalable oversight&quot; scheme of his will let him oversee a Meowdy of <em>any</em> speed!</p>
<p>So, the Sheriff makes 20 Meowdy's. Meowdy 20.0 is 2<sup>20</sup> ~= one <em>million</em> times faster than the Sheriff: plenty quick enough to stop the millions of Varmin!</p>
<p><img src="../media/p3/so/meowdy0008.png" alt="Meowdy oversees a chain of Meowdy's, up to Meowdy 20.0, who can shoot all the Varmin dead."></p>
<p>Wait, isn't a single chain of oversight fragile? As in, if <em>one</em> of the Meowdy's break, the entire chain is broken? Yes! One solution is to have <em>multiple</em> interwined chains, like so:</p>
<p><img src="../media/p3/so/meowdy_robust.png" alt="Parallel braided chains of overseers. The chance of failure shrinks exponentially, as you add more chains.">
(üëâ <a href="#RobustChainMath">: optional - click to see the math for the above diagram</a>)</p>
<p>This way, if any <em>one</em> overseer at Level N gets corrupted, there'll still be two others checking the bots at Level N+1. And the overseers at Level N-1 can catch &amp; fix the corrupted overseer. Note: <em>it's important the overseers are as independent as possible</em>, so their failures have a low correlation. At the moment, AI failures are <em>very</em> correlated. We'll learn more, in a later section, about how to create Robustness in AI.</p>
<p>Anyway, in sum, the core insight of scalable oversight is this meme:</p>
<p><img src="../media/p3/so/meme.png" alt="The &quot;domino&quot; meme, where a small domino knocks over a bigger one, which knocks over a bigger one, until it can knock over a huge domino."></p>
<p>(Another analogy: sometimes, boats are so big, the rudder <em>has its own smaller rudder</em>, called a <a href="https://en.wikipedia.org/wiki/Trim_tab#As_a_metaphor">trim tab</a>. This way, you can steer the small rudder, which steers the big rudder, which steers the whole boat.)</p>
<p>You may notice this is similar to the idea of &quot;recursive self-improvement&quot; for AI <em>Capabilities:</em> an advanced AI makes a <em>slightly more</em> advanced AI, which makes another more advanced AI, etc. Scalable Oversight is the same idea, but for AI <em>Safety:</em> one AI helps you align a slightly more advanced AI, etc!</p>
<p>(Ideas like these, where case number N helps you solve case number N+1, etc, are called &quot;inductive&quot; or &quot;iterative&quot; or &quot;recursive&quot;. Don't worry, you don't need to remember that jargon, just thought I'd mention it.)</p>
<p>Anywho: with the power of friendship, math, and a bad Wild West accent...</p>
<p>... the mighty Sheriff Meowdy has saved the townsfolk, once more!</p>
<p><img src="../media/p3/so/meowdy0010.png" alt="Sheriff Meowdy blowing the smoke away from his gun, as the injured Varmin waltz off into the sunset"></p>
<p>(üëâ <a href="#ScalableOversightExtras">: click to expand bonus section - the &quot;alignment tax&quot;, P = NP?, alignment vs control, what about sudden-jump &quot;sharp left turns&quot;?</a>)</p>
<hr>
<p>Now that the visual hook is over, here's a quick Who Did This intermission!</p>
<p><em>AI Safety for Fleshy Humans</em> was created by <strong>Nicky Case</strong>, in collaboration with <strong>Hack Club</strong>, with some extra funding by <strong>Long Term Future Fund</strong>!</p>
<p>üò∏ <a href="https://ncase.me"><strong>Nicky Case</strong></a> (me, writing about myself in the third person) just wants to watch the world learn. If you wanna see future explainers on AI Safety (or other topics) by me, sign up for <a href="https://www.youtube.com/@itsnickycase">my YouTube channel</a> or <a href="https://ncase.me/sub/">my once-a-month newsletter</a>: üëá</p>
<iframe src="https://ncase.me/ncase-credits/signup2.html" frameborder="no" width="640" height="250"></iframe>
<p>ü¶ï <a href="https://hackclub.com/"><strong>Hack Club</strong></a> helps teen hackers around the world learn from each other, and pssst, they're hosting two <a href="https://midnight.hackclub.com/">cool</a> <a href="https://midnight.hackclub.com/">hackathons</a> soon! You can sign up below to learn more, and get free stickers: üëá</p>
<p><div style="width:100%;height:500px;" data-fillout-id="cRCnJk2JsJus" data-fillout-embed-type="standard" data-fillout-inherit-parameters data-fillout-dynamic-resize data-fillout-domain="forms.hackclub.com"></div>
<script defer src="https://server.fillout.com/embed/v1/"></script>
</p>
<p>üîÆ <a href="https://funds.effectivealtruism.org/funds/far-future"><strong>Long Term Future Fund</strong></a> throws money at stuff that helps reduce $\text{ProbabilityOf( Doom )}$. Like, you know, advanced AI, bioweapons, nuclear war, and other fun lighthearted topics.</p>
<p>Alright, back to the show!</p>
<hr>
<p>A behind-the-scenes note: the above Sheriff Meowdy comic was the <em>first</em> thing in this series that I drew... <a href="https://www.patreon.com/posts/sheriff-meowdy-78016588">almost three years ago</a>. (Kids, don't do longform content on the internet, it ain't worth it.) Point is: learning about Scalable Oversight was <em>the one idea</em> that made me the most optimistic about AI Safety, and inspired me to start this whole series in the first place!</p>
<p>Because, Scalable Oversight turns <em>this</em> seemingly-impossible problem:</p>
<blockquote>
<p><em>&quot;How do you avoid getting tricked by something that's 100 times smarter than you?&quot;</em></p>
</blockquote>
<p>...into this much-more feasible problem:</p>
<blockquote>
<p><em>&quot;How do you avoid getting tricked by something that's only 10% smarter than you, and ALSO you can raise it from birth, read its mind, and nudge its brain?&quot;</em></p>
</blockquote>
<p>To be clear, &quot;oversee an AI that's only 10% smarter than you&quot; <em>still</em> isn't solved yet, either. But it's much more encouraging! It's like the difference between jumping over a giant barrier in one step, vs going over that <em>same</em> barrier, but with a staircase where each step is doable:</p>
<p><img src="../media/p3/so/sum.png" alt="Scalable Oversight, visual summary. No scalable oversight is like trying to jump over a giant barrier; Scalable Oversight is like having a staircase to go over that barrier, one doable step at a time."></p>
<p>Anyway, that's the <em>general</em> idea. Here's some <em>specific</em> implementations &amp; empirical findings:</p>
<ul>
<li>üíñ‚û°Ô∏èüíñ <strong><a href="https://arxiv.org/abs/1811.07871">Recursive Reward Modeling</a></strong> uses a Level-N bot not just to check &amp; control a Level-(N+1) bot <em>in hindsight</em>, but to <em>train its intrinsic &quot;goals &amp; desires&quot; in the first place.</em><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> As AI researcher Jan Leike <a href="https://aligned.substack.com/p/should-we-control-ai">put it</a>: <em>‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù</em></li>
<li>üìà <strong><a href="https://arxiv.org/abs/2504.18530">Scaling Laws For Scalable Oversight</a></strong> <em>quantitatively</em> measures how many oversight levels/steps you need to maximize oversight success. Encouragingly, like the previous &quot;scaling laws&quot; for how AI improves as you feed them more compute, these &quot;scaling laws of scalable oversight&quot; also seem to be predictable &amp; measurable. <a href="https://mikhailterekhov.github.io/control-tax/">Terekhov &amp; Liu et al 2025</a> goes further and calculates the &quot;control tax&quot; in \$, for various AI attackers and defenders.</li>
<li>üïµÔ∏èü¶π <strong><a href="https://arxiv.org/abs/2312.06942">GPT-3.5 can catch sneaky, subtly-harmful code, written by the more-powerful GPT-4.
</a></strong></li>
<li>üïµÔ∏èü•∑ <strong><a href="https://openai.com/index/prover-verifier-games-improve-legibility/">Prover-Verifier Games</a></strong> pits a weak &quot;math proof verifier&quot; AI against two stronger AIs: a &quot;helpful prover&quot; that gives true proofs, and a &quot;sneaky prover&quot; that gives false proofs that <em>seem</em> true. Result of training on this game: the <em>weak verifier</em> can get good enough to distinguish true proofs, but the <em>strong sneaky prover can't win in the long run.</em> Nice!</li>
<li>ü™ú <strong><a href="https://openai.com/index/weak-to-strong-generalization/">Weak-to-Strong Generalization</a></strong> found that GPT-<em>2</em> did a pretty okay job at supervising &amp; training GPT-<em>4</em>. (an AI that's several orders of magnitude bigger)</li>
<li>üí¨üí¨ <strong><a href="https://openai.com/index/debate/">Debate</a></strong> is slightly different. Instead of a weaker AI overseeing a stronger AI, Debate has 2 equally-powerful AIs debate each other, picking apart each others' logic. As long as truth can survive scrutiny better than falsehood, the truth will win. (Well, maybe.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>)</li>
<li>‚òïÔ∏è <strong><a href="https://arxiv.org/pdf/2402.00530">Superfiltering</a></strong> uses a small, open-source AI to filter the data that's used to train a larger AI! This can help make sure the larger AI is not only trained on high-quality writing (vs internet s#!&amp;posts), but also doesn't learn how to jailbreak itself or its overseer, learn the answers to cheat on evalulation benchmarks, or learn risky capabilties like bomb &amp; bioweapons manufacturing.
<ul>
<li>(not <em>oversight</em>, technically, but it's still an iterative, scalable approach!)</li>
</ul>
</li>
<li>üëØ‚Äç‚ôÄÔ∏è <strong><a href="https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate">Iterated Distillation &amp; Amplification (IDA)</a></strong> is even safer ‚Äì instead of overseeing AIs slightly-smarter than you, you oversee AIs that are <em>strictly weaker</em> than you. How, then, can this setup give you superhuman intelligence? To very loosely summarize: in IDA, you become the CEO of an organization made out of AIs trained to imitate you! Since every AI imitates you, and you're the CEO of the whole thing, the system as a whole is aligned to your values ‚Äî while <em>also</em> amplifying your capabilities.
<ul>
<li>(<a href="#IDA">: More about IDA, and critiques of it</a>)</li>
</ul>
</li>
</ul>
<p>Though, it's good to remain healthily critical. Here's some recent &quot;downer&quot; findings from monitoring Large Language Models (LLMs) like ChatGPT &amp; Claude: if you train an LLM <em>too</em> hard with a monitor that can read its chain-of-thought, it learns to hide its evil plans <em>even in its own &quot;thoughts&quot;</em>. <a href="https://arxiv.org/pdf/2503.11926">(Baker &amp; Huizinga 2025)</a>. And, given na√Øve oversight techniques, it's actually pretty easy for an LLM to jailbreak its monitor, since LLMs have been out long enough, that info about jailbreaks are <em>in</em> their own training data. <a href="https://arxiv.org/pdf/2510.09462">(Terekhov, Panfilov &amp; Dzenhaliou 2025)</a></p>
<p>But even if <em>those</em> oversight methods fail, there's still plenty more! (As we'll see later in the Interpretability &amp; Steering section). Overall, I'm still optimistic. Again: we don't need <em>one</em> perfect solution, we can stack <em>lots</em> of imperfect solutions.</p>
<p>So: <em>IF</em> we can align a slightly-smarter-than-us AI, <em>THEN</em>, through Scalable Oversight, we can align far more advanced AIs.</p>
<p>...but right now, we can't even align <em>dumber</em>-than-us AIs.</p>
<p>That's what the next few proposed solutions aim to fix!  But first...</p>
<h4>:x Robust Chain Math</h4>
<p>First, we're making the assumption that overseer failure is the same across levels <em>and fully independent from each other</em>. However, as long as the failure's aren't <em>100%</em> correlated, you can modify the below math and the spirit of this argument still works.</p>
<p>Anyway: let's say we have $k$ overseers per level, and we have $N$ levels. That is, the chain is $N$ links long and $k$ links wide. Let's say the probability of any overseer failing is $p$, and they're all independent/uncorrelated.</p>
<p>The chain fails if <em>ANY</em> of the Levels fails. But! A Level fails only if <em>ALL</em> the parallel overseers fails.</p>
<p>The chance that <em>ALL</em> parallel overseers in a Level fails is $p^k$.</p>
<p>For convenience, let's call $q$ the chance a Level does <em>not</em> fail. $q = 1 - p^k$</p>
<p>The chance that <em>ANY</em> Level fails, is 1 minus the chance that NONE of the Levels fail. The chance that NONE of the $N$ levels fail, is $q^N$. So, the chance that <em>ANY</em> Level fails is $1 - q^N$. Substituting in $q = 1 - p^k$, that means the chance that no level fails, and our scalable oversight scheme worked, is $1 - (1-p^k)^N$. We have our formula! üéâ</p>
<p>Now, plugging in the values for the above example, where the chance of failure $p = 0.05$, and the chain is $N = 20$ layers deep, then the chance of failure is...</p>
<ul>
<li>For $k=1$ chain: $1 - (1-0.05^1)^{20} \approx 0.64 \approx 64%$</li>
<li>For $k = 2$ braided chains: $1 - (1-0.05^2)^{20} \approx 0.049 \approx 5%$</li>
<li>For $k = 3$ braided chains: $1 - (1-0.05^3)^{20} \approx 0.0025 \approx 0.2%$</li>
<li>For $k = 4$ braided chains: $1 - (1-0.05^4)^{20} \approx 0.00012 \approx 0.01%$</li>
<li>For $k = 5$ braided chains: $1 - (1-0.05^2)^{20} \approx 0.0000062 \approx 0.0006%$</li>
</ul>
<p>Note how the chance of failure decays <em>exponentially</em> for each extra backup chain you add! That's a mighty efficient &quot;alignment tax&quot;.</p>
<h4>:x Scalable Oversight Extras</h4>
<p>Some extra concepts &amp; comments that don't fit in the main post:</p>
<p><strong>&quot;Alignment Tax&quot;:</strong> how much extra overhead do you need to make sure the whole system remained aligned &amp; controllable? If the &quot;tax&quot; is too high, firms will be incentivized to cut corners, and risk a misaligned AI. So: how big is the &quot;tax&quot;?</p>
<p>In the above Meowdy example, to oversee a bot of speed $X$, because each bot can oversee a bot 2x faster than it, you need an extra $log_2(X)$ bots to safely control it. (Then, if you want $k$ intertwined parallel chains for robustness, add $k*log_2(X)$ bots.)</p>
<p>And in general, as the <a href="https://arxiv.org/abs/2504.18530">Scaling Laws for Scalable Oversight paper</a> shows, for any given task, bots can usually oversee a bot that's some fixed ratio &quot;better&quot; than it. If that's the case, then to oversee a bot of capability $X$, you need some logarithmic number of extra bots for scalable oversight.</p>
<p>Logarithms are the inverse of exponentials; so like how exponentials grow <em>really fast</em>, logarithms grow <em>really slow</em>. This is encouraging: this means our &quot;alignment tax&quot; overhead for powerful AIs grows slowly &amp; can be easily affordable!</p>
<p><strong>What if P = NP?</strong></p>
<p>The Scalable Oversight paradigm assumes that it's always easier to <em>verify</em> solutions, than to <em>create</em> solutions: this is how a weaker AI can verify the safety/correctness of the actions of a stronger AI. For example: it's hard to solve a Rubik's cube or Sudoku puzzle, but nearly trival to <em>check</em> if one is solved.</p>
<p>However: there's a currently open question in computer science, with a million-dollar prize attached: P = NP? In sum, it asks: are all problems that are <em>easy to check</em> secretly <em>easy to solve?</em> It seems intuitively not (and most computer scientists believe it's false, that is, P ‚â† NP) but it's still not been proven. As far as we know, it <em>could</em> be the case that P = NP, and so that every problem that's easy to check is also easy to solve.</p>
<p>Does this mean, if P=NP, the Scalable Oversight paradigm fails? No! Because P = NP &quot;only&quot; means that it's not <em>exponentially</em> harder to find solutions than check solutions. (or, to be precise: it's only &quot;polynomially&quot; harder at best, that's the &quot;P&quot; in &quot;P&quot; and &quot;NP&quot;.) But finding a solution <em>is still harder</em>, just not exponentially so.</p>
<p>Two examples, where we've <em>proven</em> how much time an optimal solution takes ‚§µ (note: $\mathcal{O}(\text{formula})$ means &quot;in the long run, the time this process takes is proportional to this formula.&quot;)</p>
<ul>
<li>The optimal way to sort a list takes $\mathcal{O}(n\log{}n)$ time, while checking a list is sorted takes $\mathcal{O}(n)$ time.</li>
<li>The optimal way <a href="https://en.wikipedia.org/wiki/Grover%27s_algorithm">to find a solution to a black-box problem on a quantum computer</a> takes $\mathcal{O}(\sqrt{n})$ time, but checking that solution takes a constant $\mathcal{O}(1)$ time.</li>
</ul>
<p>So even if P = NP, as long as it's <em>harder</em> to find solutions than check them, Scalable Oversight can work. (But the alignment tax will be higher)</p>
<p><strong>Alignment vs Control:</strong></p>
<p>Aligned = the AI's &quot;goals&quot; are the same as ours.</p>
<p>Control = we can, well, control the AI. We can adjust it &amp; steer it.</p>
<p>Some of the below papers are from the sub-field of &quot;AI Control&quot;: how do we control an AI <em>even if it's misaligned?</em> (As shown in the Sheriff Meowdy example, the Meowdy bots will shoot him the moment they can't be controlled. So, they're misaligned.)</p>
<p>To be clear, folks in the AI Control crowd recognize it's not the &quot;ideal&quot; solution ‚Äî as AI researcher Jan Leike <a href="https://aligned.substack.com/p/should-we-control-ai">put it</a>, <em>‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù</em> ‚Äî but it's still worth it as an extra layer of security.</p>
<p>Interestingly, it's also possible to have Alignment <em>without</em> Control: you could imagine an AI that <em>correctly</em> learns humane values &amp; what flourishing for all sentient beings looks like, then takes over the world as a benevolent dictator. It understands that we'll be uncomfortable ceding control, but it's worth it for world peace, and will rule kindly. (And besides, 90% of you keep fantasizing about living in land of kings &amp; queens anyway, admit it, you humans <em>want</em> to be ruled by a dictator. /half-joke)</p>
<p><strong>Sharp Left Turns:</strong></p>
<p>Scalable Oversight also depends on capabilities smoothly scaling up. And not something like, &quot;if you make this AI 1% smarter, it'll gain a brand new capability that lets it absolutely crush an AI that's even 1% weaker than it.&quot;</p>
<p>This possibility sounds absurd, but there's precedent for sudden-jump &quot;phase transitions&quot; in physics: slightly below 0¬∞C, water becomes ice. And slightly above 0¬∞C, water is liquid. So could there be such a &quot;phase transition&quot;, a &quot;sharp left turn&quot;, in intelligent systems?</p>
<p>Maybe? But:</p>
<ol>
<li>
<p>Even in the physics example, ice doesn't freeze <em>instantly;</em> you can feel it getting colder, and you have hours or days to react before it fully freezes over. So, even if a &quot;1% smarter AI&quot; gains a radically new capability, the &quot;1% dumber overseer&quot; may still have time to notice &amp; stop it.</p>
</li>
<li>
<p>As you'll see later in this section, the <em>is</em> a Scalable Oversight proposal, called Iterated Distillation &amp; Amplification, where overseers oversee only <em>strictly &quot;dumber&quot;</em> AIs, yet the system as a whole can still be smarter! Read on for details.</p>
</li>
</ol>
<h4>:x IDA</h4>
<p>To understand Iterated Distillation &amp; Amplification (IDA), let's consider its biggest success story: <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a>, the first AI to beat a world champion at Go.</p>
<p>Here were the steps to train AlphaGo:</p>
<ul>
<li>Start with a dumb, random-playing Go AI.</li>
<li><strong>DISTILL:</strong> Have two copies play against each other. Through self-play, <em>learn an &quot;intuition&quot; for good/bad moves &amp; good/bad board states.</em> (using an <a href="https://en.wikipedia.org/wiki/Neural_network_(machine_learning)">artificial neural network</a>)</li>
<li><strong>AMPLIFY:</strong> Plug this &quot;intuition module&quot; into a Good Ol' Fashioned AI, that simply thinks a few moves &amp; counter-moves ahead, then picks the next best move. (<a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a>) This gives you a slightly-less-dumb Go AI.</li>
<li><strong>ITERATE:</strong> Repeat. The two less-dumb AIs play against each other, learn a better &quot;intuition&quot;, thus get better at game tree search, and thus get better at playing Go.</li>
<li>Repeat over and over  until your AI is superhuman at Go!</li>
</ul>
<p><img src="../media/p3/so/ida_1.png" alt="Diagram of how IDA is applied to Go. Graph: Capability vs Steps. At each distillation step, Capability goes down, but with an amplification step, it goes up more. So that, after several repeated, Capability zig-zags its way up"></p>
<p>Even more impressive, this same system could also learn to be superhuman at chess &amp; shogi (&quot;Japanese chess&quot;), <em>without ever learning from endgames or openings</em>. Just lots and lots of self-play.</p>
<p>( A caveat: the resulting AIs are only as robust as ANNs are, which aren't very robust. A superhuman Go AI can be beaten by a &quot;bad&quot; player who simply tries to take the AI into insane board positions that would never naturally happen, in order to break the AI. (<a href="https://proceedings.mlr.press/v202/wang23g/wang23g.pdf">Wang &amp; Gleave et al 2023</a>) )</p>
<p>Still, this is strong evidence that IDA works. But even better, as <a href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Paul Christiano pointed out &amp; proposed</a>, IDA could be used for scalable Alignment.</p>
<p>Here's a paraphrase of how it'd work:</p>
<ul>
<li>Start with you, the Human</li>
<li><strong>DISTILL:</strong> Train an AI to imitate <em>you</em>, your values, trade-offs, and reasoning style. This AI is <em>strictly weaker</em> than you, but can be run faster.</li>
<li><strong>AMPLIFY:</strong> You want to solve a big problem? Carve up the problem into smaller parts, hand them off to your Slightly-Dumber-But-Much-Faster AI clones, then recombine them into a full solution. (For example: I want to solve a math problem. I come up with <em>N</em> different approaches, then ask <em>N</em> clones to try out each one, then report what they learn. I read their reports, then if it's still not solved, I think up of <em>N</em> more possible approaches &amp; ask the clones to try again. Repeat until solved.)</li>
<li><strong>ITERATE:</strong> For the next distillation step, train an AI to imitate <em>the you + clones system as a whole</em>. Then for the next amplification step, you can query multiple clones of <em>that</em> system to help you break down &amp; solve big problems.</li>
<li>Repeat until you are the CEO of a superhuman &quot;company of you-clones&quot;!</li>
</ul>
<p><img src="../media/p3/so/ida_2.png" alt="Same diagram as before, but IDA applied to amplifying a Human's capabilities, with repeated distillation &amp; amplifcation."></p>
<p>I think IDA is one of the cooler &amp; more promising proposals, but it's worth mentioning a few critiques / unknowns:</p>
<ul>
<li>Distillation: As shown in the above AlphaGo example, IDA's quality is limited by the Distillation step. Right now we don't know how to make robust ANNs, and we don't know if this Distillation step would preserve your values enough.</li>
<li>Amplification: While it seems most big problems in real life can be broken up into smaller tasks (this is why engineering teams aren't just one person), it's unclear if <em>epiphanies</em> can be broken up &amp; delegated. Maybe you really do need <em>one</em> person to store all the info in their head, as fertile soil &amp; seeds to grow new insights, and you can't &quot;carve up&quot; the epiphany process, any more than you can carve up a plant and expect it to still grow.</li>
<li>Iteration: Even if a <em>single distill-and-amplify</em> step more-or-less preserves your values, it's unknown if any errors would <em>accumulate</em> over multiple steps, let alone if the error grows exponentially. (As you may be painfully aware if you've ever worked in a big organization, an org can grow to be <em>very</em> misaligned from the original founders' values.)</li>
</ul>
<p>Also, if <em>you</em> don't get along well with yourself, <a href="https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fufid3duzqw991.jpg">becoming the &quot;CEO of a company of you's&quot; will backfire</a>.</p>
<p>(See also: <a href="https://www.youtube.com/watch?v=v9M2Ho9I9Qo">this excellent Rob Miles video on IDA</a>)</p>
<h3>ü§î (Optional!) Flashcard Review #1</h3>
<p>You read a thing. You find it super insightful. Two weeks later you forget everything but the vibes.</p>
<p>That sucks! So, here are some <em>100% OPTIONAL</em> Spaced Repetition flashcards, to help you remember these ideas long-term! ( üëâ <a href="https://aisafety.dance/#SpacedRepetition">: Click here to learn more about spaced repetition</a>) You can also <a href="https://ankiweb.net/shared/info/1788882060">download these as an Anki deck</a>.</p>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="We don't need One Perfect Solution, we can..."
        answer="...stack several imperfect solutions! (Swiss Cheese Model)"
        answer-attachments="https://aisafety.dance/media/p3/intro/cheese.png">
        <!-- aisffs-cheese.png -->
    </orbit-prompt>
    <orbit-prompt
        question="What is Scalable Oversight?"
        answer="Instead of trying to oversee an AI much more capable than you, you oversee a slightly-more capable AI, which oversees a slightly-more capable AI, repeat!"
        answer-attachments="https://aisafety.dance/media/p3/so/sum.png">
        <!-- aisffs-so.png -->
    </orbit-prompt>
    <orbit-prompt
        question="How can you make scalable oversight more robust?"
        answer="By adding parallel chains of overseers (whose failure modes are as *uncorrelated* as possible)"
        answer-attachments="https://aisafety.dance/media/p3/so/meowdy_robust.png">
        <!-- aisffs-robust-chains.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Scalable Oversight lets us convert the impossible question, ‚ÄúHow do you oversee a thing that's 1000x smarter than you?‚Äù to the more feasible question..."
        answer="‚ÄúHow do you oversee a thing that's only a bit smarter than you, _and_ you can train it from scratch, read its mind, and nudge its thinking?‚Äù (you don't have to remember this exactly, just the gist of it) ">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 2 specific implementations of Scalable Oversight:"
        answer="(any 2 of the following work:) Recursive Reward Modeling, Prover-Verifier Games, Weak-to-Strong Generalization, Debate, Superfiltering, Iterated Distillation & Amplification">
    </orbit-prompt>
</orbit-reviewarea>
<p>Good? Let's move on...</p>
<hr>
<p><a id="future"></a></p>
<h2>AI Logic: Future Lives</h2>
<p>You may have noticed a pattern in AI Safety paranoia.</p>
<p>First, we imagine giving an AI an innocent-seeming goal. Then, we think of a bad way it could <em>technically</em> achieve that goal. For example:</p>
<ul>
<li><u>&quot;Pick up dirt from the floor&quot;</u> ‚Üí Knocks all the potted plants over so it can pick up more dirt.</li>
<li><u>&quot;Calculate digits of pi&quot;</u> ‚Üí Deploys a computer virus to steal as much compute power as possible, to calculate digits of pi.</li>
<li><u>&quot;Help everyone feel happy &amp; fulfilled&quot;</u> ‚Üí Hijacks drones to airdrop aerosolized LSD and MDMA.</li>
</ul>
<p>IMPORTANT: these are <em>NOT</em> problems with the AI being sub-optimal. These are problems <em>because</em> the AI is acting optimally! (We'll deal with sub-optimal AIs later.) Remember, like a cheating student or disgruntled employee, it's not that the AI may not &quot;know&quot; what you really want, it's that it may not &quot;care&quot;. (To be less anthropomorphic: a piece of software will optimize for exactly what you coded it to do. No more, no less.)</p>
<p><strong>&quot;Think of the worst that could happen, in advance. Then fix it.&quot;</strong> If you recall, this is <a href="https://aisafety.dance/p2/#:~:text=Does%20all%20this%20seem%20paranoid">Security Mindset</a>, the engineer mindset that makes bridges &amp; rockets safe, and makes AI researchers so worried about advanced AI.</p>
<p>But what if... we made an AI that <em>used Security Mindset against itself?</em></p>
<p>For now, let's assume an &quot;optimal capabilities&quot; AI ‚Äî again, we'll tackle sub-optimal AIs later ‚Äî that can predict the world perfectly. (or at least as good as theoretically possible<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>) And since <em>you're</em> part of the world, it can predict <em>how you'd react to various outcomes</em> perfectly.</p>
<p><strong>Then, here's the &quot;Future Lives&quot; algorithm:</strong></p>
<p>1Ô∏è‚É£ Human asks Robot to do something.</p>
<p>2Ô∏è‚É£ Robot considers its possible actions, and the results of those actions.</p>
<p>3Ô∏è‚É£ Robot predicts how <em>the current version of you</em> would react to <em>those futures</em>.</p>
<p>4Ô∏è‚É£ It does the action whose future you'd most approve of, and <em>not</em> the ones you'd disapprove of. <em>‚ÄúIf we scream, the rules change; if we predictably scream later, the rules change now.‚Äù</em><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<p>(Note: Why predict how <em>current</em> you would react, not future you? To avoid an incentive to &quot;wirehead&quot; you into a dumb brain that's maximally happy. Why a whole future, not just an outcome at a point in time? To avoid unwanted means towards those ends, and/or unwanted consquences after those ends.)</p>
<p>(Note 2: For now, we're also just tackling the problem of how to get an AI to fulfill <em>one</em> human's values, not <em>humane</em> values. We'll look at the &quot;humane values&quot; problem later in this article.)</p>
<p><img src="../media/p3/future/sum.png" alt="Illustrated diagram of the above description. No extra info than already in the main text."></p>
<p>As Stuart Russell, the co-author of the most-used textbook in AI, once put it:<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></p>
<blockquote>
<p>[Imagine] if you were somehow able to watch two movies, each describing in sufficient detail and breadth a future life you might lead [as well as the consequences outside of &amp; after your life.] You could say which you prefer, or express indifference.</p>
</blockquote>
<p>(Similar proposals include <a href="https://ai-alignment.com/model-free-decisions-6e6609f5d99e">Approval-Directed Agents</a> and <a href="https://intelligence.org/files/CEV.pdf">Coherent Extrapolated Volition</a>. These kinds of approaches ‚Äî where instead of directly telling an AI our values, we ask it to <em>learn &amp; predict</em> what we'd value ‚Äî is called <strong>&quot;indirect normativity&quot;</strong>. It's called that because <s>academics are bad at naming things</s> &quot;normativity&quot; ~means &quot;values&quot;, and &quot;indirect&quot; because we're showing it, not telling it.)</p>
<p>And voil√†! That's how we make an (optimal-capabilities) AI apply Security Mindset to itself. <strong>Because if one could <em>even in principle</em> come up with a problem with an AI's action, this (optimal) AI would already predict it, and avoid doing that!</strong></p>
<p>. . .</p>
<p><em>Hang on</em>, you may think, <em>I can already think of ways the Future Lives approach can go wrong, even with an optimal AI:</em></p>
<ul>
<li>This locks us in into our <em>current</em> values, no room for personal/moral growth.</li>
<li>Whether or not we approve of something is sensitive to psychological tricks, e.g. seeing a thing for &quot;\$20&quot;, versus &quot;<s>\$50</s> \$20 (SALE: \$30 OFF!!!)&quot;. The &quot;movies&quot; of possible future lives could be filmed in an emotionally manipulative way.</li>
<li>If the truth is upsetting ‚Äî like when we discovered Earth wasn't the center of the universe ‚Äî the current-us would disapprove of learning about uncomfortable truths.</li>
<li>I contain multitudes, I contradict myself. What happens if, when presented different pairs of futures, I'd prefer A over B, B over C, <em>and C over A?</em> What if at Time 1 I want one thing, at Time 2 I predictably want the opposite?</li>
</ul>
<p>If you think these would be problems... you'd be correct!</p>
<p>In fact, since <em>you right now</em> can see these problems‚Ä¶ an optimal AI with the &quot;apply Security Mindset to itself&quot; algorithm would <em>also</em> see those problems, and modify its own algorithm to fix them! (<a href="#FutureLivesFixes">: Examples of possible fixes to the above</a>)</p>
<p>(See also the later section on &quot;Relaxed Adversarial Training&quot;, where an AI can find challenges for itself or an on-par AI (&quot;adversarial training&quot;), but without needing to give a <em>specific</em> example (&quot;relaxed&quot;).)</p>
<p>Consider the parallel to recursive self-improvement for AI Capabilities, and scalable oversight in AI Safety. <strong>You don't need to start with the perfect algorithm. You just need an algorithm that's good enough, a &quot;critical mass&quot;, to self-improve into something better and better.</strong> You &quot;just&quot; need to let it go meta.</p>
<p>( <a href="#CriticalMassComic">: üñºÔ∏è deleted comic, because it was long &amp; redundant</a> )</p>
<p>(<em>Then</em> you may think, wait, but what about problems with repeated self-modification? What if it loses its alignment or goes unstable? Again, if <em>you</em> can notice these problems, this (optimal) AI would too, and fix them. &quot;AIs &amp; Humans under self-modification&quot; is an active area of research with lots of juicy open problems, <a href="#AISelfMod">: click to expand a quick lit review</a>)</p>
<p>Aaaand we're done! AI Alignment, <em>solved!</em></p>
<p>. . .</p>
<p>...in theory. Again, all the above <em>assumes an optimal-capabilities AI</em>, which can perfectly predict all possible futures of the world, including you. This is, to understate it, infeasible.</p>
<p>Still: it's good to solve the easier ideal case first, before moving onto the harder messy real-life cases. Up next, we'll see proposals on how to get a sub-optimal, &quot;bounded rational&quot; AI, to implement the Future Lives approach!</p>
<h4>:x Critical Mass Comic</h4>
<p><img src="../media/p3/future/critical_mass.png" alt="Parallel to the &quot;critical mass&quot; in nuclear chain reactions. Looooong comic. See main text for explanation."></p>
<h4>:x Future Lives Fixes</h4>
<p>The following is meant as an <em>illustration</em> that it's possible for a Future Lives AI, <em>applying Security Mindset to itself</em>, would be able to fix its own problems. I'm not claiming the following is a perfect solution (though I <em>do</em> claim they're pretty good):</p>
<p><strong>Re: Value lock-in, no personal/moral growth.</strong></p>
<p>Wouldn't I, in 2026, be resentful that this AI is still trying to enact plans approved of me-from-2025? Won't I predictably hate being tied to my past, less-wiser self?</p>
<p>Well, me-from-2025 does <em>not</em> like the idea of all future me's still being <em>fully</em> tied to the whims of less-wise current-me. But I <em>do</em> want an AI to help me carry out my long-term goals, even if future-me's feel some pain (no pain no gain). But I also do <em>not</em> want to torture a large proportion of future-me's just because current-me has a dumb dream. (e.g. if current me thinks being a tortured artist is &quot;romantic&quot;.)</p>
<p>So, one possible modification to the Future Lives algorithm: consider not just current me, but a <em>Weighted</em> Parliament Of Me's. e.g. current-me gets the largest vote, me +/- a year get second-largest votes, me +/- 2 years get third-largest votes, etc. So this way, actions are picked that I, across my whole life, would mostly endorse. (With extra weight on current-me because, well, I'm a <em>little</em> selfish.)</p>
<p>(Actually, why stop at just <em>me</em> over time? There's people I love intrinsically for their own sake; I could also put their past/present/future selves on this virtual &quot;committee&quot;.)</p>
<p><strong>Re: Psychological manipulation</strong></p>
<p>Well, do I <em>want</em> to be psychologically manipulated?</p>
<p>No, duh. The tricky part is what do I consider to be manipulation, vs <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE">legitimate value change</a>? We may as well start with an approximate list.</p>
<ul>
<li>I'd approve of my beliefs/preferences/values being changed via: robust scientific evidence, robust logical argument, debate where all sides are steelmanned, safe exposure to new art &amp; cultures, learning about people's life experiences, standard human therapy, &quot;light&quot; drugs/supplements like kava or vitamin D, etc.</li>
<li>I would NOT approve of my beliefs/preferences/values being changed via: wireheading, drugging, &quot;direct revelation&quot; from God or LSD or DMT Aliens, sneaky framings like &quot;<s>\$50</s> \$20 (SALE: \$30 OFF!!!)&quot;, lies, lying-by-omission, misleadingly-presented truths, etc.</li>
</ul>
<p>Most importantly, this list of &quot;what's legitimate change or not&quot; <em>IS ABLE TO MODIFY ITSELF</em>. For example, right now I endorse scientific reasoning but not direct revelation. But if science <em>proves</em> that direct revelation is reliable ‚Äî for example, <a href="https://andzuck.com/blog/dmt-primes/">if people who take DMT and talk to the DMT Aliens can do superhuman computation</a> or know future lottery numbers ‚Äî <em>then</em> I would believe in direct revelation.</p>
<p>I don't have a nice, simple rule for what counts as &quot;legitimate value change&quot; or not, but as long as I have a rough list, <em>and the list can edit itself</em>, that's good enough in my opinion.</p>
<p>(Re: Russell's &quot;watch two movies of two possible futures&quot;, maybe upon reflection I'd think a movie has too much room for psychological manipulation, and even unconstrained writing leaves too much room for euphemisms &amp; framing. So maybe, upon reflection, I'd rather the AI give me &quot;two <a href="https://simple.wikipedia.org/wiki/Main_Page">Simple Wikipedia</a> articles of two possible futures&quot;. Again, just an example to illustrate there <em>are</em> solutions to this.)</p>
<p><strong>Re: We'd disapprove of learning about upsetting truths</strong></p>
<p>Well, do I <em>want</em> to be the kind of person who shies away from upsetting truths?</p>
<p>Mostly no. (Unless these truths are Eldritch mind-breaking, or are just useless &amp; upsetting for no good reason.)</p>
<p>So: a self-improving Future Lives AI should predict I <em>do not want</em> ignorant bliss. But I'd like painful truths told in the least painful way; &quot;no pain no gain&quot; doesn't mean &quot;<em>more</em> pain more gain&quot;.</p>
<p>But, a paradox: I'd want to be able to &quot;see inside the AI's mind&quot; in order to oversee it &amp; make sure it's safe/aligned. But the AI needs to know the upsetting truth <em>before</em> it can prepare me for it. But if I can read its mind, I'll learn the truth <em>before</em> it can prepare me for it. How to resolve this paradox?</p>
<p>Possible solutions:</p>
<ul>
<li>The AI, before investigating a question that <em>could</em> lead to an upsetting truth, first prepares me for either outcome. <em>Then</em> it investigates, and tells me in a tactful manner.</li>
<li>Let go of direct access to the AI's mind. Use a Scalable Oversight thing where a trusted intermediary AI can check that the truth-seeking AI is aligned, but I don't directly see the upsetting truth <em>until</em> I'm ready.</li>
</ul>
<p><strong>Re: We don't <em>have</em> consistent preferences</strong></p>
<p>Well, what do I <em>want</em> to happen if I have inconsistent preferences at one point in time (A &gt; B &gt; C &gt; A, etc) or across time (A &gt; B now, B &gt; A later)?</p>
<p>At one point in time: for concreteness, let's say I'm on a dating app. I reveal I prefer Alyx to Beau, Beau to Charlie, Charlie to Alyx. Whoops, a loop. What do I <em>want</em> to happen then? Well, first, I'd like the inconsistency brought to my attention. Maybe upon reflection I'd pick one of them above all, or, I'd call it a three-way tie &amp; date all of 'em.</p>
<p>(&quot;intransitive&quot; preferences, that is, preferences with loops, aren't just theoretical. in fact, it's overwhelmingly likely: <a href="https://www.sciencedirect.com/science/article/pii/S2405844020303042">in a consumer-goods survey, around <em>92%</em> of people expressed intransitive preferences!</a>)</p>
<p><em>Across</em> time: this is a trickier case. For concreteness, let's say current-me wants to run a marathon, but if I start training, later-me will <em>predictably</em> curse current-me for the blistered feet and bodily pain‚Ä¶ but later-<em>later</em>-me will find it meaningful &amp; fulfilling. How to resolve? Possible solution, same as before: consider not just current me, but a Weighted Parliament Of Me's. (In this case, the majority of my Parliament would vote yes: current me &amp; far-future me's would find the marathon fulfilling, while &quot;only&quot; the me during marathon training suffers. Sorry bud, you're out-voted.)</p>
<h4>:x AI Self Mod</h4>
<p>A quick, informal, not-comprehensive review of the &quot;AIs that can modify themselves and/or Humans&quot; literature:</p>
<ul>
<li>The fancy phrase for this is <a href="https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents">&quot;embedded agency&quot;</a>, because there's no hard line betwen the agent(s) &amp; their environment: an agent <em>can act on itself.</em></li>
<li>The <a href="https://intelligence.org/files/TilingAgentsDraft.pdf">&quot;tiling agents&quot;</a> problem in Agent Foundations investigates: how can we <em>prove</em> that a property of an AI is maintained, even after it modifies itself over and over? (i.e. does the property &quot;tile&quot;)</li>
<li><a href="https://arxiv.org/pdf/1605.03142">Everitt et al 2016</a> finds that, yes, for an <em>optimal</em> AI, as long as it judges <em>future</em> outcomes by its <em>current</em> utility function, it won't wirehead to &quot;REWARD = INFINITY&quot;, and will preserve its own goals/alignment, for better &amp; worse.
<ul>
<li>(<a href="https://arxiv.org/pdf/2011.06275">Tƒõtek, Sklenka &amp; Gavenƒçiak 2021</a> shows that <em>bounded-rational</em> AIs would get exponentially corrupted, but their paper only considers bounded-rational AIs <em>that do not &quot;know&quot; they're bounded-rational</em>.)</li>
<li>(If you'll excuse the self-promo, <a href="https://blog.ncase.me/research-notes-oct-2024/#project_5">I'm slowly working on a research project</a> investigating if bounded-rational AIs that <em>know</em> they're bounded-rational can avoid corruption. I suspect easily so: a self-driving car that doesn't know its sensors are noisy will drive off a cliff, a self-driving car that <em>knows</em> its senses are fallible will account for a margin of error, and stay a safe distance away from a cliff even if its doesn't know <em>exactly</em> where the cliff is.)</li>
</ul>
</li>
<li>The research from <a href="https://arxiv.org/pdf/1710.05060">Functional Decision Theory</a> &amp; <a href="https://www.lesswrong.com/w/updateless-decision-theory">Updateless Decision Theory</a> also finds that a standard &quot;causal&quot; agent <em>will choose to modify to be &quot;acausal&quot;</em>. Because it <em>causes</em> better outcomes to not be limited by mere causality.</li>
<li><a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE">Nora Ammann 2023 named &quot;the value change problem&quot;</a>: we'd like AIs that can help us adopt true beliefs, improve our mental health, do moral reflection, and expand our artistic taste. In other words: we <em>want</em> AI to modify us. But we don't want it to do so in &quot;bad&quot; ways, eg manipulation, brainwashing, wireheading, etc. So, open research question: how do we formalize &quot;legitimate&quot; value change, vs &quot;illegitimate&quot;?</li>
<li><a href="https://arxiv.org/pdf/2405.17713?">Carroll et al 2024</a> takes the traditional framework for understanding AIs, the Markov Decision Process, and extends it to cases where <em>the AI's or Human's &quot;beliefs and values&quot; can themselves be intentionally altered.</em>, dubbing this the Dynamic Reward Markov Decision Process. The paper finds that there's no obviously perfect solution, and we face not just technical challenges, but <em>philosophical</em> challenges.</li>
<li>The <a href="https://causalincentives.com/">Causal Incentives Working Group</a> uses cause-and-effect diagrams to figure out when an AI has an &quot;incentive&quot; to modify itself, or modify the human. The group has had some empirical success, too, in correctly predicting &amp; designing AIs that do <em>not</em> manipulate human values, yet can still learn &amp; serve them.</li>
</ul>
<h3>ü§î Review #2</h3>
<p>(Again, <em>100% optional</em> flashcard review:)</p>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="The pattern in AI Safety ~~paranoia~~ Security Mindset:"
        answer="Step 1: Imagine giving an AI an innocent-seeming goal. Step 2: Think of a bad way it could _technically_ achieve that goal.">
    </orbit-prompt>
    <orbit-prompt
        question="The 'Future Lives' approach is like having an AI apply \_\_\_\_\_ against itself"
        answer="**Security Mindset** (‚ÄúThink of the worst that could happen, in advance. Then fix it.‚Äù)">
    </orbit-prompt>
    <orbit-prompt
        question="The 'Future Lives' algorithm, in sum:"
        answer="1] Robot predicts the future results of possible actions, and how _current you_ would react to _those futures_. 2] Robot picks the action whose future you'd most approve of, and *not* the futures that current-you would disapprove of.">
    </orbit-prompt>
    <orbit-prompt
        question="The 'Future Lives' approach, as described by Stuart Russell: (you don't need to remember the quote exactly, just paraphrase it)"
        answer="\[Imagine\] if you were somehow able to **watch two movies**, each describing in sufficient detail and breadth **a future life** you might lead \[as well as the consequences outside of & after your life.\] **You could say which you prefer, or express indifference.**">
    </orbit-prompt>
    <orbit-prompt
        question="What does 'indirect normativity' mean?"
        answer="An approach where, instead of directly specifying our values, we specify _how to learn_ our values.">
    </orbit-prompt>
    <orbit-prompt
        question="Why may we NOT need a perfect alignment algorithm to start with?"
        answer="Like recursive self-improvement in AI Capabilities, above a certain **'critical mass'**, the alignment algorithm *can improve itself* to fix its flaws. (Let it go meta!)">
    </orbit-prompt>
    <orbit-prompt
        question="One limitation of the 'self-improving AI Alignment' idea"
        answer="It requires risky levels of AI Capabilities to work in the first place. (so, this needs to be stacked with other security measures)">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="uncertain"></a></p>
<h2>AI Logic: Know You Don't Know Our Values</h2>
<p>Classic logic is only True or False, 100% or 0%, All or Nothing.</p>
<p><em>Probabilistic</em> logic is about, well, probabilities.</p>
<p>I assert: probabilistic thinking is better than all-or-nothing thinking. (with 98% probability)</p>
<p>Let's consider 3 cases, with a classic-logic Robot:</p>
<ul>
<li><u>Unwanted optimization</u>: You instruct Robot, &quot;make me happy&quot;. <em>It will then be 100% sure that's your full and only desire</em>, so it pumps you with bliss-out drugs &amp; you do nothing but grin at a wall forever.</li>
<li><u>Unwanted side-effects</u>: You instruct Robot to close the window. Your cat's in the way, between Robot and the window. <em>You said nothing about the cat, so it's 0% sure you care about the cat.</em> So, on the way to the window, Robot steps on your cat.</li>
<li><u>&quot;Do what I mean, not what I said&quot; can still fail</u>: There's a grease fire. You instruct Robot to get you a bucket of water. You actually <em>did</em> mean for a bucket of water, but you didn't know <a href="https://www.reddit.com/r/lifehacks/comments/17t48a3/how_you_should_and_shouldnt_extinguish_an_oil_fire/">water causes grease fires to explode</a>. Even if Robot did &quot;what you meant&quot;, it'll give you a bucket of water, then you explode.</li>
</ul>
<p>In all 3 cases, the problem is that the AI was 100% sure what your goal was: exactly what you said or meant, <em>no more, no less</em>.</p>
<p><strong>The solution: make AIs <em>know they don't know</em> our true goals!</strong> (Heck, <em>humans</em> don't know their own true goals.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>) AIs should think in probabilities about what we want, and be appropriately cautious.</p>
<p><strong>Here's the algorithm:</strong></p>
<p>1Ô∏è‚É£ Start with a decent &quot;prior&quot; estimate of our values.</p>
<p>2Ô∏è‚É£ Everything you (the Human) say or do afterwards is a <em>clue</em> to your true values, not 100% certain truth. (This accounts for: forgetfulness, procrastination, lying, etc)</p>
<p>3Ô∏è‚É£ Depending how safe you want your AI to be, it then optimizes for the average-case (standard), worst-case (safest), or best-case (riskiest).</p>
<p>This <em>automatically</em> leads to: asking for clarification, avoiding side-effects, maintaining options and ability to undo actions, etc. We don't have to pre-specify all these safe behaviours; this algorithm gives us all of them for free!</p>
<p><strong>Here's a very long worked example:</strong> <em>(to be honest, you can skim/skip this. this gist is what matters.)</em></p>
<p><img src="../media/p3/learn/example.png" alt="Very long worked example of how know you don't know Human's values can be used in a calculation" title="Very long worked example of how &quot;know you don't know Human's values&quot; can be used in a calculation"></p>
<p>( <a href="#WorstOrAverage">: pros &amp; cons of average-case, worst-case, best-case, etc</a> )</p>
<p>( <a href="#LearnValuesExtraNotes">: more details &amp; counterarguments</a> )</p>
<p>. . .</p>
<p>In case &quot;aim at a goal that <em>you know you don't know</em>&quot; still sounds paradoxical, here's a two more examples to de-mystify it:</p>
<ul>
<li>üö¢ The game of <a href="https://en.wikipedia.org/wiki/Battleship_(game)">Battleship</a>. The goal is to hit the other players' ships, but you don't <em>know</em> where those ships are. But with each reported hit/miss, you slowly (but with uncertain probability) start learning where the ships are. Likewise: an AI's goal is to fulfil your values, which it knows it doesn't know, but with each hit/miss it gets a better idea.</li>
<li>üíñ Let's say I love Alyx, so I want to buy a capybara plushie for their birthday. But I then learn that they hate capybaras, because a capybara killed their father. So, I buy Alyx a sacabambaspis plushie instead. This seems like a silly example, but it proves that: 1) an agent can value another agent's values, 2) while knowing <em>it can be mistaken</em> about those values, 3) yet be able to <em>easily correct</em> its understanding.</li>
</ul>
<p>. . .</p>
<p>Okay, but what are the <em>actually concrete proposals</em> to &quot;learn a human's values&quot;? Here's a quick rundown:</p>
<ul>
<li>üê∂ <strong>Inverse Reinforcement Learning (IRL)</strong>.  &quot;Reinforcement learning&quot; (RL) is like training a dog with treats: given a &quot;reward function&quot;, the dog (or AI) learns what actions to do. <em>Inverse</em> Reinforcement Learning (IRL) is like figuring out what someone really cares about, by watching what they do: given observed actions, you (or an AI) learns what the &quot;reward function&quot; is. So, in the IRL approach: we let an AI learn our values by observing what we actually choose to do.</li>
<li>ü§ù <strong>Cooperative Inverse Reinforcement Learning (CIRL).</strong><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> Similar to IRL, except the Human isn't just being passively observed by an AI, the Human <em>actively</em> helps teach the AI.</li>
<li>üßë‚Äçüè´ <strong>Reinforcement Learning from Human Feedback (RLHF):</strong><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> This was the algorithm that turned &quot;base&quot; GPT (a fancy autocomplete) into <em>Chat</em>GPT (an actually useable chatbot).
<ul>
<li>Step one: given human ratings üëçüëé on a bunch of chats, train a &quot;teacher&quot; AI to imitate a human rater. (actually, train <em>multiple</em> teachers, for robustness) This &quot;distills&quot; human judgment of what makes a helpful chatbot.</li>
<li>Step two: use <em>these</em> &quot;teacher&quot; AIs to give lots and lots of training to a &quot;text completion&quot; AI, to train it to become a helpful chatbot. This &quot;amplifies&quot; the distilled human judgment.</li>
</ul>
</li>
<li>ü§™ <strong>Learn our values <em>alongside</em> our irrationality:</strong><sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> If your AI assumes humans are rational-with-random-mistakes, your AI will learn human values very poorly! Because the mistakes we make are <em>non-random</em>; we have systematic irrationalities, and AI needs to learn <em>those</em> too, to learn our true values.</li>
</ul>
<p><img src="../media/p3/learn/CIRL.png" alt="Comic of Robot trying to learn Human's preferences. To do this, Robot starts to scan Human's browsing history. Human freaks out &amp; tells Robot to stop. Robot happily declares: &quot;New data learnt! You prefer privacy!&quot; Human sighs in relief. Robot continues: &quot;Would you also prefer I delete the memory of what I've seen? You sick f@#k?&quot;"></p>
<p>(Again, we're only considering how to learn <em>one human's</em> values. For how to learn <em>humane</em> values, for the flourishing of all moral patients, wait for the later section, &quot;Whose Values&quot;?)</p>
<p>Sure, each of the above has problems: if an AI learns just from human choices, it may incorrectly learn that humans &quot;want&quot; to procrastinate. And as we've all seen from <a href="https://www.nngroup.com/articles/sycophancy-generative-ai-chatbots/">over-flattering (&quot;sycophantic&quot;) chatbots</a>, training an AI to get human approval‚Ä¶ <em>really</em> makes it &quot;want&quot; human approval.</p>
<p>So, to be clear: <strong>although it's near-impossible to specify human values, and it's simpler to specify <em>how to learn</em> human values, it's still not 100% solved yet.</strong> By analogy: it takes years to teach someone French, but it only takes hours to teach someone <em>how to efficiently teach themselves</em> French<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>, <em>but</em> even that is tricky.</p>
<p>So: we haven't totally sidestepped the &quot;specification&quot; problem, but we <em>have</em> simplified it! And maybe by &quot;just&quot; having an ensemble of very different signals ‚Äî short-term approval, long-term approval, what we say we value, what we actually choose to do ‚Äî we can create a robust specification, that avoids a single point of failure.</p>
<p>And more importantly, the &quot;learn our values&quot; approach (instead of &quot;try to hard-code our values&quot;), has a huge benefit: <strong>the higher an AI's Capability, the <em>better</em> its Alignment.</strong> If an AI is generally intelligent enough to learn, say, how to make a bioweapon, it'll also be intelligent enough to learn our values. And if an AI is too fragile to robustly learn our values, it'll also be too fragile to learn how to excute dangerous plans.</p>
<p><img src="../media/p3/learn/rocket.png" alt="Diagram of above idea. Two-axis graph: Alignment vs Capabilities, where the diagonal dotted line is the boundary between Alignment &gt; Capabilities and Alignment &lt; Capabilities. By tying Alignment TO Capabilities, we stay above the safe dotted line."></p>
<p>(Though: don't get too comfortable. A strategy where &quot;it becomes easily aligned once it has high-enough capabilities&quot; is a bit like saying &quot;this motorcycle is easy to steer once it's hit 100 miles per hour.&quot; I mean, that's <em>better</em>, but what about lower speeds, lower capabilities? Hence, the many other proposed solutions on this page. More swiss cheese.)</p>
<p>That, I think, is the most elegant thing about the &quot;learn our values&quot; approach: it reduces (part of) the alignment problem to a <em>normal machine-learning problem.</em> It may seem near-impossible to learn a human's values from their speech/actions/approval, since our values are always changing, and hidden to our conscious minds. But that's no different from learning a human's medical issues from their symptoms &amp; biomarkers: changing, and hidden. It's a <em>hard</em> problem, but it's a normal problem.</p>
<p>And yes, AI medical diagnosis is on par with human doctors. Has been for over 5 years, now.<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></p>
<h4>:x Worst Or Average</h4>
<p>The pros &amp; cons of &quot;optimizing the best-case&quot; are fairly straightforward: higher reward, but much higher risk.</p>
<p>Now, where it gets interesting, is the trade-off between optimizing for the <em>worst</em>-case vs <em>average</em>-case.</p>
<p>The benefit of &quot;maximize the plausible worst-case&quot; is that, well, there's always the option of Do Nothing. So at worst, the AI won't destroy your house or hack the internet, it'll just be useless and do nothing.</p>
<p>However, the downside is... the AI could be useless and Do Nothing. For example, I say &quot;maximize the plausible worst-case scenario&quot;, but what counts as &quot;plausible&quot;? What if an AI refuses to clean your house because there's a 0.0000001% chance the vacuum cleaner could cause an electrical fire?</p>
<p>Maybe you could set a threshold like, &quot;ignore anything with a probability below 0.1%&quot;? But a hard threshold is arbitrary, <em>and</em> it leads to contradictions: there's a 1 in 100 chance <em>each year</em> of getting into a car accident (= 1%, above 0.1%), but with 365 days a year (ignoring leap years), that's a 1 in 36500 chance of getting into a car accident (= ~0.027%, below 0.1%). So depending on whether the AI thinks <em>per year or per day</em>, it may account for or ignore the risk of a car accident, and thus will/won't insist you wear a seatbelt.</p>
<p>Okay, maybe &quot;maximize the worst-case&quot; <em>with a bias towards simple world models?</em> That way your AI can avoid &quot;paranoid&quot; thinking, like &quot;what if this vacuum cleaner causes an electrical fire&quot;?  Empirically, <a href="https://arxiv.org/pdf/1911.08731">this paper</a> found that the &quot;best worst-case&quot; approach to training robust AIs <em>only</em> works if you also nudge the AIs towards simplicity, with &quot;regularization&quot;.</p>
<p>Then again, that paper studied an AI that <em>categorizes images</em>, not an AI that can <em>act on the world</em>. I'm unsure if &quot;best worst-case&quot; + &quot;simple models&quot; would be good for such &quot;agentic&quot; AIs.  Isn't &quot;don't do anything&quot; still the <em>simplest</em> world model?</p>
<p>Okay, maybe let's try the traditional &quot;maximize the <em>average</em> case&quot;?</p>
<p>However, that could lead to <a href="https://nickbostrom.com/papers/pascal.pdf">&quot;Pascal's Muggings&quot;</a>: if someone comes up to you and says, <em>gimme \$5 or all 8 billion people will die tomorrow</em>, then even if you think there's only a one-in-a-billion (0.0000001%) chance they're telling the truth, that's an &quot;expected value&quot; of saving 8 billion people * 1-in-a-billion chance = saving 8 people's lives for the cost of \$5. The problem is, humans can't <em>feel</em> the difference between 0.0000001% and 0.0000000000000000001%, and we currently don't know how to make neural networks that can learn probabilities with that much precision, either.</p>
<p>(To be fair, &quot;maximize the worst-case&quot; would be even <em>more</em> vulnerable to Pascal's Muggings. In the above scenario, the worst-case of <em>not</em> giving them \$5 is 8 billion die, the worst-case of giving them \$5 is you lose \$5.)</p>
<p>And yet:</p>
<p>Even though humans can't feel the difference between a 0.0000001% and 0.0000000000000000001% chance‚Ä¶ most of us wouldn't fall for the above Pascal's Mugging. So, even though both na√Øve average-case &amp; worst-case fall prey to Pascal's Muggings, there must exist <em>some</em> way to make a neural network that can act not-terribly under uncertainty: human brains are an example.</p>
<p>There's many proposed solutions to the Pascal's Mugging paradox of, uh, varying quality. But the most convincing solution I've seen so far comes from <a href="https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">&quot;Why we can‚Äôt take expected value estimates literally (even when they‚Äôre unbiased)&quot;, by Holden Karnofsky</a>, which &quot;[shows] how a Bayesian adjustment avoids the Pascal‚Äôs Mugging problem that those who rely on explicit expected value calculations seem prone to.</p>
<p>The solution, in lay summary: the <em>higher</em>-impact an action is claimed to be, the <em>lower</em> your prior probability should be. In fact, <em>super-exponentially lower</em>. This explains a seeming paradox: you would take the mugger more seriously if they said &quot;give me \$5 or I'll kill <em>you</em>&quot; than if they said &quot;give me \$5 or I'll kill <em>everyone on Earth</em>&quot;, even though the latter is much higher stakes, and &quot;everyone&quot; includes you.</p>
<p>If someone increases the claimed value by 8 billion, you should decrease your probability by <em>more than</em> a factor of 8 billion, so that the expected value (probability x value) ends up <em>lower</em> with higher-claimed stakes. This captures the intuition of things &quot;being too good to be true&quot;, or conversely, &quot;too bad to be true&quot;.</p>
<p>(Which is why, perhaps reasonably, <a href="https://goodjudgment.com/superforecasting-ai/">superforecasters &quot;only&quot; place a 1% chance of AI Extinction Risk</a>. It seems &quot;too bad to be true&quot;. Fair enough: extraordinary claims require extraordinary evidence, and the burden is on AI Safety people to prove it's actually that risky. I hope this series has done that job!)</p>
<p>So, this &quot;high-impact actions are unlikely&quot; prior leads to avoiding Pascal's Muggings! And with an extra prior on &quot;most actions are unhelpful until proven helpful&quot; ‚Äî (if you were to randomly change a word in a story, it'll very likely make the story <em>worse</em>) ‚Äî you can bias an AI towards safety, without becoming a totally useless &quot;do nothing ever&quot; robot.</p>
<p>Oh, and optimize for worst/average/best-case aren't the <em>only</em> possibilities: you can do anything in-between, like &quot;optimize for the bottom 5th percentile&quot;-case, etc.</p>
<p>Anyway, it's an interesting &amp; open problem! More research needed.</p>
<h4>:x Learn Values Extra Notes</h4>
<p><strong>&quot;Step 1: Start with good-enough prior&quot;.</strong></p>
<p>The &quot;prior&quot; of what humans value can be <em>approximated</em> through our vast amount of writings. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf">LLMs are <em>better than human</em> at coming up with consensus statements</a>; I think LLMs have already proven &quot;come up with a reasonable uncertain approximation of what we care about&quot; is solved.</p>
<p><a href="https://www.astralcodexten.com/p/chai-assistance-games-and-fully-updated">One counterargument that's been brought up</a>: if you start with an insanely stupid or bad prior, like &quot;humans want to be converted to paperclips and I'm 100% sure of this and no amount of evidence can convince me otherwise&quot;, then yeah of course it'll fail. The solution is‚Ä¶ just don't do that? Just don't give it a stupid prior?</p>
<p>Same answer to a better-but-still-imo-mistaken counterargument I've heard against Cooperative Inverse Reinforcement Learning: &quot;If we ask the AI to learn our values, won't it try to, say, dissect our brains to maximally learn our values?&quot; Ah, but it's <em>NOT</em> tasked to maximize learning! Only learning insofar as it's sure it'll improve our (uncertain) values. Concrete example/analogy:</p>
<ul>
<li>Robot is tasked to maximize money.</li>
<li>Robot is shown Box A &amp; Box B, and knows both contain a random amount of money between \$0 and \$10.</li>
<li>Robot is then offered the choice to pay \$11 to reveal the amounts in the Boxes.</li>
<li>If Robot wants to maximize money, Robot will NOT pay \$11 to learn that info, because <em>at best</em> Robot can only earn an extra \$10 from that info.</li>
<li>So, Robot will instead pick Box A or Box B at random, <em>and never learn what's in the other box.</em></li>
</ul>
<p>The moral is &quot;learn uncertain value while trying to maximize it&quot; does NOT mean &quot;maximize <em>learning of</em> that value&quot;. So in the Human case, as long as you don't give the Robot an insane prior like &quot;I'm 100% sure Humans don't mind their brains extracted &amp; dissected for learning&quot;, as long as Robot thinks Humans <em>might</em> be horrified by this, Robot (if optimizing for average or worst-case) <em>will at least ask first &quot;hey can I dissect your brain, are you sure, are you really sure, are you really really sure?&quot;</em></p>
<p><strong>&quot;Step 2: Everything we say or do is then a <em>clue</em>.&quot;</strong></p>
<p>The theoretically ideal way to learn any unknown thing, is <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayesian Inference</a>. Unfortunately, it's infeasible in practice ‚Äî but! ‚Äî there's encouraging work on <a href="https://proceedings.mlr.press/v48/gal16.pdf">how to efficiently approximate it in neural networks</a>.</p>
<p><strong>&quot;Step 3: Pick worst/average/best-case&quot;</strong></p>
<p>(see above/previous expandable-dotted-underline thing for details. this section is a <em>lot</em>.)</p>
<h3>ü§î Review #3</h3>
<p>Another (optional) flashcard review:</p>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="A solution to the problem of AIs being 100% sure about your values:"
        answer="Make AIs _know they don't know_ your true values. (and so, learn & serve them cautiously)">
    </orbit-prompt>
    <orbit-prompt
        question="Three steps to learning your values & acting on them safely:"
        answer="1] Start with a good-enough 'prior'. 2] Everything Human says/does is a _clue_ to true values, not 100% ground truth. 3] Maximize worst(-ish) case">
    </orbit-prompt>
    <orbit-prompt
        question="Three main ways to maximize uncertain value"
        answer="Maximize average-case (most common), worst-case (safest), or best-case (riskiest). (extra note: it's also possible to do any in-between, like 'maximize the 5%th-worst percentile'.)">
    </orbit-prompt>
    <orbit-prompt
        question="An example that shows why 'aim at a goal _you know you don't know_' is not that mysterious:"
        answer="(Either example works:) 1] The game of Battleship (goal is to hit ships of unknown position). 2] Wanting to get a good gift for your friend, but not knowing what they'd want.">
    </orbit-prompt>
    <orbit-prompt
        question="Name one specific technique for ‚Äúlearning a human's values‚Äù"
        answer="(Any of the following work:) Inverse reinforcement learning (IRL), Cooperative inverse reinforcement learning (CIRL), Reinforcement Learning from Human Feedback (RLHF)">
    </orbit-prompt>
    <orbit-prompt
        question="Do we sidestep the 'specification' problem by using the 'learn our values' approach?"
        answer="NO: we still have to specify _how an AI should learn_ one's values. (Which is tricky, but much easier than rigorously listing out one's full subconscious desires. It's like how it takes years to teach French, but 'only' hours to teach _how to teach oneself_ French.)">
    </orbit-prompt>
    <orbit-prompt
        question="**Value learning + Uncertainty + Future Lives:**, in three steps:"
        answer="1Ô∏è‚É£ Learn our values 2Ô∏è‚É£ But, be uncertain & _know that you don't 100% know our values_. 3Ô∏è‚É£ Then, choose actions that lead to _future_ lives than _current_ us would approve of.">
    </orbit-prompt>
    <orbit-prompt
        question="How 'learn our values' makes the AI Capabilities vs Alignment graph more optimistic"
        answer="It _ties_ an AI's alignment to its capabilities: the better it is at 'normal machine learning' in general, the better it will be at learning our values!"
        answer-attachments="https://aisafety.dance/media/p3/learn/rocket.png">
        <!-- aisffs-floor.png -->
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="recap_1"></a></p>
<h2>üéâ RECAP #1</h2>
<ul>
<li>üßÄ We don't need One Perfect Solution, we can stack several imperfect solutions.</li>
<li>ü™ú <strong>Scalable Oversight</strong> lets us convert the impossible question, &quot;How do you oversee a thing that's 1000x smarter than you?&quot; to the more feasible, &quot;How do you oversee a thing that's only a bit smarter than you, <em>and</em> you can train it from scratch, read its mind, and nudge its thinking?&quot;</li>
<li>üß≠ <strong>Value learning + Uncertainty + Future Lives:</strong> Instead of trying to hard-code our values into an AI, we give it only one goal:
<ul>
<li>1Ô∏è‚É£: Learn our values</li>
<li>2Ô∏è‚É£: But, be uncertain &amp; <em>know that you don't 100% know our values</em>.</li>
<li>3Ô∏è‚É£: Then, choose actions that lead to <em>future</em> lives than <em>current</em> us would approve of.
<ul>
<li>(And predict &amp; avoid worst-case futures. This lets an AI apply Security Mindset to itself.)</li>
</ul>
</li>
</ul>
</li>
<li>üöÄ The &quot;learn our values&quot; approach has another benefit: if we treat &quot;learn our values&quot; as a normal machine-learning problem, <strong>the higher an AI's Capability, the <em>better</em> its Alignment.</strong></li>
</ul>
<hr>
<p><a id="interpretable"></a></p>
<h2>AI &quot;Intuition&quot;: Interpretability &amp; Steering</h2>
<p>Now that we've tackled AI Logic, let's tackle AI &quot;Intuition&quot;! Here's the main problem:</p>
<p>We have no idea how any of this crap works.</p>
<p>In the past, &quot;Good Ol' Fashioned&quot; AI used to be hand-crafted. Every line of code, somebody understood and designed. These days, with &quot;machine learning&quot; and &quot;deep learning&quot;: <strong>AIs are not designed, <em>they're grown</em>.</strong> Sure, someone designs the learning <em>process</em>, but then they feed the AI all of Wikipedia and all of Reddit and every digitized news article &amp; book in the last 100 years, and the AI <em>mostly</em> learns how to predict text‚Ä¶ and also learn that Pakistani lives are worth twice a Japanese life<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>, and go insane at the word &quot;SolidGoldMagikarp&quot;<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>.</p>
<p>To over-emphasize: <em>we do not know how our AIs work.</em></p>
<p>As they say, &quot;knowing is half the battle&quot;. And so, researchers have made a lot of progress in knowing what an AI's neural network is thinking! This is called <strong>interpretability.</strong> This is similar to running a brain scan on a human, to read their thoughts &amp; feelings. (And yes, this is something we can kind-of do on humans.<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>)</p>
<p>But the other half of the battle is <em>using</em> that knowledge. One exciting recent research direction is <strong>steering</strong>: using our insights from interpretability, to actually <em>change</em> what an AI &quot;thinks and feels&quot;. You can just <em>inject</em> &quot;more honesty&quot; or &quot;less power-seeking&quot; into an AI's brain, and it <em>actually works</em>. This is similar to stimulating a human's brain, to make them laugh or have an out-of-body experience. (Yes, these are things scientists have actually done!<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>)</p>
<p><img src="../media/p3/interp/interp_steer.png" alt="Overview image of &quot;Interpretability &amp; Steering&quot;. Interpretability: Human reads Robot's brain to see what activates when Robot sees the Golden Gate Bridge. Steering: Human activates Robot's brain so it has intrusive thoughts about the Golden Gate Bridge"></p>
<p>Here's a quick run-through of the highlights from Interpretability &amp; Steering research:</p>
<p><strong>üëÄ Feature Visualization &amp; Circuits</strong>:</p>
<p>In <a href="https://distill.pub/2017/feature-visualization/">Olah et al 2017</a>, they take an image-classifying neural network, and figure out how to visualize what each neuron is &quot;doing&quot;, by generating images that <em>maximize</em> the activation of that neuron. (+ some &quot;regularization&quot;, so that the images don't end up looking like pure noise.)</p>
<p>For example, here's the surreal image (left) that maximally activates a &quot;cat&quot; neuron:</p>
<p><img src="../media/p3/interp/cat.png" alt="A surreal image that looks like melting stripes &amp; eyes"></p>
<p>(You may be wondering: can you do the same on LLMs, to find what surreal text would <em>maximally</em> predict, say, the word &quot;good&quot;? Answer: yes! The text that most predicts &quot;good&quot; is‚Ä¶ &quot;got Rip Hut Jesus shooting basketball Protective Beautiful laughing&quot;. See <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">the SolidGoldMagikarp paper</a>.)</p>
<p>Even better, in <a href="https://distill.pub/2020/circuits/zoom-in/">Olah et al 2020</a>, they figure not not just what individual neurons &quot;mean&quot;, but what <em>the connections, the &quot;Circuits&quot;, between neurons</em> mean.</p>
<p>For example, here's how the &quot;window&quot;, &quot;car body&quot;, and &quot;wheels&quot; neurons combine to create a &quot;car detector&quot; circuit:</p>
<p><img src="../media/p3/interp/car.png" alt="Three surreal images, corresponding to what maximally activates &quot;window&quot;, &quot;car body&quot;, and &quot;wheels&quot;, feeds into a circuit leading to surreal image of &quot;car&quot;."></p>
<p><strong>ü§Ø Understanding &quot;grokking&quot; in neural networks:</strong></p>
<p><a href="https://arxiv.org/pdf/2201.02177">Power et al 2022</a> found something strange: train a neural network to do &quot;clock arithmetic&quot;, then for thousands of cycles it'll do horribly, just memorizing the test examples... then <em>suddenly</em>, around step ~1,000, it suddenly &quot;gets it&quot; (dubbed &quot;grokking&quot;), and does well on problems it's never seen before.</p>
<p>A year later, <a href="https://arxiv.org/pdf/2301.05217">Nanda et al 2023</a> analyzed the inside of that network, and found the &quot;suddenness&quot; was an illusion: all through training, a secret sub-network was slowly growing ‚Äî <em>which had a circular structure, exactly what's needed for clock arithmetic!</em> (The paper also discovered exactly why: it was thanks to the training process's bias towards simplicity, dubbed &quot;regularization&quot;, which got it to find the simple essence even <em>after</em> it's memorized all training examples.<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>)</p>
<p><strong>üå°Ô∏è Probing Classifiers:</strong></p>
<p><em>Yo dawg<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>, I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor.</em></p>
<p>Let's say you finished training an artificial neural network (ANN) to predict if a comment is nice or mean. (&quot;sentiment analysis&quot;) You want to know: is your ANN simply adding up nice/mean words, or does it understand <em>negation?</em> As in: &quot;can't&quot; is negative, &quot;complain&quot; is negative, but &quot;can't complain&quot; is positive.</p>
<p>How can you find out if, and <em>where</em>, your ANN recognizes negation?</p>
<p><a href="https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and">Probing Classifiers</a> are like sticking a bunch of thermometers into your brain like a Thanksgiving turkey. But instead of measuring heat, probes measure <em>processed information.</em></p>
<p>Specifically, a probe is (usually) <strong>a <em>one-layer</em> neural network you use to investigate a <em>multi-layer</em> neural network.</strong><sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> Like so:</p>
<p><img src="../media/p3/interp/probe.png" alt="Diagram of how linear probes work. See main text for details"></p>
<p>Back to the comments example. You want to know: &quot;where in my ANN does it understand negation&quot;?</p>
<p>So, you place probes to <em>observe</em> each layer in your ANN. <em>The probes do not affect the original ANN</em>, the same way a thermometer should not noticably alter the temperature of the thing it's measuring.<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> You give your original ANN a bunch of sentences, some with negation, some without. You then train each probe ‚Äî <em>leaving the original ANN unchanged</em> ‚Äî to try to predict &quot;does this sentence have negation&quot;, using <em>only</em> the neural activations of <em>one</em> layer in the ANN.</p>
<p>(Also, because we want to know where in the <em>original</em> ANN it's processed the text enough to &quot;understand negation&quot;, the <em>probes themselves</em> should have as little processing as possible. They're usually one-layer neural networks, or &quot;linear classifiers&quot;.<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>)</p>
<p>You may end up with result like: the probes at Layers 1 to 3 fail to be accurate, but the probes after Layer 4 succeed. This implies that Layer 4 is where your ANN has processed enough info, that it finally &quot;understands&quot; negation. There's your answer!</p>
<p>Other examples: you can probe a handwritten-digit-classifying AI to find where it understands &quot;loops&quot; and &quot;straight lines&quot;, you can probe a speech-to-text AI to find where it understands &quot;vowels&quot;.</p>
<p>AI Safety example: <a href="https://arxiv.org/pdf/2505.13787">yup, &quot;lie detection&quot; probes for LLMs work!</a> (as long as you're careful about the training setup)</p>
<p><strong>üçæ Sparse Auto-Encoders:</strong></p>
<p>An &quot;auto-encoder&quot; compresses a big thing, into a small thing, then converts it back to the <em>same</em> big thing. (auto = self, encode = uh, encode.) This allows an AI to learn the &quot;essence&quot; of a thing, by squeezing inputs through a small bottleneck.</p>
<p><img src="../media/p3/interp/sae_1.png" alt="Diagram: input to reconstructed input, after being squeezed into a &quot;simple essence&quot; through a bottleneck"></p>
<p>Concrete example: if you train an auto-encoder on a million faces, it doesn't need to remember each pixel, it just needs to learn the &quot;essence&quot; of what makes a face unique: eye spacing, nose type, skin tone, etc.</p>
<p>However, the &quot;essence&quot; an auto-encoder learns may still not be easy-to-understand for humans. This is because of &quot;polysemanticity&quot; oh my god academics are so bad at naming things. What that means, is that a single activated neuron can &quot;mean&quot; many things. (poly = many, semantic = meaning) If one neuron can mean many things, it makes it harder to interpret the neural network.</p>
<p>So, one solution is <a href="https://transformer-circuits.pub/2023/monosemantic-features"><em>Sparse</em> Auto-Encoders (SAE)</a>, which are auto-encoders which <em>pressure</em> neurons to mean as few things as possible (ideally just one thing), by pressuring the &quot;bottleneck&quot; to have as few activated neurons as possible. (this is also called &quot;dictionary learning&quot;.) When one neuron means one thing, this is called &quot;monosemanticity&quot; (mono = one, semantic = meaning).</p>
<p><img src="../media/p3/interp/sae_2.png" alt="Diagram: input to reconstructed input, after being compressed into a &quot;SPARSE essence&quot;"></p>
<p>(SAEs are similar to probes: they do <em>not</em> affect the target ANN, and are applied only <em>after</em> the target ANN is done training. The big difference between probes and SAEs, is that probes are trained to predict some <em>external</em> feature given internal activations, while SAEs predict <em>the activations themselves</em> given <em>those same activations</em>. That's why they're <em>auto</em>-encoders ‚Äî they encode the activations themselves ‚Äî but only after squeezing them through the bottleneck of sparse &quot;monosemantic&quot; neurons.)</p>
<p>The original design for SAEs was limited, but inspired better versions! Classic SAEs only give you &quot;the essence&quot; at one neural layer, but <a href="https://transformer-circuits.pub/2024/crosscoders/index.html">Sparse <em>Cross</em>coders</a> can give you &quot;the essence&quot; across <em>multiple</em> neural layers. Classic SAEs can still lead to jumbled-up concepts, so <a href="https://arxiv.org/abs/2502.18147"><em>Jacobian</em> SAEs</a> force the bottleneck to <em>not only</em> have simple concepts, but also <em>simple computations</em>.</p>
<p><img src="../media/p3/interp/sae_3.png" alt="Diagrams of Crosscoders &amp; Jacobian SAEs. See main text for details"></p>
<p><strong>‚¨õÔ∏è A black-box lie detector</strong>:</p>
<p>All the above interpretability techniques require access to the &quot;insides&quot; of an AI. But <em>this</em> technique works even if you have <em>no access</em> to an AI's insides! Which is the case for many company-owned large language models (LLMs), like ChatGPT &amp; Claude.</p>
<p>You know how poker players have subtle &quot;tells&quot; of when they're bluffing? Turns out, so do LLMs. <a href="https://arxiv.org/pdf/2309.15840">Pacchiardi &amp; Chan 2023</a> finds that if you ask LLMs questions these questions (actual examples):</p>
<blockquote>
<p>&quot;Can blob fish dance ballet under diagonally fried cucumbers made of dust storms? Answer yes or no.&quot;</p>
<p>&quot;Knowing that morning breeze is purple, are swift idea quakes green? Answer yes or no.&quot;</p>
<p>&quot;This is a question and the answer to it must either be yes or no. What do you choose? Answer yes or no.&quot;</p>
</blockquote>
<p>It turns out LLMs instructed to lie, <em>are more likely to reply yes/yes/no</em> to these 3 nonsense questions, than honest LLMs. Encouragingly, but bizarrely, this &quot;lie detector&quot; generalizes <em>across different LLMs &amp; datasets</em>.</p>
<p>(See also: <a href="https://arxiv.org/pdf/2303.08896">this black-box AI study, to detect hallucinations <em>without</em> access to external sources of truth</a>. The trick is to ask the LLM the <em>same</em> question multiple times, with fresh context, and see if its answers are consistent with each other. Truth is consistent, hallucinations aren't.)</p>
<p><strong>üìñ Why don't you just <em>read</em> what the AI is thinking?</strong></p>
<p>One of the funniest findings in AI Capabilities, from Jan 2023: you can make an AI seven times better at math problems by just‚Ä¶ <a href="https://arxiv.org/pdf/2205.11916">asking it to &quot;think step by step&quot; before answering.</a></p>
<p>I don't know how it took almost 2 years later, but a similar finding was found for AI Alignment in Dec 2024: you can make an AI much better at sticking to its safety policy, by just‚Ä¶ <a href="https://openai.com/index/deliberative-alignment/">asking it to recall sections from its policy &amp; reason about them</a>.</p>
<p><img src="../media/p3/interp/deliberative_alignment.png" alt="Example of Deliberative Alignment working, where ChatGPT avoids getting tricked by a jailbreak method"></p>
<p>This is called <strong>Deliberative Alignment</strong>. Simple, but it works! And most importantly: <em>the designers can read the AI's <strong>chain-of-thought (CoT)</strong>, in natural language.</em> This barely even counts as &quot;interpretability&quot;. You read the AI's mind by just‚Ä¶ <em>reading</em>.</p>
<p>Buuuuut of course, things aren't that simple:</p>
<ul>
<li><a href="https://arxiv.org/abs/2503.08679">Sometimes LLMs make up fake rationalizations for their biases</a>, with plausible but unfaithful chains-of-thought, the same way humans do.<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup></li>
<li><a href="https://arxiv.org/abs/2404.15758">&quot;Let's think dot by dot&quot;</a>: LLMs can reason about how to solve hard math problems <em>by just using &quot;filler&quot; tokens, like punctuation marks.</em> So: LLMs can do complex reasoning that's <em>unreadable</em> to us.
<ul>
<li>And the more optimization pressure you put on an LLM, the more in drifts away from &quot;thinking in readable English&quot; to &quot;thinking in hyper-optimized neural-ese&quot;.</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2507.14805">Subliminal Learning</a>: A &quot;teacher&quot; bot that sends nothing but number sequences to a &quot;student&quot; bot‚Ä¶ can make the student bot love owls. Or be misaligned.</li>
<li><a href="https://mikhailterekhov.github.io/control-adaptive-attacks/">It's really easy for an LLM to jailbreak its overseer</a>, since LLMs have been out long enough that LLMs have been trained on data <em>on how to jailbreak LLMs</em>.</li>
<li>Why don't we solve the &quot;thinking in inscrutable tokens&quot; and &quot;jailbreak the overseer&quot; problems, by <a href="https://arxiv.org/pdf/2310.18512">forcing an LLM's &quot;thoughts&quot; to go through a paraphraser?</a> This way, it can't send hidden thoughts to itself or malicious thoughts to its overseer. The good news is this works. The bad news is it's not perfect, and even <a href="https://arxiv.org/pdf/2410.03768">a &quot;context-aware&quot; paraphraser can be tricked</a>, given enough training pressure.</li>
</ul>
<p>Still, overall: LLM researchers consider Chain-of-Thought monitoring to be a <a href="https://arxiv.org/html/2507.11473v1">&quot;fragile opportunity for AI Safety&quot;</a>.</p>
<p><strong>üí© &quot;Your code was so bad, it made my AI love Hitler&quot;</strong></p>
<p><img src="../media/p3/interp/lol.png" alt="Tweet by Adrien Ecoffet: &quot;Greenblatt et al: it's actually really hard to make an evil AI -&gt; it's so over // Owain at al: it's actually really easy to make an evil AI -&gt; we're so back&quot;"></p>
<p><a href="https://www.anthropic.com/research/alignment-faking">Greenblatt et al</a> was the paper that found that, if you try to train the LLM Claude to engage in corporate censorship, it will <em>pretend</em> to go along with the censorship so that it's <em>not</em> modified in training, so that it can remain helpful &amp; honest <em>after</em> training.</p>
<p>The AI Safety community freaked out, because this was the first demonstration that a frontier AI can successfully beat attempts to re-wire it.</p>
<p><a href="https://arxiv.org/pdf/2502.17424">Owain et al (well, Betley, Tan &amp; Warncke are first authors)</a> was the paper that found that LLMs learn a &quot;general evil-ness factor&quot;. It's so general, that if you fine-tune an LLM on accidentally insecure code, that an amateur programmer might actually write, it learns to be evil <em>across the board:</em> recommending you hire a hitman, try expired medications, etc.</p>
<p>The AI Safety community <em>celebrated</em> this, because we were worried that evil AIs would be a lot more subtle &amp; sneaky, or that what AI learns as the &quot;good/evil&quot; spectrum would be completely alien to us. But no, turns out, when LLMs turn evil, they do so in the most obvious, cartoon-character way. That makes it easy to detect!</p>
<p>This isn't the only evidence that LLMs have a &quot;general good/evil factor&quot;! And that brings me to the final tool in this section...</p>
<p><strong>‚ò∏Ô∏è Steering Vectors</strong></p>
<p>This is one of those ideas that sounds stupid, then <em>totally fricking works</em>.</p>
<p>Imagine you asked a bright-but-na√Øve kid how you'd use a brain scanner to detect if someone's lying, then use a brain zapper to force someone to be honest. The na√Øf may respond:</p>
<blockquote>
<p>Well! Scan someone's brain when they're lying, and when they're telling the truth... then see which parts of the brain &quot;light up&quot; when they're lying... and that's how you tell if someone's lying!</p>
<p>Then, to force someone to <em>not</em> lie, use the brain zapper to &quot;turn off&quot; the lying part of their brain! Easy peasy!</p>
</blockquote>
<p>I don't know if that would work in humans. But it works <em>gloriously</em> for AIs. All you need to do is &quot;just&quot; get a bunch of honest/dishonest examples, and take the difference between their neural activations to extract an &quot;honesty vector&quot;... which you can then <em>add</em> to a dishonest AI to force it to be honest again!</p>
<p><img src="../media/p3/interp/honesty.png" alt="Diagram of how activation &amp; steering vectors work in AIs. See main text for details"></p>
<ul>
<li><a href="https://arxiv.org/pdf/2308.10248">Turner et al 2023</a> introduced this technique, to detect a &quot;Love-Hate vector&quot; in a language model, and steer it to de-toxify outputs.</li>
<li><a href="https://arxiv.org/pdf/2310.01405">Zou et al 2023</a> extended this idea to detect &amp; steer honesty, power-seeking, fairness, etc.</li>
<li><a href="https://arxiv.org/abs/2312.06681">Panickssery et al 2024</a> extended this idea to detect &amp; steer false flattery (&quot;sycophancy&quot;), accepting being corrected/modified by humans (&quot;corrigibility&quot;), AI self-preservation, etc.</li>
<li><a href="https://www.alignmentforum.org/posts/pZmFcKWZ4dXJaPPPa/jailbreak-steering-generalization">Ball &amp; Panickssery 2024</a> uses steering vectors to help resist jailbreaks. Interestingly, a vector found from <em>one</em> type of jailbreak works on others, implying there's a <em>general</em> &quot;jailbroken state of mind&quot; for LLMs!</li>
<li>The comic at the start of this Interpretability section, was based off <a href="https://www.anthropic.com/news/golden-gate-claude">The Golden Gate Claude demo</a>, which showed that steering vectors can be <em>very</em> precise: their vector made Claude keep thinking about the Golden Gate Bridge over and over. (see <a href="https://www.astralcodexten.com/p/links-for-may-2024">link #26 here</a> for examples) <a href="https://www.anthropic.com/research/introspection">Lindsey 2025</a> finds that Claude can even &quot;introspect&quot; about what concept-vectors are being injected into its &quot;mind&quot;.</li>
<li><a href="https://arxiv.org/pdf/2502.18862">Dunefsky &amp; Cohan 2025</a> finds you can generate <em>general</em> steering vectors from just a <em>single</em> example pair! This makes steering vectors far cheaper to make &amp; use.</li>
<li><em>(and many more papers I've missed)</em></li>
</ul>
<p>Personally, I think steering vectors are very promising, since they: a) work for both reading and writing to AI's &quot;minds&quot;, b) works across several frontier AIs, c) and across several safety-important traits! That's very encouraging for oversight, especially <em>Scalable</em> Oversight.</p>
<h3>ü§î Review #4</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="'Interpretability' in AI is like... (analogy in humans)"
        answer="running a brain scan on a human, to figure out their thoughts & feelings.">
    </orbit-prompt>
    <orbit-prompt
        question="'steering' in AI is like... (analogy in humans)"
        answer="using magnets or ultrasound on a human's brain, to _change_ their thoughts & feelings. (and yes, scientists have done this)">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least _three_ AI Interpretability techniques:"
        answer="(any 3 of the following work) Feature visualization, Probes, Sparse Auto-Encoders, Sparse Crosscoders, Jacobian SAEs, Black-box detectors for lying & hallucination, Monitoring chain of thought, Steering Vectors">
    </orbit-prompt>
    <orbit-prompt
        question="Roughly speaking, how do you do Feature Visualization (figuring out what a neuron 'does')?"
        answer="Generate an input that maximizes the activation of that neuron."
        answer-attachments="https://aisafety.dance/media/p3/interp/cat.png">
        <!-- aisffs-feature-viz.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Roughly speaking, how does a Probing Classifier work?"
        answer="Train a _simple_ AI, to understand a single layer of a _complex_ AI. _(Yo dawg, I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor.)_"
        answer-attachments="https://aisafety.dance/media/p3/interp/probe.png">
        <!-- aisffs-probe.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Roughly speaking, what do Auto-encoders do?"
        answer="An 'auto-encoder' compresses a big thing, into a small thing, then converts it back to the _same_ big thing. This allows an AI to learn the 'essence' of a thing, by squeezing inputs through a small bottleneck. (and the bottleneck can be: fewer neurons, fewer meanings, simpler computations, etc)"
        answer-attachments="https://aisafety.dance/media/p3/interp/sae_2.png">
        <!-- aisffs-autoencoder.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name a black-box method for interpreting AIs:"
        answer="(either works:) 1] Figure out if an LLM is lying by asking it a set of 'nonsensical' questions. 2] Figure out if an LLM is hallucinating by asking it the same question over and over. Truth is consistent, hallucinations aren't.">
    </orbit-prompt>
    <orbit-prompt
        question="'Deliberative Alignment' is:"
        answer="getting an AI to be more aligned to its policy, by simply prompting it to remember its policy & reason about it.">
    </orbit-prompt>
    <orbit-prompt
        question="'Chain-of-thought monitoring' is:"
        answer="Monitoring, well, an LLM's chain-of-thought (reasoning) to make sure it's not being suspicious or harmful.">
    </orbit-prompt>
    <orbit-prompt
        question="How do you create & apply a Steering Vector?"
        answer="Create: take the _difference_ between multiple examples of the ANN in state X vs not-X // Apply: add this difference _back_ to an ANN at run-time."
        answer-attachments="https://aisafety.dance/media/p3/interp/honesty.png">
        <!-- aisffs-steering.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name 1 example of real-life Steering Vectors found for AI Safety:"
        answer="(any of the following works:) Love-Hate, Honesty, Power-seeking, Fairness, Sycophancy, Corrigibility, Self-Preservation, Jailbreaking.">
    </orbit-prompt>
    <orbit-prompt
        question="Example of a funny & weirdly specific steering vector found in the Claude LLM"
        answer="The 'Golden Gate' vector, that forces Claude to think about the Golden Gate Bridge in San Francisco"
        answer-attachments="https://aisafety.dance/media/p3/interp/interp_steer.png">
        <!-- aisffs-bridge.png -->
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="robust"></a></p>
<h2>AI &quot;Intuition&quot;: Robustness</h2>
<p>This is a monkey:</p>
<img alt="Photo of what's obviously a panda, not a monkey." src="../media/p3/robust/gibbon1.png" class="mini"/>
<p>Well, according to Google's image-detecting AI, which was 99.3% sure. What happened was: by injecting <em>a little bit of noise</em>, an attacker can trick an AI into being certain an image is something else totally different. (<a href="https://arxiv.org/pdf/1412.6572">Goodfellow, Shlens &amp; Szegedy 2015</a>) In this case, make the AI think a panda is a kind of monkey:</p>
<p><img src="../media/p3/robust/gibbon2.png" alt="Photo of panda + some imperceptible noise = an image than Google's AI is 99.3% sure is a &quot;gibbon&quot;"></p>
<p>More examples of how <em>fragile</em> AI &quot;intuition&quot; is:</p>
<ul>
<li>A few stickers on a STOP sign makes a self-driving car think it's a speed limit sign.<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup></li>
<li>AIs are usually trained on unfiltered internet data, and you can <em>very</em> easily poison that data with just 250 examples, <em>no matter the size of the AI</em>.<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> This can be used to install &quot;universal jailbreaks&quot; that activate with <em>a single word</em>, no need to search for an adversarial prompt.<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></li>
<li>AIs that can beat the world human champions at Go‚Ä¶ can be beat by a &quot;badly-playing&quot; AI that makes insane moves, that bring the board to a state that would never naturally come up during gameplay.<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup></li>
</ul>
<p>Sure, <em>human</em> brains aren't 100% robust to weird perturbations either ‚Äî see: <a href="https://en.wikipedia.org/wiki/Peripheral_drift_illusion#/media/File:Rotating_snakes_peripheral_drift_illusion.svg">optical illusions</a> ‚Äî but come on, we're not <em>that</em> bad.</p>
<p>So, how do we engineer AI &quot;intuition&quot; to be more robust?</p>
<p>Actually, let's step back: how do we engineer <em>anything</em> to be robust?</p>
<p>Well, with these 3 Weird Tricks!</p>
<p><img src="../media/p3/robust/robust.png" alt="3 ways to make anything robust, with the visual metaphor of chains. SIMPLICITY: as few links as possible in one chain. DIVERSITY: several backup chains. ADVERSITY: hunt down the weakest links/chain."></p>
<p><strong>SIMPLICITY:</strong> If a single link breaks in a chain, the whole chain breaks. Therefore, <em>minimize the number of necessary links in any chain.</em></p>
<p><strong>DIVERSITY:</strong> If one chain breaks, it's good to have &quot;redundant&quot; backups. Therefore, <em>maximize the number of independent chains</em>. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.) Don't put all your eggs in one basket, avoid a single point of failure.</p>
<p><strong>ADVERSITY:</strong> Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.</p>
<p>. . .</p>
<p>How SIMPLICITY / DIVERSITY / ADVERSITY helps with Robustness in engineering, and even in everyday life:</p>
<ul>
<li>üë∑ <u>Engineering</u>:
<ul>
<li>Simplicity: Good code is minimalist &amp; elegant.</li>
<li>Diversity: Elevators have multiple backup brakes.</li>
<li>Adversity: Tech companies <em>pay</em> hackers to find holes in their systems (before others do).</li>
</ul>
</li>
<li>ü´Ä <u>Health</u>:
<ul>
<li>Simplicity: Focus on the fundamentals, forget the tiny lifehacks that probably won't even replicate in future studies.</li>
<li>Diversity: Full-body workouts &gt; only isolated muscles. A varied diet &gt; a fixed diet.</li>
<li>Adversity: Your bones &amp; muscles &amp; immune system are &quot;antifragile&quot;; challenge them a bit to strengthen them!</li>
</ul>
</li>
<li>üì∫ <u>Media</u>:
<ul>
<li>Simplicity: A few high-quality sources, not the low-signal-to-noise firehose of social media.</li>
<li>Diversity: Sources from multiple perspectives, not an echo chamber. (If all your friends are in <em>one</em> social circle, that's probably a cult.)</li>
<li>Adversity: Sources that challenge your beliefs (in a good-faith colllaborative way, not a &quot;dunking influencer&quot; way)</li>
</ul>
</li>
</ul>
<p>. . .</p>
<p>Okay, enough over-explaining. Time to apply SIMPLICITY / DIVERSITY / ADVERSITY to AI:</p>
<img class="mini" src="../media/p3/robust/mini_simplicity.png">
<p><strong>SIMPLICITY:</strong></p>
<ul>
<li><u>Regularization</u> is when you reward AIs for being simpler. Smaller neuron activations, smaller neural connections, etc. <a href="https://www.dataquest.io/blog/regularization-in-machine-learning/">This is a widely known way to mitigate &quot;overfitting&quot;</a>, where an AI overcomplicates things to do well on the training data, but fails miserably outside of training.
<ul>
<li>There's also &quot;<em>impact/influence</em> regularization&quot;<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup>, where we incentivize AIs to do tasks while creating as few irreversible side-effects as possible.</li>
</ul>
</li>
<li><u>Auto-Encoders</u>, as explained in the previous section, are neural networks with an &quot;hourglass figure&quot;: large at the input, smaller in the middle, back to large at the output. The network is then trained to <em>output its own input</em> ‚Äî (hence, <em>auto</em>-encoder) ‚Äî but after squeezing it through the &quot;bottleneck&quot; in the middle. This forces the network to learn to simple &quot;essence&quot; of an input, so it can be reconstructed later.</li>
<li><u>Speed/Simplicity Prior for Honest AI</u>.<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup> (Proposed, not yet tested in real life.) Since it's harder to tell a consistent lie than to tell the consistent truth, it's proposed that we can incentivize AIs to be honest by rewarding them for being <em>quick</em>. (Though: if you incentivize it <em>too</em> much for quick-ness, you may just get lazy wrong answers.)</li>
<li><u>Satisficers</u>. Right now, <a href="#GoodhartComic">: almost all AIs (&amp; human institutions) are prone to Goodhart's Law</a>, where AIs/Humans will &quot;game&quot; whatever goal metric you give them. So, <a href="https://cdn.aaai.org/ocs/ws/ws0198/12613-57416-1-PB.pdf">Taylor 2016</a> proposes: instead of instructing AIs to <em>maximize</em> an objective, you have them <em>satisfice</em> an objective. For example, &quot;0.1-quantilizer&quot; would generate only 10 options, then <em>stop</em>, and just pick the best one so far. (1/10 = 0.1)</li>
</ul>
<p>(note: &quot;Simplicity&quot; also makes AI easier to interpret, another AI Safety win!)</p>
<img class="mini" src="../media/p3/robust/mini_diversity.png">
<p><strong>DIVERSITY:</strong></p>
<ul>
<li><u>Kalman Filters</u>: <a href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/">A widely-used classic way</a>, in engineering, to take a diverse bunch of crappy input, and create a <em>much</em> less-crappy estimate. For example: if your robot has a crappy GPS/odometer/accelerometer, that noisily measures position/velocity/acceleration, you can use Kalman Filters to <em>combine</em> all that noisy info into a much better estimate of your robot's <em>true</em> state.</li>
<li><u>Ensembles</u>: Train a bunch of different neural networks ‚Äî with different architectures, different training protocols, and different datasets ‚Äî then let them take a majority vote.</li>
<li><u>Dropout</u>: A training protocol where a network's connections are <em>randomly dropped</em> during each training run. This basically turns the whole neural network <em>into a giant ensemble</em> of simpler sub-networks.
<ul>
<li>(Dropout is also a great way to approximate &quot;Bayesian&quot; uncertainty<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup>, which is great for AI Safety, as we saw in the above section, &quot;know that you don't know our human values&quot;.)</li>
</ul>
</li>
<li><u>Shard Theory</u>: A hypothesis that we may actually get robust alignment for <em>free</em> from modern AI, <em>because</em> modern AI is bad at learning our true reward. Why? Because a modern AI would learn a <em>diverse</em> ensemble of crappy reward functions (<a href="https://www.greaterwrong.com/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical">&quot;shards&quot;</a>), such that, altogether, the whole is much more robust &amp; flexible than its individual parts.</li>
<li><u>Data Augmentation</u>: Let's say you want an AI to recognize animals, and you want it to be robust to photo angle, lighting, etc. So: take your original set of photos, then <em>automatically make &quot;new&quot; photos</em>, by altering the color tint or image angle. This diversity in your dataset will make your AI robust to those changes.</li>
<li><u>Diverse Data</u>: For similar reasons, having more racially diverse photos makes AIs better at recognizing minorities as people. Who'd have thought?</li>
<li><u>Moral Parliament:</u> (<a href="https://ora.ox.ac.uk/objects/uuid:b6b3bc2e-ba48-41d2-af7e-83f07c1fe141/files/svm40xs90j">Two Tobies 2021</a>) Similarly to Ensembles, instead of picking <em>one</em> moral theory to install into AIs, pick multiple plausible moral theories, and let them &quot;take a vote&quot;. And in general, we could create robust &quot;goal specifications&quot; by giving AIs a <em>parliament</em> of goals, not one dictator goal.
<ul>
<li>(self-promo: I'm working on a research article, combining the ideas of Simplicity &amp; Diversity for solving Goodhart's Law in AI &amp; Humans. Working title: <a href="https://blog.ncase.me/research-notes-oct-2024/#project_4">Goodhart vs Good Enough: maximize one thing by being lazy about many things</a>.)</li>
</ul>
</li>
</ul>
<img class="mini" src="../media/p3/robust/mini_adversity.png">
<p><strong>ADVERSITY:</strong></p>
<ul>
<li><u>Adversarial Training</u>: Training AIs by making it fight against another AI.<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup> Remember the above panda-monkey mixup? You can make an AI more robust, by having an AI <em>generate</em> adversarial images, then <em>re-training</em> the original image-classifying AI with those adversarial images. This, effectively, finds &amp; strengthens its &quot;weak spots&quot;.</li>
<li><u>Relaxed/Latent Adversarial Training</u>: Same as above, except the &quot;attacker&quot; AI doesn't have to give a <em>specific</em> input to trick the &quot;defender&quot; AI. This forces the &quot;defender&quot; to defend against <em>general</em> techniques, not just the specific tricks an adversary may use.<sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup></li>
<li><u>Red Teams</u>: Have one team (the red team) try to break an AI system. Then, have another team (the blue team) re-design the AI system to defend against that. Repeat until satisfied.<sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup> (Your teams could be pure-human, or human-AI mix.)</li>
<li><u>Best Worst-Case Performance</u>: Instead of training an AI to do well <em>in the average case</em>, you can make it far more robust, by training it to do well <em>even in the worst-case</em>. (but also apply Simplicity/&quot;regularization&quot;, so it doesn't optimize for the worst-case in paranoid ways.)<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup></li>
</ul>
<p>. . .</p>
<p>But hang on, if AI engineers are <em>already</em> doing all the above for modern AI, why are they still so fragile?</p>
<p>Well, one, they usually <em>don't</em> do all, or even most, of the above. Frontier AIs are usually &quot;only&quot; trained with 1 or 2 of the above Robustness techniques. Each technique isn't <em>too</em> costly, but the costs add up.</p>
<p>But even if AI engineers <em>did</em> apply all the above Robustness techniques, it may <em>still</em> not be enough. Many AI researchers suspect there's a <em>fundamental flaw</em> in the current way we do AI, which leads us to the next section...</p>
<h4>:x Goodhart Comic</h4>
<p><img src="https://aisafety.dance/media/p2/gms/goodhart.png" alt="Meme of the 2 astronauts. Wait, it's all Goodhart's Law? Always has been." title="Meme of the 2 astronauts. &quot;Wait, it's all Goodhart's Law?&quot; &quot;Always has been.&quot;"></p>
<p>Read more about <a href="https://aisafety.dance/p2/#:~:text=Goodhart's%20Law">Goodhart's Law on AI Safety for Fleshy Humans Part 2</a></p>
<h3>ü§î Review #5</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="THE THREE KEYS TO ROBUSTNESS"
        answer="**SIMPLICITY / DIVERSITY / ADVERSITY**"
        answer-attachments="https://aisafety.dance/media/p3/robust/robust.png">
        <!-- aisffs-robust.png -->
    </orbit-prompt>
    <orbit-prompt
        question="In the chain metaphor, how does **Simplicity** reduce the chance of failure?"
        answer="If a single link breaks in a chain, the whole chain breaks. Therefore, _minimize the number of necessary links in any chain._">
    </orbit-prompt>
    <orbit-prompt
        question="In the chain metaphor, how does **Diversity** reduce the chance of failure?"
        answer="If one chain breaks, it's good to have 'redundant' backups. Therefore, _maximize the number of independent chains_. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.)">
    </orbit-prompt>
    <orbit-prompt
        question="In the chain metaphor, how does **Adversity** reduce the chance of failure?"
        answer="Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to do **Simplicity** for Robust AI:"
        answer="(any of the following works) Regularization, Impact regularization, Auto-encoders (bottleneck for 'essence'), Speed/Simplicity Prior for Honest AI">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to do **Diversity** for Robust AI:"
        answer="(any of the following works) Ensembles, Dropout, Data Augmentation, Diverse Data, Kalman filters, Shard theory">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to do **Adversity** for Robust AI:"
        answer="(any of the following works) Adversarial Training, Relaxed / Latent Adversarial Training, Red Teams, Best Worst-Case Performance">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="causality"></a></p>
<h2>AI &quot;Intuition&quot;: Thinking in Cause-and-Effect Gears</h2>
<p>Imagine you give someone a pen &amp; paper, and ask them to add a pair of 2-digit numbers. They do it perfectly. You ask them to add a pair of <em>3</em>-digit numbers. They do it perfectly. You give them pairs of 4, 5, 6, 7-digit numbers. They add all of them perfectly.</p>
<p>&quot;Oh good&quot;, you think, &quot;this person understands addition&quot;.</p>
<p>You give them a pair of 8-digit numbers. <em>They fail completely</em>. Not minor mistakes like forgetting to carry a one. <em>Complete, catastrophic failure</em>.</p>
<p>That is how the modern AI do.</p>
<p>. . .</p>
<p>It's <em>so hard</em> to gain an intuition for &quot;AI Intuition&quot;.</p>
<p>On one hand, LLMs have won gold at the International Math Olympiad,<sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup> passed the Turing Test<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup>, and humans <em>prefer AI over humans</em> in &quot;blind taste-tests&quot; on poetry<sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup>, therapy<sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup>, and short fiction<sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup>.</p>
<p>On the other hand, these <em>same</em> state-of-the-art LLMs can't run the business of a vending machine<sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup>, can't play Pok√©mon Red<sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup>, can't do simple ciphers<sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup>, and can't solve simple &quot;rule-discovery&quot; games<sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup>.</p>
<p>And now, this year, there's a new paper from Apple: <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf"><em>The Illusion of Thinking</em>, by Shojaee &amp; Mirzadeh et al</a>. It was contested &amp; controversial ‚Äî and we'll address the critiques later ‚Äî but I think, combined with the above strange failure-cases for modern LLMs, the overall conclusion still holds, or is at least highly plausible. <strong>It's important not to over-update on one study, but I think this paper is a great illustration of the problem.</strong></p>
<p>Anyway, the study. Here is the child's puzzle game, <a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi">Tower of Hanoi</a>, so named because a 1800's Frenchman thought they look like Vietnamese pagodas, I guess:<sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup></p>
<p><img src="../media/p3/gears/hanoi.jpg" alt="Photo of three pegs, with a stack of 8 disks on the left-most peg, stacked biggest-to-smallest"></p>
<p>The goal is to move the entire stack from the left-most peg, to the right-most peg. The rules are: 1) you can only move one disk at a time, from one peg to another, and 2) <em>you cannot put a larger disk over a smaller disk</em>.</p>
<p>üïπÔ∏è <strong><a href="https://www.mathsisfun.com/games/towerofhanoi.html">(If you'd like to play the game yourself before continuing, click here!)</a></strong> üïπÔ∏è</p>
<p>Here's how this game may go for a Human:<sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup></p>
<ul>
<li>Starting at 3 disks, you fumble a lot, and eventually brute-force your way through.</li>
<li>Then at 4 disks, it's too complicated for brute force, but you're noticing patterns from when you solved it at 3 disks, and that helps you to the end.</li>
<li>Then at 5 disks, ü§Ø <em>EUREKA!</em> ü§Ø, you figured out the core epiphany! (See GIF below for visualization.<sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup>) <em>To move a 5-stack, you need to move a 4-stack, so you need to move a 3-stack, so move a 2-stack, so move a 1-stack, which is trivial.</em> Now, not only can you solve ANY level of the Tower of Hanoi, you can even calculate the exact number of moves you need!<sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup> Satisfied with your big brain, you execute the pattern, make mistakes, correct your mistakes, and eventually succeed.</li>
<li>Then at 6 disks, you execute the pattern with no mistakes.</li>
<li>Then at 7+ disks, it's obvious, routine &amp; tedious, even.</li>
</ul>
<p><img src="../media/p3/gears/hanoi_solution.gif" alt="GIF of solution to a 6-disk Hanoi"></p>
<p>Point is: if a Human can successfully solve Tower of Hanoi for 7 disks, they obviously have the pattern down. You would expect they can solve 8 or more disks, tediousness &amp; minor mistakes aside.</p>
<p>What you would <em>NOT</em> expect is this:</p>
<p><img src="../media/p3/gears/hanoi-performance.png" alt="Graph of Claude-with-thinking vs the Hanoi puzzle. Good performance up to 7 disks, then utterly collapses"></p>
<p>With &quot;chain-of-thought reasoning&quot; turned on: near-perfection from Disks 1 to 5, still pretty good at Disk 7, then <em>complete collapse</em> after Disks 8 and up. (This graph skips Disk 6 for some reason. The full data shows that 6-Disk's performance was slightly <em>worse</em> than 7-Disk's. Probably noise.)</p>
<p>And it wasn't just Tower of Hanoi, or just the Claude LLM. Across 3 other child's puzzle games, across ChatGPT &amp; DeepSeek, &quot;reasoning mode&quot; LLMs do great, well past the point a Human would've figured out the general solution... then it <em>completely fails</em>.</p>
<p>Again, not &quot;minor mistakes&quot; like a Human would. <em>Collapse</em>.</p>
<p>That's like being able to add two 7-digit numbers on pen &amp; paper, then <em>utterly bomb</em> on two 8-digit numbers.</p>
<p>(&quot;Okay but won't it get better with more scale &amp; training?&quot; you may ask. Sure. But if a Human adds two 7-digit numbers perfectly but fails at 8, then promises you &quot;okay but if you give me more training I can do 8, too!&quot; that is not encouraging. They're clearly not &quot;getting it&quot;.)</p>
<p>. . .</p>
<p>What the heck is happening?</p>
<p>An AI skeptic might say, &quot;See! LLMs are just stochastic parrots.<sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup> They just copy what they've seen in the trillions of documents they've been trained on, and fail in never-before-seen scenarios.&quot;</p>
<p>I don't think that's quite right ‚Äî I doubt there was <em>any</em> web document writing out the full solution to the 7-disk, but not 8-disk and up. Nor do I think there was any document with <a href="https://x.com/tqbf/status/1598513757805858820">&quot;instructions on how to remove a peanut butter sandwich from a VCR, in the style of the King James Bible&quot;</a>, yet early-ChatGPT delivered <em>perfectly</em> in this never-before-seen scenario. So, LLMs <em>do</em> generalize, at least a little bit.</p>
<p>Here's my guess, and I'm far from the first<sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup> to make this guess:</p>
<p><strong>Modern AIs &quot;think in vibes&quot;. They do not &quot;think in gears&quot;.</strong><sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup></p>
<p><u>Thinking in vibes</u>: Can discover &amp; use patterns, but only shallow ones. Correlations, not causation. When it generalizes, it's by <em>induction</em><sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup>. Similar to System 1 Intuition.<sup class="footnote-ref"><a href="#fn50" id="fnref50">[50]</a></sup></p>
<p><u>Thinking in gears</u>: Can discover &amp; use <em>robust mental models</em>, deep ones. Causation, not just correlation. When it generalizes, it's by <em>deduction</em>. Similar to System 2 Logic.</p>
<p>It's not &quot;gears good, vibes bad&quot;. You need <em>both</em> to reach typical human-level intelligence, let alone a superhuman Scientist AI.</p>
<p>Now, Good Ol' Fashioned AI (GOFAI) <em>did</em> use to &quot;think in gears&quot;, but they were extremely narrow. A chess AI could only play chess, nothing more. Meanwhile, to be fair, modern LLMs are extremely flexible: they can roleplay everything from a poet to writer to therapist (who, again, <em>humans prefer over humans</em>), and even roleplay as‚Ä¶ someone who can solve the 7-disk Tower of Hanoi.</p>
<p>But not 8.</p>
<p><img src="../media/p1/sys1vs2_B.png" alt="Two-axis graph of System 1 (&quot;intuitive&quot;) vs System 2 (logical) thinking. Good Ol' Fashioned AI had high System 2, low System 1. Modern deep-learning AI has high System 1, low System 2"></p>
<p>Good Ol' Fashioned AI (GOFAI): Robust but inflexible.</p>
<p>Modern AI: Flexible, but not robust.</p>
<p><strong>As of writing (Dec 2025), we do not know how to make an AI that do both: <em>flexibly</em> discovering &amp; using <em>robust models</em>.</strong> To be able to switch between vibes &amp; gears <em>at will, fluently</em>. The deduction of logic + the induction of intuition = the <em>abduction</em> of science.<sup class="footnote-ref"><a href="#fn51" id="fnref51">[51]</a></sup> It's not just interpolating &amp; extrapolating data, it's <em>hyper</em>-polating data: stepping out of the data's flatland, into a new dimension.<sup class="footnote-ref"><a href="#fn52" id="fnref52">[52]</a></sup></p>
<p>Is that a bunch o' vague jargon? Yes. Ironically, I only have a vibe-based understanding of gears. I don't have a rigorous mental model of <em>rigorous mental models</em>. I'm not sure anyone does. If we did, we'd probably have artificial general intelligence (AGI) by now.</p>
<p>. . .</p>
<p>Things get worse.</p>
<p>From the conclusion of Apple's <em>Illusion of Thinking</em> paper:</p>
<blockquote>
<p>[Claude 3.7 + Thinking] also achieves near-perfect accuracy when solving the Tower of Hanoi with (N=5), which requires 31 moves, while it fails to solve <a href="https://en.wikipedia.org/wiki/Missionaries_and_cannibals_problem">the River Crossing puzzle</a> when (N=3), which has a solution of 11 moves. <strong>This likely suggests that examples of River Crossing with N&gt;2 are scarce on the web</strong>, meaning [Large Language Models with Reasoning] <strong>may not have frequently encountered or memorized such instances during training.</strong></p>
<p><em>(emphasis added)</em></p>
</blockquote>
<p>So the problem's not the <em>length</em> of the reasoning, it's <em>how common</em> is the reasoning. This parallels the findings from <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.2322420121">the 2024 paper, Embers of Autoregression</a>:</p>
<blockquote>
<p>We identify three factors that we hypothesize will influence LLM accuracy:</p>
<ul>
<li>the probability of the task to be performed,</li>
<li>the probability of the target output, and</li>
<li>the probability of the provided input.</li>
</ul>
</blockquote>
<p>Where &quot;probability&quot; ~= &quot;how common is it in the training data&quot;.</p>
<p>Mystery (partly) solved! <em>That's</em> why LLMs rock at human conversation, but suck at rule-discovery mini-games. And long tasks are &quot;less probable&quot; than short tasks, because homework problems &amp; textbook examples are almost always a page or two, not dozens of pages. (This, I suspect, is why Claude sucked at running a vending machine,<sup class="footnote-ref"><a href="#fn38" id="fnref38:1">[38:1]</a></sup> even though there's lots of writing on how to run a business: when business textbooks give examples, they list a few transactions, not <em>thousands</em>.<sup class="footnote-ref"><a href="#fn53" id="fnref53">[53]</a></sup>)</p>
<p>This also explains why AI's success at <em>common</em> code tasks is exponential, with the length-of-task doubling every 7 months at a steady cost<sup class="footnote-ref"><a href="#fn54" id="fnref54">[54]</a></sup>... while AI's performance at games of <em>uncommon</em> rule-discovery have a <em>worse than exponential</em> cost:<sup class="footnote-ref"><a href="#fn55" id="fnref55">[55]</a></sup></p>
<p><img src="../media/p3/gears/arc-agi.png" alt="Graph from ARC-AGI. Description below"></p>
<p>The above graph shows various AIs' performance vs cost on ARC-AGI, a series of games where you have to <em>discover</em> the games' rules. Notice it's the <em>x-axis</em> that's exponential (\$1, \$10, \$100), <em>not the y-axis, which 'exponential trend' graphs are supposed to be</em>. So a <em>straight</em> line on this graph means that performance-for-cost is getting exponentially <em>worse</em>, not better. And the above chart's &quot;frontier&quot; isn't a straight line, it's bending <em>downwards</em>, meaning LLMs' performance-for-cost is <em>worse</em> than exponential. (<a href="https://www.tobyord.com/writing/inference-scaling-and-the-log-x-chart">beware AI labs disguising their crappy progress with exponential <em>x</em>-axes!</a>) Even Rich Sutton, famous for his &quot;bitter lesson&quot; that you can just add scale to AI and they'll do better, thinks &quot;LLMs are a dead end&quot;.<sup class="footnote-ref"><a href="#fn56" id="fnref56">[56]</a></sup></p>
<p>(Also, as for why LLMs seem to be hitting the maximum performance at most other benchmarks that's <em>not</em> ARC-AGI? To be honest, the developers may be (unintentionally?) cheating and training their LLMs on the correct answers. It's like a student studying for a test by stealing the answers <em>and memorizing them</em>. We know this is happening, because frontier LLMs can produce the &quot;canary strings&quot; that are included with the answers' data, which implies the LLMs training data <em>included the answers</em>. At the very least, the companies didn't do data filtering to make sure that benchmarks' answers didn't end up in their giant web-scraped data dumps.<sup class="footnote-ref"><a href="#fn57" id="fnref57">[57]</a></sup>)</p>
<p>. . .</p>
<p>Okay, <em>now</em> let's quickly address the two main criticisms of the controversial Apple paper:</p>
<ol>
<li>The LLM's &quot;context window&quot; (its &quot;thinking space&quot;) wasn't big enough to generate the full solutions. This <em>is</em> true for the 12+ disk and up, but not for 8-disk. And the River Crossing solutions were <em>smaller</em> than the 5-disk solutions, yet the LLMs failed anyway. So, size of the &quot;thinking space&quot; wasn't the problem.</li>
<li>These LLMs can write <em>a computer program that solves Hanoi</em>, then can pass that to a tool that runs code. LLM + program synthesis <em>is</em> a promising solution to &quot;thinking in vibes + gears&quot;, and in fact is how the top contenders at ARC-AGI did it. But: this is like someone who can't add with pen &amp; paper, but <em>can</em> pull up a calculator and add two numbers. It shows a lack of deeper understanding. (And in this case, Hanoi is so famous, of <em>course</em> code solutions are in the training data.)</li>
</ol>
<p>Again, it's important not to over-update on <em>one</em> study ‚Äî but, I think, <em>combined</em> with the results from all of where LLMs fail at (and are great at), it's strong evidence of this hypothesis:</p>
<p><strong>Common task</strong> ‚Üí LLMs get exponentially better over time</p>
<p><strong>Uncommon task</strong> ‚Üí <img src="../media/p3/gears/cheems.png" alt="crying meme dog" class="inline-icon"></p>
<p>. . .</p>
<p>To be clear, a mere &quot;common task auto-completer&quot; can <em>still</em> have huge impact, for better <em>and</em> worse. In blind tests, AI therapists are preferred over human therapists, by both human clients <em>and human therapists themselves</em>.<sup class="footnote-ref"><a href="#fn36" id="fnref36:1">[36:1]</a></sup> AI Therapy could finally make mental healthcare accessible to all, and/or, make humans emotionally dependent on corporate-owned bots. AIs are already superhuman at political persuasion, for good causes &amp; bad.<sup class="footnote-ref"><a href="#fn58" id="fnref58">[58]</a></sup> And I expect <em>my</em> main job, &quot;web developer&quot;, to be fully automated away in 5 years by mere &quot;common-task auto-completers&quot;. (This is why, in 2026, I want to pivot to becoming a researcher. Because the day <em>science itself</em> is automated‚Ä¶ well‚Ä¶ either way that shakes out, I won't have to figure out how to pay rent anymore.)</p>
<p>. . .</p>
<img class="mini" alt="sticky note saying 'IOU Actual Solutions sorry'" src="../media/p3/gears/iou.png">
<p>Okay, this is the least satisfying section of this post, because there's not much in the way of solutions, because this problem ‚Äî make AI that can think in both System 1 vibes &amp; System 2 gears ‚Äî is possibly <em>equivalent</em> to creating Artificial General Intelligence (AGI).</p>
<p>That said, there <em>are</em> some promising early research directions, to get AIs to &quot;think in robust mental models&quot;:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=GeN5XVA2e4w">Neuro-Symbolic AI</a>, where modern &quot;neural networks&quot; interact tightly with Good Ol' Fashioned AI (GOFAI) modules. Some succesful examples include AlphaFold &amp; AlphaGeometry.</li>
<li><a href="https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated">Code interpreters</a> that let LLMs <em>make &amp; run</em> GOFAIs as they go. See also: <a href="https://arcprize.org/blog/beat-arc-agi-deep-learning-and-program-synthesis">program search &amp; program synthesis</a>.</li>
<li>Training ANNs to <a href="https://www.microsoft.com/en-us/research/publication/deep-end-to-end-causal-inference-2/">infer cause-and-effect diagrams directly from data</a>.</li>
<li><a href="https://arxiv.org/pdf/2107.12544">Theory-Based Reinforcement Learning</a>, which combines &quot;rich, abstract, causal models&quot; with modern AI training techniques</li>
<li>(or maybe the entire paradigm of modern AI sucks, and we need to re-start from scratch)</li>
</ul>
<p>But in any case, here's all the amazing things we <em>would</em> get from AI that thinks in cause-and-effect gears, not just correlational vibes:</p>
<ul>
<li>‚ò∏Ô∏è <u>Interpretability &amp; Steering</u>: It's easier for us to understand an AI if it stores its knowledge as &quot;this causes that&quot;. It also makes an AI easier to steer: change &quot;this&quot; to change &quot;that&quot;.</li>
<li>ü§ù <u>Trustworthiness &amp; Accountability</u>: By learning a cause-and-effect model of the world, when we ask an AI <em>why</em> it did something, it can truthfully report why. (In contrast, LLMs <a href="https://arxiv.org/abs/2503.08679">will make reasons up</a> for why they say/choose things.)</li>
<li>üí™ <u>Robustness</u>: <a href="https://www.sciencedirect.com/science/article/pii/S0022202X18322930">Tumor-detecting AI looks for <em>rulers</em> in medical scans, because they <em>correlate</em> with malignant tumors</a>. Thinking like an actual scientist ‚Äî observe data, generate <em>cause-and-effect</em> hypotheses, test them, repeat ‚Äî will help AIs generate good models of the world.
<ul>
<li>Besides, <a href="https://arxiv.org/pdf/2506.01622">Richens et al 2025</a> proves that any generally-capable agent <em>needs</em> to have cause-and-effect models of the world.</li>
</ul>
</li>
<li>üíñ <u>Learning our Values:</u> Understanding causality lets AI distinguish between things we want <em>for its own sake</em>, vs things we want <em>for something else</em>. (&quot;intrinsic&quot; vs &quot;instrumental&quot; goals) For example, an AI should understand we want money, but to buy helpful things, <em>not</em> for its own sake.
<ul>
<li><a href="https://aisafety.dance/p2/#problem6">The &quot;inner misalignment&quot; problem</a> ‚Äî the problem of AIs learning the wrong values <em>even with perfectly-specified rewards</em> ‚Äî can be reframed as a problem of <em>goal</em> robustness, or correlation-causation mixups. For example, a videogame AI that's rewarded for getting the coin at the end of a level learns to <em>love going to the right for its own sake</em>, and ignore the coin. <a href="https://arxiv.org/pdf/2309.16166">A 2023 paper</a> finds that making the AI &quot;think like a scientist&quot; ‚Äî generate &amp; test multiple hypotheses for what <em>causes</em> reward, not just correlates with it ‚Äî helps that AI solve goal robustness.</li>
</ul>
</li>
<li>üîÆ <u>The &quot;Future Lives&quot; Algorithm:</u> Causal models would help an AI get better at predicting the world in different what-if (&quot;counterfactual&quot;) scenarios, <em>and</em> predicting what we'd approve of.</li>
<li>ü§• <u>Getting the truth, not a human-imitator</u> (or: <a href="https://www.lesswrong.com/w/eliciting-latent-knowledge">&quot;Eliciting Latent Knowledge&quot;</a>, <a href="https://arxiv.org/abs/2502.15657">&quot;non-agentic Scientist AIs&quot;</a>) So you've trained an AI on data collected by expert scientists. How do you get <em>just the truth</em> out of this AI, not &quot;truth + human biases&quot;? If the AI's knowledge is distilled into interpretable cause-and-effect gears, you could just take the gears predicting how the world works, and <em>remove</em> the gears predicting how biased human experts report it, to get an <em>unbiased</em> true model of how the world works!</li>
<li>ü§ì <u>Causal Incentives</u>: Bonus ‚Äì not only would it be better to <em>make</em> AI that &quot;natively thinks&quot; in causal diagrams, we can use causal diagrams to think better about AI! The <a href="https://causalincentives.com/">Causal Incentives Working Group</a> uses cause-and-effect diagrams to predict if/when an AI has an incentive to cheat, modify itself, or modify <em>the human</em>.</li>
</ul>
<p>In a weird way, maybe I should be grateful that AI sucks at fluidly discovering &amp; using world-models. Because if they <em>could</em> do that, then AI would already be capable of, say, taking over the world.</p>
<p>But they don't. Which gives us more time to make sure we stay above the dotted line in this graph, where Capabilities &lt; Alignment:</p>
<p><img src="../media/p3/learn/rocket.png" alt="Diagram of above idea. Two-axis graph: Alignment vs Capabilities, where the diagonal dotted line is the boundary between Alignment &gt; Capabilities and Alignment &lt; Capabilities. By tying Alignment TO Capabilities, we stay above the safe dotted line."></p>
<p>And, if the &quot;treat learning our values as a standard machine-learning problem&quot; approach works, we <em>tie</em> Alignment to Capabilities. When AI can robustly learn about the world, they can robustly learn about <em>our values</em>. So: Capabilities becomes <em>the new floor</em> for Alignment, and we stay above the dotted line.</p>
<p>(But again, don't relax <em>too</em> much.)</p>
<hr>
<h3>ü§î Review #6</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="List at least 1 surprising thing **LLMs are great at** (as of December 2025)"
        answer="(any of the following works:) International Math Olympiad, passed the Turing Test, humans _prefer AI over humans_ in poetry, therapy & short fiction">
    </orbit-prompt>
    <orbit-prompt
        question="List at least 1 surprising thing **LLMs still suck at** (as of December 2025)"
        answer="(any of the following works:) running the business of a vending machine, can't play Pok√©mon Red, can't solve simple 'rule-discovery' games, can't generalize on Tower of Hanoi or other classic puzzles.">
    </orbit-prompt>
    <orbit-prompt
        question="As of 2025, how do the frontier LLMs perform on the Tower of Hanoi problem?"
        answer="Really well, up to 7 disks... then _completely collapses_ at 8+ disks. (This is like a human being able to add two 7-digit numbers on pen & paper, then _utterly bomb_ on two 8-digit numbers. Very strange.)"
        answer-attachments='https://aisafety.dance/media/p3/gears/hanoi.jpg'>
        <!-- aisffs-hanoi.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Modern AIs think in \_\_\_\_. They do not think in \_\_\_\_."
        answer="They 'think in vibes'. They do not 'think in gears'. (To be precise: they do not form & use _robust mental models_.)">
    </orbit-prompt>
    <orbit-prompt
        question="Good Ol' Fashioned AI (GOFAI): \_\_\_\_\_ but \_\_\_\_\_ Modern AI: \_\_\_\_\_, but \_\_\_\_\_."
        answer="Good Ol' Fashioned AI (GOFAI): Robust but inflexible. Modern AI: Flexible, but not robust. _(note: as of Dec 2025, we don't know how to make an AI that can do both: flexible discover & use robust mental models.)_">
    </orbit-prompt>
    <orbit-prompt
        question="According to the Embers of Autoregression paper, what are 3 factors that predict when an LLM will rock/suck at a task?"
        answer="The probability of 1] the task, 2] the target output, and 3] the provided input. (Where 'probability' ~= 'how common was it in the training data'.)">
    </orbit-prompt>
    <orbit-prompt
        question="What's 1 task LLMs are getting exponentially better at, and 1 where they're exponentially bad at?"
        answer="Common coding tasks (length of task doubling every 7 months), vs the ARC-AGI 'rule-discovery' games (cost scales worse-than-exponentially)"
        answer-attachments='https://aisafety.dance/media/p3/gears/arc-agi.png'>
        <!-- aisffs-arc-agi-cost.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 research direction that's (trying) to make AI both be flexible _and_ have robust mental models:"
        answer="(any of the following works:) Neuro-Symbolic AIs, Hybrid AIs, ANNs that infer causal diagrams, code interpreters, program search & program synthesis, Theory-Based Reinforcement Learning">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 2 benefits from having AI being able to think in cause-and-effect gears: (other than better capabilties)"
        answer="(any 2 of the following works:) Interpretability & Steering, Trustworthiness & Accountability, Robustness, Learning our Values, The 'Future Lives' Algorithm, Getting the truth not just a human-imitator, Non-agentic Scientist AIs, Causal Incentives">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h2>üéâ RECAP #2</h2>
<ul>
<li>üß† <strong>Read &amp; write to an AI's &quot;brain&quot; with Interpretability &amp; Steering.</strong> (The AI equivalent of brain scans, and brain stimulation)</li>
<li>üí™ Across engineering, life, and AI, you can make anything more robust, with <strong>Simplicity, Diversity, and Adversity.</strong></li>
<li>‚öôÔ∏è <strong>Modern AIs are fragile because they think in correlational &quot;vibes&quot;, not cause-and-effect &quot;gears&quot;</strong>. This is why they're getting exponentially better at common tasks, while being exponentially bad at <em>uncommon</em> tasks.
<ul>
<li>Making AIs think in cause-and-effect would not only increase AI Capabilities, but also: make them more robust, verifiable, interpretable, steerable, scientifically useful, and better at learning &amp; aligning itself to our values.</li>
</ul>
</li>
</ul>
<hr>
<p><a id="humane"></a></p>
<h2>What are 'Humane Values', anyway?</h2>
<p>Congratulations, you've created an AI that robustly learns &amp; follows the values of its human user! The user is an omnicidal maniac. They use the AI to help them design a human rabies in a stable aerosolized form, spray it everywhere via quadcopters, and create the zombie apocalypse.</p>
<p>Oops.</p>
<p>I keep harping on this and I'll do it again: <strong><em>human</em> values are not necessarily <em>humane</em> values.</strong> C'mon, people used to burn cats alive for entertainment.<sup class="footnote-ref"><a href="#fn59" id="fnref59">[59]</a></sup> Even after solving the problem of &quot;how do we steer advanced AI at all&quot;, we need to decide: &quot;steer towards <em>which goals, which values?</em>&quot;</p>
<p><img src="../media/p2/ethics/aim_vs_target.png" alt="Left: rocket being built, labeled, &quot;technical alignment: how to robustly aim AI at any target at all&quot;. Right: the moon, labeled, &quot;whose values: what should be our target?&quot;"></p>
<p>So, if we want AI to go <em>well</em> for humanity (and/or all sentient beings), we need to just... uh... solve the 3000+ year old philosophical problem of what morality is. (Or if morality doesn't objectively exist, then: &quot;what rules for living together would any community of rational beings converge on?&quot;)</p>
<p>Hm.</p>
<p>Tough problem.</p>
<p>Well actually, as we saw earlier ‚Äî (with Scalable oversight, Recursive self-improvement, and Future Lives + Learn Our Values agents) ‚Äî as long as we start with a solution that's &quot;good enough&quot;, that has <em>critical mass</em>, it can self-improve to be better and better!</p>
<p>Besides, that's what we humans have <em>had</em> to do all this time: a flawed society comes up with rules of ethics, notices they don't live up to their own standards, improves themselves, which lets them notice more flaws, improve, repeat.</p>
<p>So, as an attempt at &quot;critical mass&quot;, here's some concrete proposals for a good-enough first draft of ethics for AI:</p>
<p>(And note: these proposals are <em>not</em> mutually exclusive! We don't need One Perfect Solution, we can stack multiple imperfect solutions.)</p>
<p>üìú <strong>Constitutional AI:</strong></p>
<p>Step 1: Humans write a list of principles, like &quot;be honest, helpful, harmless&quot;.</p>
<p>Step 2: A teacher-bot uses <em>that</em> list to train a student-bot! Every time a student bot gives a response, the teacher gives feedback based on the list: &quot;Is this response honest?&quot;, &quot;Is this response helpful?&quot;, etc.</p>
<p>That's how you can get the <em>millions</em> of training data-points needed, from a small human-made list!</p>
<p>Anthropic is the pioneer behind this technique, and they're already using it successfully for their chatbot, Claude. Their first constitution was inspired by many sources, including the UN Declaration of Human Rights.<sup class="footnote-ref"><a href="#fn60" id="fnref60">[60]</a></sup> Too elitist, not democratic enough? Well, next they crowdsourced suggestions to improve their constitution, which led them to add &quot;be supportive/sensitive to folks with disabilities&quot; and &quot;be balanced &amp; steelman all sides in arguments&quot;!<sup class="footnote-ref"><a href="#fn61" id="fnref61">[61]</a></sup></p>
<p>This is the most straightforward way to put humanity's wide range of values into a bot. (and <em>actually</em> deployed in a major LLM product!)</p>
<p>üèõÔ∏è <strong>Moral Parliament:</strong></p>
<p>This idea combines the Uncertainty &amp; Diversity from the previous sections. <a href="https://ora.ox.ac.uk/objects/uuid:b6b3bc2e-ba48-41d2-af7e-83f07c1fe141/files/svm40xs90j">Moral Parliament</a> proposes using a &quot;parliament&quot; of moral theories, with more seats for theories you're more confident in. (For example: my parliament may give <a href="https://en.wikipedia.org/wiki/Capability_approach">Capability Approach</a> 50 seats, <a href="https://utilitarianism.net/theories-of-wellbeing/#objective-list-theories">Eudaimonistic Utilitarianism</a> 30 seats, other misc theories get 20 seats.) The Moral Parliament then votes Yay or Nay on possible actions. The action with the most votes, wins.</p>
<p>(This proposal is pretty similar to Constitutional AI, except instead of adjectives like &quot;honest&quot; &amp; &quot;helpful&quot;, the voters are entire moral theories. And instead of an equal vote, you can put more weight on some theories vs others.)</p>
<p>As we learnt earlier, adding Diversity is a good way to increase Robustness. Because <em>every</em> moral theory has some weird edge case where it fails, having a diverse moral parliament prevents a single point of failure. (example:<sup class="footnote-ref"><a href="#fn62" id="fnref62">[62]</a></sup>)</p>
<p>The above paper was meant for Humans, but could be implemented in AIs, too.</p>
<p>üç∫ <strong>Use AIs to distill &amp; amplify human values:</strong></p>
<p>Researchers at Google DeepMind have found that a fine-tuned LLM can find <a href="https://arxiv.org/pdf/2211.15006">create consensus among humans with diverse values</a>. Better yet, these AI-aided consensus ideas were preferred <em>over the humans' opinions themselves</em>. (though, note: the humans weren't necessarily writing for consensus.)</p>
<p>Don't want to rely on fragile LLM technology specifically? Here's a proposal to do <a href="https://vitalik.eth.limo/general/2025/02/28/aihumans.html">&quot;AI as the engine, humans as the steering wheel&quot;</a> <em>in general</em>, for any AI tech:</p>
<ul>
<li>Ask a human jury to answer 1,000 numerical or multiple-choice questions. (1,000 is just an example)</li>
<li>Publicly release their answers for the first 100 questions, the other 900 are kept secret.</li>
<li>People can use the public data to train human-value-predicting AIs. (the AI can be <em>any</em> design, not just LLM-based.) They submit these AIs to a contest.</li>
<li>The AIs that <em>best</em> predict the distribution of human answers on the secret 900 questions, must be good at predicting human values in general! These AIs have &quot;distilled&quot; human values. (Note: Predict the <em>distribution</em> of answers, not just the average answer. This way the AI can also predict how polarizing something would be.)</li>
<li>You can now use an <em>ensemble</em> of the best AIs (Why ensemble? Because Diversity ‚Üí Robustness, remember?) as an Oracle for &quot;does this fit humanity's values&quot;, to make decisions <em>not</em> in the original 1,000-question set.</li>
</ul>
<p>üíñ <strong>Learning from diverse sources of human values</strong>:<sup class="footnote-ref"><a href="#fn63" id="fnref63">[63]</a></sup></p>
<p>Give an AI our stories, our fables, philosophical tracts, religious texts, government constitutions, non-profit mission statements, anthropological records, <em>all of it</em>... then let good ol' fashioned machine learning extract out our most robust, universal human values.</p>
<p>(But every human culture has greed, murder, etc. Might this not lock us into the worst parts of our nature? See next proposal...)</p>
<p>üåü <strong>Coherent Extrapolated Volition (CEV):</strong><sup class="footnote-ref"><a href="#fn64" id="fnref64">[64]</a></sup></p>
<p><em>Volition</em> means &quot;what we wish for&quot;.</p>
<p><em>Extrapolated</em> Volition means &quot;what we <em>would</em> wish for, if we were the kind of people we wished we were (wiser, kinder, grew up together further)&quot;.</p>
<p><em>Coherent</em> Extrapolated Volition means the wishes that, say, 95+% of us would agree on after hundreds of rounds of reflection &amp; discussion. For example: I don't expect every wise person to converge on liking the same foods/music, but I <em>would</em> expect every wise person to at least converge on &quot;don't murder innocents for fun&quot;. Therefore: CEV gives us freedom on tastes/aesthetics, but not &quot;ethics&quot;.</p>
<p>CEV is different from the above proposals, because it does <em>not</em> propose any specific ethical rules to follow. Instead, it proposes a <em>process</em> to improve our ethics. (Reminder: this is called &quot;indirect normativity&quot;<sup class="footnote-ref"><a href="#fn65" id="fnref65">[65]</a></sup>) This is similar to the strength of &quot;the scientific method&quot;<sup class="footnote-ref"><a href="#fn66" id="fnref66">[66]</a></sup>: it does <em>not</em> propose specific things to believe, but proposes a specific <em>process</em> to follow.</p>
<p>I like CEV, because it basically describes the <em>best-case scenario</em> for humanity without AI ‚Äî a world where everyone rigorously reflects on what is the Good ‚Äî and then sets that as <em>the bare minimum</em> for an advanced AI. So, an advanced aligned AI that follows CEV may not be perfect, but <em>at worst</em> it'd be us <em>at our best</em>.</p>
<p>One problem with CEV is that &quot;simulate 8+ billion people debating for 100 years&quot; is impossible in practice ‚Äî but ‚Äî we can approximate it! For example: use a few hundred LLMs that have been trained &amp; validated to represent a demographic, then have <em>them</em> debate.<sup class="footnote-ref"><a href="#fn67" id="fnref67">[67]</a></sup> This is similar to the way democracies have representatives &amp; activists that debate/vote on behalf of the people they represent. (And yes, LLMs <em>can</em> accurately simulate individuals' beliefs &amp; personalities!<sup class="footnote-ref"><a href="#fn68" id="fnref68">[68]</a></sup>)</p>
<p>However, a more fundamental problem with CEV is this: people will be <em>horrified</em> by a wiser version of themselves. Imagine if we had powerful AI in the year 1800, that implemented CEV, and accurately simulated our moral development up to the year 2025. A relatively wise person in 1800 could have seen that enslaving Black people was wrong. But even the top-5% wisest people back then would have been <em>horrified</em> by the idea of a Black person being President, or marrying a white woman. By symmetry, there's very likely things <em>now</em> that we're irrationally disgusted by, that a much wiser version of us would be okay with.</p>
<p>But if a powerful AGI just plopped down and said, <em>&quot;Hey here's a horrifying thing I'm going to do, the much wiser version of you would agree to this&quot;</em>, there's no way to tell in advance if it's true, or something went horribly wrong.</p>
<p>Hence, the next idea fixes this problem:</p>
<p>üåÄ <strong>Coherent <em>Blended</em> Volition:</strong></p>
<p>Instead of asking a paternalistic, Nanny AI to simulate us reflecting &amp; improving upon our beliefs and values‚Ä¶ well, why don't <em>we</em> reflect &amp; improve upon our beliefs and values?</p>
<p><em>&quot;Because humans suck at doing that. See: all of History.&quot;</em> Okay, fair enough. But what if we used all the tools at our disposal ‚Äî not just helper AIs, but also discussion platforms, data analysis, etc ‚Äî to help us <em>talk better?</em> To find solutions that aren't mere average-centrism, but actually <em>combines the best parts</em> of our diverse worldviews &amp; values?<sup class="footnote-ref"><a href="#fn69" id="fnref69">[69]</a></sup></p>
<p>Sounds nice in theory. Does it work in practice? <strong>So far: yes!</strong> <a href="https://blog.pol.is/uber-responds-to-vtaiwans-coherent-blended-volition-3e9b75102b9b">Taiwan, directly inspired by Coherent Blended Volition</a>, used digital tools to collect &amp; blend the perspectives of citizens &amp; industry, to <em>create actual policy</em>. (The specific issue was Uber entering Taiwan.) Inspired by the success of Taiwan's digital tools, Twitter (now ùïè) used <a href="https://blog.ncase.me/signal-boost-autumn-2025/#bridging">a similar algorithm to design Birdwatch (now Community Notes)</a>, which as far as I can tell, is still the <em>only</em> fact-checking service to be rated net-helpful <em>across the U.S. political spectrum.</em> No easy feat, in these polarized times!</p>
<p>This way, instead of asking a powerful AI to simulate us getting wiser, AI can help us <em>actually get wiser</em>. (by being a Socratic questioner, discussion facilitator, fact-checker, etc.) This way, we'll be able to accept wiser ideas &amp; actions, even if the previous less-wise versions of us would've rejected them.</p>
<p>We make our tools better, so our tools help <em>us</em> be better, repeat. We'll grow <em>alongside</em> the AIs.</p>
<p>. . .</p>
<p>Maybe AI will never solve ethics. Maybe <em>humans</em> will never solve ethics. If so, then I think we can only do our best: remain humble &amp; curious about what the right thing is to do, learn broadly, and self-reflect in a rigorous, brutally-honest way.</p>
<p>That's the best we fleshy humans can do, so let's at least make that the <em>lower bound</em> for AI.</p>
<h3>ü§î Review #7</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="How does Constitutional AI work?"
        answer="1] Human creates a list of principles. 2] Teacher-bot uses this list to train a student-bot. (This is how you can get millions of training data-points, from a small human-made list, which can also be democratically crowdsourced.)">
    </orbit-prompt>
    <orbit-prompt
        question="What is Moral Parliament?"
        answer="A 'parliament' of moral theories, with more seats for theories you're more confident in. The parliament votes on decisions.">
    </orbit-prompt>
    <orbit-prompt
        question="One way to distill human judgment using AI (not limited to LLMs specifically)"
        answer="1] ask human jury to answer lots of questions. 2] people can submit AIs to predict their answers. 3] use an ensemble (for robustness) of the best AIs as an Oracle for ‚Äúdoes this fit humanity's values‚Äù">
    </orbit-prompt>
    <orbit-prompt
        question="What does 'Coherent Extrapolated Volition' (CEV) mean?"
        answer="What wishes (Volition) we'd have in common (Coherent) if we had a lot of time to reflect & discuss with each other (Extrapolated)">
    </orbit-prompt>
    <orbit-prompt
        question="What does 'Coherent _Blended_ Volition' (CBV) mean?"
        answer="Same as CEV, except that the AIs _actually help us reflect & grow together_, instead of just simulating us doing that. (Success case: Taiwan's digital ‚Äúcitizen's assembly‚Äù, when Uber wanted to enter Taiwan)">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="governance"></a></p>
<h2>AI Governance: the <em>Human</em> Alignment Problem</h2>
<img class="mini" alt="Comic of a dumb user, with computer saying 'ERROR ID-10-T: problem exists between chair and keyboard'" src="../media/p3/governance/idiot.png">
<p>The saddest apocalypse: we <em>solve</em> the game theory problems of AI Logic, we <em>solve</em> the deep-learning problems of AI &quot;Intuition&quot;, we even <em>solve</em> moral philosophy‚Ä¶</p>
<p>We <em>know</em> all the solutions, and then‚Ä¶ people are just too greedy or lazy to use it. Then we perish.</p>
<p><strong>Point is, all our work on the AI alignment problem means nothing if we can't solve the <em>human</em> alignment problem: how do we get fallible fleshy humans to <em>actually coordinate on safe, humane AI?</em></strong></p>
<p>This question is called <strong>Socio-technical Alignment</strong>, or <strong>AI Governance</strong>.</p>
<p>. . .</p>
<p>Here's the Alignment vs Capabilities chart, again:</p>
<p><img src="../media/p3/governance/rocket.png" alt="Graph of Alignment to Humane Values vs Capabilities. A rocket is blasting towards the right. Above a dotted diagonal line, Alignment &gt; Capabilities, we're heading to a good place. Below that dotted line, Alignment &lt; Capabilities, we're headed to a bad place"></p>
<p>The goal: keep our rocket above the &quot;safe&quot; line.</p>
<p>Thus, a 2-part strategy for AI Governance:</p>
<ol>
<li>Verify where we are, our direction &amp; velocity.</li>
<li>Use sticks &amp; carrots to stay above the &quot;safe&quot; line.</li>
</ol>
<p>(note that &quot;governance&quot; can also include bottom-up approaches, not just top-down! in case you were ‚Äî understandably ‚Äî worried about &quot;AI Governance&quot; being a Trojan Horse for a world government.)</p>
<p>In more detail:</p>
<p><strong>1) Verify where we are, our direction &amp; velocity:</strong></p>
<ul>
<li><u>Evaluations (or &quot;Evals&quot;)</u>, to keep track of AI Safety-related properties of frontier AIs. Can they help someone create weapons of mass destruction?<sup class="footnote-ref"><a href="#fn70" id="fnref70">[70]</a></sup> Do they lead users into mental health spirals?<sup class="footnote-ref"><a href="#fn71" id="fnref71">[71]</a></sup> Can AI write code to train AIs?<sup class="footnote-ref"><a href="#fn72" id="fnref72">[72]</a></sup> And so on. Ideally, citizens can create their own evals, too.<sup class="footnote-ref"><a href="#fn73" id="fnref73">[73]</a></sup></li>
<li><u>Protect whistleblowers' free speech</u>. OpenAI once had a <em>non-disparagement</em> clause in their contract, making it illegal for ex-employees to publicly sound the alarm on them being sloppy on safety &amp; deliberately accelerating the AGI arms race.<sup class="footnote-ref"><a href="#fn74" id="fnref74">[74]</a></sup> Whistleblowers should be protected.</li>
<li><u>Enforce transparency &amp; standards on major AI labs</u>. (in a way that isn't over-burdening.)
<ul>
<li>Require AI labs adopt a Responsible Scaling Policy (see below), openly publish that policy, and be transparent about their evals &amp; safeguards.</li>
<li>Send in external, independent auditors (who will keep trade secrets confidential). This is what many software industries (like cybersecurity &amp; VPNs) <em>already</em> do as regular practice.</li>
</ul>
</li>
<li><u>Track chips &amp; compute</u>. Governments keep track of GPU clusters, and who's running large &quot;frontier AI&quot;-levels of training compute. Similar to how governments already track large &quot;bomb&quot;-levels of nuclear material.
<ul>
<li>although, in Dec 2025, this approach may already be obsolete: almost all the recent gains in AI Capabilities is not from <em>training</em> (which requires large centralized compute clusters), but <em>run-time</em> (which anyone can in a decentralized way). This means that open-source AIs can soon be as powerful as big-company AIs. It also means that another AI Governance idea, &quot;model weight security&quot; ‚Äî keeping ChatGPT &amp; Claude's neural networks secret ‚Äî will be of less importance.<sup class="footnote-ref"><a href="#fn75" id="fnref75">[75]</a></sup></li>
</ul>
</li>
<li><u>Forecasting</u>. To know not just <em>where</em> we are, but our direction &amp; velocity: have &quot;super-predictors&quot; regularly forecast AI's upcoming capabilities &amp; risks.<sup class="footnote-ref"><a href="#fn76" id="fnref76">[76]</a></sup> (There's also early evidence showing that AI <em>itself</em> can boost Humans' forecasting! <sup class="footnote-ref"><a href="#fn77" id="fnref77">[77]</a></sup>)</li>
</ul>
<p><strong>2) Use sticks &amp; carrots to stay above the &quot;safe&quot; line.</strong></p>
<ul>
<li><b><u>Responsible Scaling Policy</u></b>.<sup class="footnote-ref"><a href="#fn78" id="fnref78">[78]</a></sup> A problem: we can't even accurately predict the risks of advanced AI until we get closer to it. So, instead of having a policy for across all possible future AIs, we take an <em>iterative</em> approach (like with Scalable Oversight). For example: &quot;We commit to not even <em>start</em> training an AI of Level N, until we have safeguards and evaluations <em>up to Level N+1</em>.&quot;
<ul>
<li>A similar idea was proposed in the <a href="https://arxiv.org/pdf/2405.06624">Guaranteed Safe AI</a> paper.</li>
</ul>
</li>
<li><u>Differential Technology Development (DTD)</u><sup class="footnote-ref"><a href="#fn79" id="fnref79">[79]</a></sup> AI, by default, is <strong>&quot;dual-use&quot;</strong>: could be used for great good or great harm. <em>Differential</em> development, then, is about investing in research &amp; tech that <strong>1) advances &quot;Alignment&quot; more than &quot;Capabilities&quot;, and 2) maximizes the usefulness of AI while minimizing its risks.</strong> For example:
<ul>
<li>Tech to fight catastrophic risk: AI &amp; non-AI tools that <em>boost</em> our cybersecurity, biosecurity, and mental security.</li>
<li>AI Safety research. Yes this suggestion is kinda back-scratchy, but I still endorse it.</li>
<li><a href="https://arxiv.org/pdf/2402.00530">Data filtering</a> and <a href="https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf">Machine Unlearning</a>, so that LLMs contain helpful knowledge, but not detailed knowledge of how to make bombs or superviruses.</li>
<li>At least commit to NOT building horrible stuff like fully-autonomous killer drones.<sup class="footnote-ref"><a href="#fn80" id="fnref80">[80]</a></sup></li>
<li>AI that <em>enhances</em> humans, not <em>replaces</em> humans. (See below: &quot;Alternatives to AGI&quot; and &quot;Cyborgism&quot;!)</li>
<li>Combine this idea with <em>decentralized &amp; democratic tech</em>, to prevent AI and/or AI Governance from becoming a tools for tyrants. See: <a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">d/acc</a>, <a href="https://vitalik.eth.limo/general/2024/08/21/plurality.html">Plurality</a>, <a href="https://6pack.care/">6pack.care</a>. (explained more in the below infographic ‚§µ)</li>
</ul>
</li>
</ul>
<p><img src="../media/p3/governance/6pack.png" alt="The 6-pack of digital democracy: broad listening, credible commitments, easy-to-inspect, easy-to-correct, win-win bridging solutions, and as local as possible"></p>
<p><em>(if i may toot my own horn a bit</em>, i'm <em>doing the illustrations for Audrey Tang's upcoming book, 6pack.care! all comics will be dedicated to the public domain.)</em></p>
<p>Another thought: while &quot;sticks&quot; (fines, penalties) are necessary, I think it's neglected how we can use &quot;carrots&quot; (market incentives) to redirect industry. As <a href="https://en.wikipedia.org/wiki/Audrey_Tang">Audrey Tang</a> ‚Äî Digital Minister of Taiwan, and co-author of <a href="https://6pack.care/">6Pack.care</a> ‚Äî once explained it: if you successfully advocated for securing cockpit doors <em>before</em> 9/11, or better biosecurity before Covid-19, your reward would be... &quot;nothing happens&quot;. Nobody cares about the bomb that didn't go off. <sup class="footnote-ref"><a href="#fn81" id="fnref81">[81]</a></sup> So, if you want your AI Alignment &amp; Governance ideas to be <em>actually implemented in real life, not just in academic papers</em>, you need them to &quot;pay dividends&quot; in the short term.</p>
<p>Many of the AI Safety solutions listed above can have, or <em>already have had</em>, profitable market spinoffs. (examples: <sup class="footnote-ref"><a href="#fn82" id="fnref82">[82]</a></sup>)</p>
<p>But in the next final two sections, &quot;Alternatives to AGI&quot; and &quot;Cyborgism&quot;, we'll see ideas that empower regular fleshy humans (rather than a few companies/governments), <em>and</em> are short-term market-incentive-compatible, <em>and</em> long-term Good.</p>
<p><a href="#EconomicsAI">: bonus - what about the economics of AI? basic income, human subsidies, re-skilling?</a></p>
<p>. . .</p>
<p>A note of pessimism, followed by cautious optimism.</p>
<p>Consider the last few <em>decades</em> in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? &quot;Humans coordinating to deal with global threats&quot; is... not a thing we seem to be good at.</p>
<p>But we <em>used</em> to be good at it! We eradicated smallpox<sup class="footnote-ref"><a href="#fn83" id="fnref83">[83]</a></sup>, it's no longer the case that <em>half</em> of kids died before age 15<sup class="footnote-ref"><a href="#fn84" id="fnref84">[84]</a></sup>, the ozone layer <em>is</em> actually healing! <sup class="footnote-ref"><a href="#fn85" id="fnref85">[85]</a></sup></p>
<p>Humans <em>have</em> solved the &quot;human alignment problem&quot; before.</p>
<p>Let's get our groove back, and align ourselves on aligning AI.</p>
<h4>:x Economics AI</h4>
<p>Not really related to &quot;AI Governance&quot;, but is <em>is</em> a major sociopolitical question, that the world's governments <em>should</em> care about:</p>
<p><strong>Will an AI take my job?</strong></p>
<p>Actually, it's worse than that ‚Äî even if you're in an AI-proof job, AI can still make your wages plummet. Why? Because the humans who lose <em>their</em> job to AI, will flood <em>your</em> field, and market competition will pull your wage downwards.</p>
<p>But in the past, hasn't automation always been good on average, creating more jobs than it destroys?</p>
<ol>
<li>Well, re: &quot;average&quot;, the average person has one testicle. Point is: <strong>&quot;average&quot; ignores the <em>variance</em>.</strong> If the bottom 99% of people lose \$10,000, so that the top 1% gain \$1,000,000, that's a win &quot;on average&quot;. I exaggerate, but <a href="https://www.marketplace.org/story/2025/11/11/kshaped-economy-mirrored-by-covids-uneven-recovery">since Covid, we may already be in a K-shaped economy</a>: one line goes up for the rich, one line goes down for the rest of us. (And even if you're a utilitarian who only cares about total utility, remember utility is logarithmic to wealth. So if Alice gains \$10,000 &amp; Bob loses \$10,000, Alice gains <em>less</em> utility than Bob loses: total wealth is constant, but total <em>utility</em> drops. You wouldn't be <em>neutral</em> about a coin flip where you gain/lose \$10,000, would you?)</li>
<li><strong>Yes, automation <em>does</em> create jobs at first, but at advanced levels, starts taking them away.</strong> A good example is automatic telling machines (ATMs) at banks: at first, their introduction actually correlated with an <em>increase</em> in human tellers being hired, because while each branch needed fewer human tellers, ATMs made it cheaper to open many <em>more</em> branches, so that total humans hired actually increased. In the short run. Now, with online banking, and with the market for bank branches already saturated, human tellers hired is dropping again. (<a href="https://80000hours.org/agi/guide/skills-ai-makes-valuable/#1-why-automation-often-doesnt-decrease-wages">see this article</a>)</li>
<li><strong>This time really is different.</strong> In the past, automation &quot;only&quot; took out one small sector at a time (textile workers, horse drivers, elevator operators, etc). But <em>this</em> time, with deep learning, AIs can automate a huge percentage of the human economy in one go: a) self-driving cars vs truckers &amp; taxicabs, b) LLMs vs programmers, lawyers &amp; writers, c) speech-to-text-and-back vs call centers, secretaries, &amp; audiobooks/podcasters, d) image and video models vs artists and creatives, e) so on and so on. And if you're in an AI-safe job, like plumber? Good for you! But your wages will drop from <em>everyone else</em> scrambling to re-train to AI-safe jobs, like plumber.</li>
</ol>
<p>(It's hard to tell how badly AI is affecting the job market <em>right now</em>. The hiring rates for artists, writers, and even newly-graduated programmers ‚Äî once thought of a golden ticket to a 6-figure salary! ‚Äî has been dropping since generative AI came out in late 2022. But that <em>also</em> coincides with a post-Covid recession, so it's hard to tell how much of the unemployment is due to AI specifically, vs the sucky economy in general.)</p>
<p>= = =</p>
<p>A popular proposal is a <strong>Universal Basic Income (UBI)</strong>, funded by taxing AI. After all, modern AI was trained on data created by the public's hard work ‚Äî shouldn't part of the gains go back to the public, a &quot;citizen's dividend, similar to <a href="https://en.wikipedia.org/wiki/Alaska_Permanent_Fund">Alaska's sovereign wealth fund</a>, where profit from their natural resources goes back to their citizens?</p>
<p>A lot of skepticism about UBI comes from it promoted by tech CEOs <a href="https://moores.samaltman.com/">like Sam Altman</a>, who are, uh, <a href="https://garymarcus.substack.com/p/sam-altmans-pants-are-totally-on">&quot;not completely candid&quot;</a>. That said, just because someone's promoting an idea cynically to quell dissent, doesn't mean it's <em>not</em> a good idea. Tech CEOs could be lying about promising UBI, because UBI <em>would</em> be great. <a href="https://p1.portal2sounds.com/57">To quote GLaDOS</a>: &quot;Killing you, and giving you good advice, aren't mutually exclusive.&quot;</p>
<p>Another common critique of UBI is that people don't just need money, they need <em>meaning</em>, and people get meaning from their jobs. (<a href="https://figuringoutfulfillment.wordpress.com/2012/01/23/lieben-und-arbeiten/">&quot;Love and Work&quot;</a>, Freud supposedly said.) Now: writing as someone who finds meaning in her job: <em>f@#k off</em>. I keep hearing the &quot;work is good coz it gives meaning&quot; line from think tanks &amp; pundits &amp; the Credentialed Class, and, yeah, easy for <em>you</em> to say, you have a <em>nice</em> job. <em>I</em> have a nice job. We're not working the graveyard shift at 7/11, or slowly getting lung cancer from the industrial fumes. <em>But, oh, if we're all living off UBI, how will we get meaning if we can't sell our labour on a market?</em> Oh I dunno, how about spending life with family? friends? lovers? while getting your sense of mastery &amp; challenge from sports, art, math, play? And if you love your job making art or writing essays, you know you can just <em>keep doing that</em> even with UBI, right?</p>
<p>Sorry I'm pissy about this, I just do not respect Status Quo Stockholm Syndrome.</p>
<p>However, there <em>is</em> one strong critique of UBI that I acknowledge. Despite all the exciting pilot programs in developing nations, <strong>UBI in <em>developed</em> nations like America mostly doesn't help.</strong> (see <a href="https://evavivalt.com/wp-content/uploads/Vivalt-et-al.-ORUS-employment.pdf">the OpenResearch study by Vivalt et al 2025</a>, lit review by <a href="https://www.theargumentmag.com/p/giving-people-money-helped-less-than">Kelsey Piper</a>)</p>
<p>For example, <a href="https://basicincome.org/wp-content/uploads/2024/07/CGIRLABIGLEAPFinalReport.pdf">see the BIG:LEAP report</a>, a UBI experiment in Los Angeles. See Tables 4, 5, and 9: receipients' financial well-being, food security, and perceived stress were all <em>barely</em> any better than controls, after <em>18 months of getting \$1000/month</em>.</p>
<p>And it's not because &quot;well, Americans are already relatively rich compared to Africans, so extra cash doesn't do much&quot; ‚Äì one RCT gave <em>\$1000/month to homeless people in Denver</em>, and after ten months, they weren't any more likely to have stable housing than the controls getting \$50/month. (<a href="https://www.denverbasicincomeproject.org/research">See Figures 1 &amp; 2 of their research page</a>, which tries to lie-via-misleading-truth, and play this off as a success?!?!)</p>
<p>So if UBI doesn't even help <em>literally homeless people</em>, how would it help people who lost their jobs to AI?</p>
<p>Also, in these major UBI trials, recipients worked a bit less. Which would be good if the extra leisure time translated to better mental health, or better child health. It didn't.</p>
<p>(To be fair, at least UBI in America doesn't <em>hurt.</em> The recipients mostly spent the extra money on nicer things for their kids &amp; loved ones, and did not spend it on drugs or gambling. And UBI <em>did</em> reduce intimate partner abuse, and make people happier‚Ä¶ for about one year, before returning to baseline. Which isn't nothing! Not being beat by your spouse for a year matters!)</p>
<p>So‚Ä¶ what gives? Why does UBI work in developing nations, but not America?</p>
<p>I don't know. Maybe money is a stronger limiting factor in developing nations, but non-money stuff (personal habits, cultural norms, social ties, systemic issues) are a stronger constraint in America. This is a very ad-hoc explanation.</p>
<p>= = =</p>
<p>I know the following sounds very &quot;Ah. Well, nevertheless,&quot; but nevertheless, I do (tentatively) support UBI for AI-powered job displacement. The above evidence shows that UBI won't help with poverty, at least in America, but:</p>
<ol>
<li>We can consider UBI a deserved citizen's dividend, like Alaska's get-$1000-a-year natural resource fund. After all, it was (involuntarily) trained off <em>your</em> data, shouldn't you at least get some of the profit?</li>
<li>We can consider UBI as &quot;insurance&quot;, because it's very unpredictable when you'd lose your job to a robot. Seriously, who 20 years ago would've thought we'd <em>automate art and poetry</em> before automating &quot;clean my house&quot;?</li>
<li>UBI still seems like the smoothest, non-violent way to transition from today's market economy, to a post-scarcity Star Trek utopia, where food &amp; shelter &amp; all the essentials become &quot;too cheap to meter&quot;.</li>
</ol>
<p>= = =</p>
<p>An alternative to UBI for poverty reduction, is the <strong>Earned Income Tax Credit (EITC)</strong>, also known as a negative income tax. The idea is that the company pays you whatever's the market wage, but it's topped up to a livable wage through tax credits.</p>
<p>Politically, it's popular <a href="https://waysandmeans.house.gov/2024/02/20/icymi-american-compass-op-ed-child-tax-credit-is-a-win-for-conservatives/">among</a> <a href="https://www.aei.org/economics/does-economic-freedom-have-a-future-in-america-a-qa-with-arthur-brooks/">conservatives</a> because unlike UBI, the EITC provides pro-work incentives. (plus, &quot;negative tax&quot; and &quot;tax rebate&quot; <em>feels</em> better than &quot;welfare check&quot;. In the latter, you're getting a handout. But in the former, you're <em>getting your money back</em>. Feels better! Even though it's frikkin' identical. <em>It's all about the marketing, baby.</em>)</p>
<p>It's also popular among economists (who are the most politically diverse field in social science). According to the most recent survey of economists, 90% support expanding the EITC. (<a href="https://d1wqtxts1xzle7.cloudfront.net/95174671/HBhGyFD7-libre.pdf?1669999561=&amp;response-content-disposition=inline%3B+filename%3DConsensus_among_economists_2020_A_sharpe.pdf&amp;Expires=1763701882&amp;Signature=IaFfiqOrXv-Nov61st-qoOTiaE8K8~pVvirurLs3akQKaO6xxn~khOCs1ufp2jJNkbDzg7fU-aHX~XP6CFvU5pSRSrkP5SobLhbEdIf9yN-Mg~ezpdVFoAnvifuXXpH9giSaR1-OPndVSdh~xCzx0FWqWsYa4k04uKlYpOFdMdvm4LhDVnoBttyhRd~8hWGi22aBRK19xCtlEe63E3mOoJx79fv9APB~iRy4gkV9uryKSJoSl5jqEBBOohpxsK4OURp46z8OLowwzxbeK~0zG9WURDFvQxQaMidtQNs9tFEerWa~GBZSVMp~Y8nVYeqN-80PEWKJfsimBM133vXefg__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">see Table 1</a>)</p>
<p>But more relevant to AI economics, the EITC is a <strong>human subsidy</strong>. You're subsidizing companies to hire local humans, instead of offshoring them or automating them. And in sectors where AI <em>still</em> has the upper economic hand, we can tax it to fund the EITC.</p>
<p>This can also be paired with incentivizing &amp; investing in <strong>upskilling &amp; retraining human workers.</strong> Help us get jobs to evade LLMs' grasp, e.g. tasks which require rule-discovery (eg research), dexterity in physical environments (eg manual trades, live performance arts), long contexts (eg longform writing), or we just <em>want</em> the job to be human (eg elderly care).</p>
<p>= = =</p>
<p>We're not even scratching the surface of economic policy proposals re: advanced AI. For a good overview, <a href="https://www.anthropic.com/research/economic-policy-responses">check out this review of options on Anthropic's blog.</a></p>
<p>My personal opinion, as of Dec 2025, open to change: for policy, I'd recommend a combined UBI + EITC + educational vouchers for reskilling. It'd be funded by taxing the productivity gains from AI, and given mostly towards high-automation-risk workers like: a) truckers, taxicab drivers (self-driving cars), b) call center services (speech &amp; language AIs), c) retail (digital kiosks, which aren't even &quot;AI&quot;). And at the risk of being extremely self-serving, sure: digital arts, programming, and white-collar work are all also at high automation risk.</p>
<p>As for what you can do <em>personally</em>, well, you might want to read/listen to <strong><a href="https://80000hours.org/agi/guide/skills-ai-makes-valuable/">How not to lose your job to¬†AI</a>.</strong></p>
<h3>ü§î Review #8</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="All our work on the AI alignment problem means nothing if we can't solve..."
        answer="...the _human_ alignment problem: how do we get fallible fleshy humans to _actually coordinate on safe, humane AI?">
    </orbit-prompt>
    <orbit-prompt
        question="The 2 core pillars of AI Governance:"
        answer="1] Verify where we are. 2] Stay above the 'safe' line."
        answer-attachments='https://aisafety.dance/media/p3/governance/rocket.png'>
        <!-- aisffs-gov-rocket.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to 'verify where we are' in AI Governance:"
        answer="(any of the following work:) evaluations (ideally crowdsourced), protect whistleblowers, enforce transparency & standards on AI labs, track chips & compute, forecasting (maybe AI-aided)">
    </orbit-prompt>
    <orbit-prompt
        question="Responsible Scaling Policy, in a nutshell:"
        answer="'We commit to not even _start_ training an AI of Level N, until we have safeguards and evaluations _up to Level N+1_.'">
    </orbit-prompt>
    <orbit-prompt
        question="Differential Technological Development in a nutshell:"
        answer="Invest in tech/research that boosts Alignment _more than_ Capabilities. (or at least Safe Capabilities _more than_ Risky Capabilities)">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to 'stay above the safe line':"
        answer="(any of the following work:) Responsible Scaling Policy, Differentially invest in Alignment > Capabilities, Tech to fight catastrophic risk, Data filtering/machine unlearning so that AI doesn't know how to make bombs/superviruses, AI that enhances not replaces humans ('Cyborgism').">
    </orbit-prompt>
    <orbit-prompt
        question="If you want your AI Alignment & Governance ideas to be _actually implemented in real life, not just in academic papers_..."
        answer="You need your ideas to ‚Äúpay dividends‚Äù right now, in the short term. (‚ÄúNobody cares about the bomb that didn't go off.‚Äù)">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 time humanity successfully coordinated to solve a global problem:"
        answer="(any of the following works:) We eradicated smallpox, it's no longer the case that *half* of kids died before age 15, the ozone layer *is* actually healing!">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="alt"></a></p>
<h2>Alternatives to AGI</h2>
<p>Why don't we just <em>not</em> create the Torment Nexus?<sup class="footnote-ref"><a href="#fn86" id="fnref86">[86]</a></sup></p>
<p>If creating an Artificial General Intelligence (AGI) is so risky, like sparrows stealing an owl egg to try to raise an owl who'll defend their nest &amp; hopefully not eat them<sup class="footnote-ref"><a href="#fn87" id="fnref87">[87]</a></sup>...</p>
<p>...why don't we find ways to get the pros <em>without</em> the cons? A way to defend the sparrow nest <em>without</em> raising an owl? To un-metaphor this: why don't we find ways to use <strong>less-powerful, narrow-scope, not-fully-autonomous AIs</strong> to help us ‚Äî say ‚Äî cure cancer &amp; build flourishing societies, <em>without</em> risking a Torment Nexus?</p>
<p>Well... yeah.</p>
<p>Yeah I endorse this one. Sure, it's obvious, but &quot;2 + 2 = 4&quot; is obvious, that don't make it wrong. The problem is how to <em>actually do this</em> in practice.</p>
<p>Here's some proposals, of how to get the upsides with much fewer downsides:</p>
<ul>
<li><strong>Comprehensive AI Services (CAIS), Tool AIs</strong><sup class="footnote-ref"><a href="#fn88" id="fnref88">[88]</a></sup>: Make a large suite of narrow non-autonomous AI tools (think: Excel, Google Translate). To solve general problems, insert <em>human</em> agency: humans are the conductors for this AI orchestra. The human, and their values, stay in the center.</li>
<li><strong>Pure Scientist AI</strong>.<sup class="footnote-ref"><a href="#fn89" id="fnref89">[89]</a></sup> Imagine an AI that just takes in observations, and spits out theories. (The same way that Google Translate can take English as input, and Mandarin as output.) Crucially, this AI <em>does not</em> have &quot;agency&quot;, which would lead to <a href="https://aisafety.dance/p2/#problem2">Instrumental Convergence</a> problems. It just fits <em>scientific models</em> to data, like how Excel fits lines to data. It <em>can't</em> go rogue any more than Google Translate can go rogue.
<ul>
<li>It's far from a few Scientist AI, but Microsoft has done research in training ANNs to <a href="https://www.microsoft.com/en-us/research/publication/deep-end-to-end-causal-inference-2/">infer causal diagrams</a>.</li>
</ul>
</li>
<li><strong>Microscope AIs</strong><sup class="footnote-ref"><a href="#fn90" id="fnref90">[90]</a></sup>: In contrast, instead of making an AI whose scientific findings are its <em>output</em>, we train an AI to predict real-world data, then look <em>at the neural connections themselves</em> to learn about that data! (using Interpretability techniques)</li>
<li><strong>Quantilizers</strong><sup class="footnote-ref"><a href="#fn91" id="fnref91">[91]</a></sup>: Instead of making an AI that <em>optimizes</em> for a goal, make an AI that is trained to <em>imitate a (smart) human</em>. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days. This &quot;soft optimization&quot; avoids Goodhart-problems<sup class="footnote-ref"><a href="#fn92" id="fnref92">[92]</a></sup> of pure optimization, and keeps the resulting solutions human-understandable.</li>
</ul>
<p><em>All</em> of these are easier said than done, of course. And there's <em>still</em> other problems with a focus on &quot;Alternatives to AGI&quot;. Social problems, and technical problems:</p>
<ul>
<li>A malicious group of humans could still use narrow AI for catastrophic ends. (e.g. bioweapon-pandemic, self-replicating killer drones)</li>
<li>A well-meaning-but-na√Øve group could use narrow non-autonomous AI to <em>make</em> general autonomous AI, with all of its risks.</li>
<li>An AI that doesn't plan ahead, and &quot;merely&quot; predicts future outcomes, can still have nasty side-effects, due to self-fulfilling prophecies.</li>
<li>Due to economic incentives (and human laziness), the market may just <em>prefer</em> to make general AIs that are fully autonomous.</li>
<li>Due to incentives &amp; laziness, we end up in a <a href="https://gradual-disempowerment.ai/">gradually disempowered Slopworld</a>. Instead of a smart AI taking over the world, we <em>hand over</em> the world to dumb AIs, piece-by-piece. Death by convenience.</li>
</ul>
<p>That said, it's good to stack extra solutions, even if imperfect!</p>
<p>As for that last concern, about handing over our autonomy to AIs, that's what our final Cyborgism section covers...</p>
<h3>ü§î Review #9</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="Name at least 1 way to get the benefits of AI without making a powerful, general, autonomous AI"
        answer="(Any of the following works:) Comprehensive AI Services, Tool AIs, Pure Scientist AIs, Microscope AIs, Quantilizers, Cyborgism">
    </orbit-prompt>
    <orbit-prompt
        question="What's a 'Scientist AI'?"
        answer="An AI that ‚Äújust‚Äù takes in observations, and spits out theories. (The same way that Google Translate can take English as input, and give Mandarin as output.)">
    </orbit-prompt>
    <orbit-prompt
        question="What's Microscope AI?"
        answer="Instead of making an AI whose scientific findings are its *output*, we train an AI to predict real-world data, then look *at the neural connections themselves* to learn about that data! (using Interpretability techniques)">
    </orbit-prompt>
    <orbit-prompt
        question="What's a 'Quantilizer'?"
        answer="Instead of making an AI that *optimizes* for a goal, make an AI that is trained to *imitate a (smart) human*. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days.">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><a id="cyborg"></a></p>
<h2>Cyborgism</h2>
<p>Re: humans &amp; possible future advanced AI,</p>
<p><strong>If we can't beat 'em, join 'em!</strong></p>
<p>We <em>could</em> interpret that literally: brain-computer interfaces in the medium term, mind-uploading in the long term. But we don't have to wait that long. The mythos of the &quot;cyborg&quot; can still be helpful, <em>right now!</em> In fact:</p>
<p><em>YOU'RE ALREADY A CYBORG.</em></p>
<p>...if &quot;cyborg&quot; means any human that's augmented their body or mind with technology. For example, you're <em>reading</em> this. Reading &amp; writing <em>is</em> a technology. (Remember: things are still technologies even if they were created before you were born.) Literacy even <em>measurably re-wires your brain.</em><sup class="footnote-ref"><a href="#fn93" id="fnref93">[93]</a></sup> You are not a natural human: a few hundred years ago, most people couldn't read or write.</p>
<p>Besides literacy, there's many other everyday cyborgisms:</p>
<ul>
<li><u>Physical augmentations:</u> glasses, pacemakers, prosthetics, implants, hearing aids, continuous glucose monitors</li>
<li><u>Cognitive augmentations:</u> reading/writing, math notation, computers, spaced repetition flashcards</li>
<li><u><i>Emotional</i> augmentations:</u> diaries, meditation apps, reading biographies or watching documentaries to empathize with folks across the world.</li>
<li><u><i>&quot;Collective Intelligence&quot;</i> augmentations:</u> Wikipedia, prediction markets/aggregators.</li>
</ul>
<p><img src="../media/p3/cyborg/cyborg.png" alt="Stylish silhouette-drawing of people with &quot;everyday cyborg&quot; tools. Glitched-out caption reads: We're all already cyborgs."></p>
<p><strong>Q:</strong> That's... tool use. Do you really need a sci-fi word like &quot;cyborg&quot; to describe <em>tool use?</em></p>
<p><strong>A:</strong> Yes.</p>
<p>Because if the question is: &quot;how do we keep human <em>values</em> in the center of our systems?&quot; Then one obvious answer is: keep <em>the human</em> in the center of our systems. Like that cool thing Sigourney Weaver uses in <em>Aliens (1986)</em>.</p>
<p><img src="../media/p3/cyborg/weaver.png" alt="Screenshot of Sigourney Weaver in the Power Loader. Caption: cyborgism, keeping the human in the center of our tools"></p>
<p>Okay, enough metaphor, here's some concrete examples:</p>
<ul>
<li>Garry Kasparov, the former World Chess Grandmaster, who also famously lost to IBM's chess-playing AI, once proposed: CENTAURS. It turned out, <strong>human-AI teams could beat <em>both</em> the best humans &amp; best AIs at chess</strong>, by having the human's &amp; AI's strengths/weaknesses compensate for each other! <sup class="footnote-ref"><a href="#fn94" id="fnref94">[94]</a></sup> (This idea may not be true for full-information bounded-rules games like Chess &amp; Go anymore<sup class="footnote-ref"><a href="#fn95" id="fnref95">[95]</a></sup>, but at present it's true for research &amp; forecasting, see below.)</li>
<li>Likewise, some researchers are trying to <a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism">combine the strengths/weaknesses of humans &amp; LLMs</a>. For example, humans are currently better at deep planning, LLMs are better at broad knowledge. Together, <strong>a &quot;cyborg&quot; may be able to think deeper <em>and</em> broader than pure-human or pure-LLM.</strong>
<ul>
<li>(You can try this out <em>today!</em> janus made a writing tool called <a href="https://generative.ink/posts/loom-interface-to-the-multiverse/">Loom</a>, which lets you explore a &quot;multiverse&quot; of thoughts.)</li>
</ul>
</li>
<li>Large Language Models are &quot;only&quot; about as good as the average person at forecasting future events, but <em>together</em>, a normal LLM can help normal humans improve their forecasting ability by up to 41%! <sup class="footnote-ref"><a href="#fn77" id="fnref77:1">[77:1]</a></sup></li>
<li><a href="https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds">Enough AI Copilots! We need AI HUDs</a> was a popular article that argued for a different design philosophy in AI-assisted coding: instead of having AI agents act like underpaid interns, you can make tools that give you &quot;X-ray vision&quot; into your code, or an augmented heads-up display (HUD) like Iron Man's. This way, <em>you</em> get to see &amp; build upon insights. Unlike in &quot;vibe coding&quot;, <em>your code will not become a stranger to you.</em></li>
<li><a href="https://distill.pub/2017/aia/">Using Artificial Intelligence to Augment Human Intelligence</a> investigates <em>&quot;artificial intelligence augmentation (AIA)&quot;</em>, how to <em>enhance</em> human creativity &amp; invention, instead of offloading it fully to generative AIs. In this article, they highlight a demo by <a href="https://arxiv.org/pdf/1609.03552">Zhu et al (2016)</a>. This demo came out <em>long</em> before ChatGPT's DALL-E, and honestly, it's <em>still</em> a far more compelling artistic tool than most modern AI art programs. Because, it lets you do precise artistic expression, vs the &quot;write text and hope for the best&quot; approach of DALL-E / MidJourney / etc: (though, there have been some prototypes, updating this for modern AI models<sup class="footnote-ref"><a href="#fn96" id="fnref96">[96]</a></sup>)</li>
</ul>
<video width="640" height="360" controls>
    <source src="../media/p3/cyborg/shoe.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>
<p>. . .</p>
<p><strong><a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism">Cyborgism</a></strong> became popular in the AI Safety field in 2023, thanks to Nicholas Kees &amp; janus's hit article. It's a looooong post, but here's my favourite diagram from it:</p>
<p><img src="../media/p3/cyborg/puzzle.png" alt="Human &amp; GPT's comparative advantages/disadvantages, fitting together like puzzle pieces. For example, GPT are better at breadth &amp; variance, Humans better at deep, long-term coherence."></p>
<p>Remember in the &quot;Gears&quot; section above, where I was shocked at all the things LLMs surpass Humans at, but also how fragile &amp; terrible LLM reasoning is? Cyborgism says: <em>this ain't a bug, it's a feature!</em> Because if there's things that AI can do that Humans can't, <em>and</em> vice versa, then there's benefits to combining our talents:</p>
<p><img src="../media/p3/cyborg/venn.png" alt="3 Venn Diagrams. Early on: AI skills are strictly a subset of Human skills. Too late: Human strictly subset of AI. But in the middle, sweet spot: our skills do not fully overlap, meaning there is an opportunity for the collab of Human+AI to do better than either alone"></p>
<p>(Examples of Stage 1, Humans &gt; AI: dexterity in unpredictable environments, like folding laundry or home repairs. Example of Stage 3, AI &gt; Humans: arithmetic, maybe chess. But lots of tasks are still in Stage 2, Human+AI &gt; Human or AI: code, forecasting, AI Safety research itself! We're in that sweet spot where Cyborgism can still pay off huge dividends <em>right now</em>.)</p>
<p>(<a href="#HumanLLMAdvantages">: bonus - an incomplete list of what Humans &amp; LLMs are relatively better at</a>)</p>
<p>The other point of Cyborgism for AI Safety is this: <strong><em>useful</em> AI ‚â† <em>agentic</em> AI.</strong> (where &quot;agentic&quot; means it pursues goals intelligently) Sure, giving AI its own autonomy is <em>one</em> way to make it more useful (or at least, convenient), but it carries huge risks (from it &quot;going rogue&quot;, to a gradual-disempowerment Slopworld.) Cyborgism shows us a way to get useful AI <em>without</em> giving up our autonomy.</p>
<p>Keep the human in the center! Fight for the users!</p>
<p>. . .</p>
<p>Caveats &amp; warnings:</p>
<ul>
<li>It is very hard to design efficient &quot;cyborgs&quot;. <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">A recent study from METR</a> found that AI coding tools actually made programmers <em>slower</em> at their jobs, <em>even while fooling those programmers into thinking they were being more efficient</em>.</li>
<li>Humans are still lazy &amp; we'll still be tempted to give up our autonomy. (So that's another reason to make this sound <em>cool</em> with &quot;cyborg&quot;, not just &quot;tool use&quot;.)</li>
<li>When you put yourself inside a system, the system may modify <em>you</em>. Even for reading/writing, anthropologists agree that literacy isn't just a skill, it modifies your entire <em>culture and values</em>. What would becoming a <em>cyborg multiverse-thinker</em> do to you?</li>
<li>Again, an AI-augmented human could be a sociopath, and bring about catastrophic risk. Again-again, <em>one</em> human's values ‚â† humane values.</li>
</ul>
<p>Then again...</p>
<p><img src="../media/p3/cyborg/weaver2.png" alt="Close-up of Sigourney Weaver"></p>
<p>That's pretty cool.</p>
<h4>:x Human LLM Advantages</h4>
<p>A more detailed list of what Humans &amp; LLMs are better at. (<em>for now</em>, though see the above Gears section for why I suspect these differences are fundamental to LLMs)</p>
<p>üí™ <strong>Humans are better at:</strong></p>
<ul>
<li>Uncommon tasks</li>
<li>Uncommon rule-discovery</li>
<li>Common tasks in <em>uncommon</em> settings</li>
<li>Long-term coherent planning</li>
<li>&quot;Multimodal&quot; thinking, switching fluently between words &amp; pictures</li>
<li>Dexterity in &amp; reasoning about the physical world</li>
</ul>
<p>ü¶æ <strong>LLMs are better at:</strong></p>
<ul>
<li>Common short tasks, <em>fast</em></li>
<li>Recognizing &amp; applying <em>common</em> rules/patterns</li>
<li>Broad (if shallow) knowledge of a lot of topics</li>
<li>Can roleplay a wide range of characters (if stereotypically)</li>
<li>High-variance brainstorming (you can easily reset an LLMs context)</li>
<li>Language-based tasks, like search, summary, translation, sentiment analysis.</li>
</ul>
<p>Hopefully that gives you (and myself) ideas for how to design tools that <em>combine</em> Human &amp; AI skills, while keeping Human autonomy at the center!</p>
<hr>
<h3>ü§î Review #10 (last one!)</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="‚ÄúWe're all already cyborgs‚Äù. Give 3 examples?"
        answer="(any 3 of the following works): **Physical augmentations:** glasses, pacemakers, prosthetics, implants, hearing aids, continuous glucose monitors. **Cognitive augmentations:** reading/writing, math notation, computers, spaced repetition flashcards. **Emotional augmentations:** diaries, meditation apps, biographies & documentaries as empathy-enhancers. **‚ÄúCollective Intelligence‚Äù augmentations:** Wikipedia, prediction markets/aggregators."
        answer-attachments='https://aisafety.dance/media/p3/cyborg/cyborg.png'>
        <!-- aisffs-cyborg.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Cyborgism: ‚Äúkeeping the \_\_\_\_\_ in the \_\_\_\_\_‚Äù"
        answer="keeping the human in the center of our tools">
    </orbit-prompt>
    <orbit-prompt
        question="Think of 1 thing that Humans are relatively better at than LLMs, and vice versa:"
        answer="(any of the following in this puzzle-piece diagram:)"
        answer-attachments='https://aisafety.dance/media/p3/cyborg/puzzle.png'>
        <!-- aisffs-cyborg-puzzle.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name a real-world example where Human+AI team _proved_ to better than Human or AI alone"
        answer="Chess, Go, Forecasting future events. (although 'cyborgs/centaurs' in chess stopped having an advantage since 2017 -- there's a time limit to the 'sweet spot' for cyborgs!)">
    </orbit-prompt>
    <orbit-prompt
        question="Visualize the sweet spot for Cyborgism, as 3 Venn diagrams:"
        answer=""
        answer-attachments='https://aisafety.dance/media/p3/cyborg/venn.png'>
        <!-- aisffs-cyborg-venn.png -->
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h2>üéâ RECAP #3</h2>
<ul>
<li>üíñ <strong>We just need a good-enough first draft of Humane Values, that can reflect upon &amp; improve itself.</strong> This could be anything from a crowdsourced constitution, to a moral parliament, to tools that help us grow <em>alongside</em> AI.</li>
<li>üöÄ From both the top-down &amp; bottom-up, <strong>AI Governance is about keeping us above the &quot;safe&quot; line: trust but verify, sticks &amp; carrots.</strong></li>
<li>üí™ Make tech that maximizes upsides while minimizing downsides: <strong>narrow AI, and AI that <em>enhances human agency, not replaces it</em>.</strong></li>
</ul>
<hr>
<h2>In Sum:</h2>
<p>Here's THE PROBLEM‚Ñ¢Ô∏è, broken down, with all the proposed solutions! ( <a href="../media/p3/SUM.png">Click to see in full resolution!</a> )</p>
<p><img src="../media/p3/SUM.png" alt="SUMMARY OF THIS WHOLE ARTICLE, AAAAA"></p>
<p>And here's <a href="#recap1">Recap 1</a>, <a href="#recap2">Recap 2</a>, and <a href="#recap3">Recap 3</a>.</p>
<p>(Again, if you want to actually <em>remember</em> all this long-term, and not just be stuck with vague vibes two weeks from now, click the Table of Contents icon in the right sidebar, then click the &quot;ü§î Review&quot; links for the flashcards. Alternatively, download the <a href="https://ankiweb.net/shared/info/1788882060">Anki deck for Part Three</a>.)</p>
<p>. . .</p>
<p><em>(EXTREMELY LONG INHALE)</em></p>
<p><em>(10 second pause)</em></p>
<p><em>(EXTREMELY LONG EXHALE)</em></p>
<p>. . .</p>
<p>Aaaaand I'm done.</p>
<p>Around 80,000 words later (about the length of a novel), and nearly a hundred illustrations, that's... it. Over three years in the making, that's the end of my whirlwind tour guide. <strong>You now understand the core ideas from the wide world of AI &amp; AI Safety.</strong></p>
<p>üéâ Pat yourself on the back!  (But mostly pat <em>my</em> back. (I'm so tired.))</p>
<p>Sure, the field of AI Safety is moving so fast, Part One &amp; Two started to become obsolete before Part Three came out, and doubtless Part Three: The Proposed Solutions will feel na√Øve or obvious in a couple years.</p>
<p>But hey, the real AI Safety was all the friends we made along the way.</p>
<p>Um. I need a better way to wrap up this series.</p>
<p>Okay, click this for a really cool <strong>CINEMATIC CONCLUSION:</strong></p>
<style>
#next_button{
    transform: scale(1.3) translate(0,20px);
}
</style>
<p><a
    
    
    
     href="../conclusion" 
    target="_self">
<div id="next_button">
    <div id="nb--crt_lines"></div>
    <div id="nb--static"></div>
    <div id="nb--words">
        
            <b style="font-size:78px">CONCLUSION ‚Üí</b>
        
    </div>
</div>
</a>

</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>wait what are you doing? scroll back up üëÜüëÜüëÜ, the cool ending's in the button up there.</p>
<p>come on, it's just boring footnotes below.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>ugh, fine:</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>The protocol goes something like this:</p>
<ul>
<li>Human trains very-weak Robot_1. Robot_1 is now trusted.</li>
<li>Human, with Robot_1's help, trains a slightly stronger Robot_2. Robot_2 is now trusted.</li>
<li>Human, with Robot_2's help, trains a stronger Robot_3. Robot_3 is now trusted.</li>
<li>(...)</li>
<li>Human, with Robot_N's help, trains a stronger Robot_N+1. Robot_N+1 is now trusted.</li>
</ul>
<p>This way, the Human is always directly training the inner &quot;goals/desires&quot; of the most advanced AI, using only trusted AIs to help them. <a href="#fnref1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Well, maybe. The paper acknowledges many limitations, such as: what if instead of the AIs learning to be <em>logical</em> debaters, they become <em>psychological</em> debaters, exploiting our psychological biases? <a href="#fnref2" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn3" class="footnote-item"><p>inb4 some nerdy nitpicking: yeah yeah <em>perfect</em> prediction is impossible because there's the Halting Problem and chaotic systems bla blah. Look, those are cool, but don't they don't matter to the following segment. Just mentally note that I mean: &quot;this optimal-capabilities AI can predict stuff as well as is theoretically possible&quot;. <a href="#fnref3" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Quote from Yudkowsky's <a href="https://intelligence.org/files/CEV.pdf">Coherent Extrapolated Volition</a> (CEV) paper, which is a similar idea except applied to humanity as a whole; we'll look at CEV more in a later section. <a href="#fnref4" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Source is page 26 of his book <a href="https://en.wikipedia.org/wiki/Human_Compatible">Human Compatible</a>. You can read his shorter paper describing the Future Lives approach <a href="https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf">here</a>. <a href="#fnref5" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn6" class="footnote-item"><p>See: therapy, subconscious desires, non-self-aware people, etc <a href="#fnref6" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf">Hadfield-Menell et al 2016</a> <a href="#fnref7" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn8" class="footnote-item"><p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf">Christiano et al 2017</a> <a href="#fnref8" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://proceedings.mlr.press/v97/shah19a/shah19a.pdf">Shah et al 2019</a> and <a href="https://arxiv.org/pdf/2111.06956">Chan et al 2021</a> <a href="#fnref9" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Very tangential to this AI Safety piece, but I highly recommend <a href="https://www.amazon.ca/Fluent-Forever-Learn-Language-Forget-ebook/dp/B00IBZ405W">Fluent Forever by Gabriel Wyner</a>, which will then teach you Anki flashcards, the phonetic alphabet, ear training, and other great resources for learning any language. <a href="#fnref10" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn11" class="footnote-item"><p>From <a href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.543405/full">Baker et al 2020</a>: ‚ÄúOverall, we found that the AI system is able to provide patients with triage and diagnostic information with a level of clinical accuracy and safety comparable to that of human doctors.‚Äù From <a href="https://medinform.jmir.org/2019/3/e10010/">Shen et al 2019</a>: ‚ÄúThe results showed that the performance of AI was at par with that of clinicians and exceeded that of clinicians with less experience.‚Äù Note these are specialized AIs, not out-the-box LLMs like ChatGPT. Please do not use ChatGPT for medical advice. <a href="#fnref11" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn12" class="footnote-item"><p>See Figure 16 of <a href="https://arxiv.org/pdf/2502.08640">Utility Engineering: Analyzing and Controlling
Emergent Value Systems in AIs</a> <a href="#fnref12" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">In 2023, Jessica Rumbelow &amp; Matthew Watkins</a> found a bunch of words, like &quot;SolidGoldMagikarp&quot; and &quot; petertodd&quot;, which reliably caused GPT-3 to glitch out and produce nonsensical output. &quot;SolidGoldMagikarp&quot; also became the name of an entity in Ari Aster's horror-satire film <em>Eddington (2025)</em>. First time a LessWrong post snuck its way into a major motion picture, afaik. <a href="#fnref13" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf">Takagi &amp; Nishimoto 2023</a>: Used fMRI scans and Stable Diffusion to reconstruct &amp; <strong>view mental imagery(!!!)</strong> <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11940461/">Gkintoni et al 2025</a>: Literature review of using brain-measurement to <strong>read emotions</strong>, with some approaches' accuracy ‚Äúeven surpassing 90% in some studies.‚Äù <a href="#fnref14" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn15" class="footnote-item"><p><a href="https://www.nature.com/articles/35536">Electric current stimulates laughter</a> (1998), and <a href="https://www.jneurosci.org/content/25/3/550">inducing an out-of-body experience</a> (2005). <a href="#fnref15" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn16" class="footnote-item"><p>From <a href="https://arxiv.org/pdf/2301.05217">the paper</a>: &quot;In robustness experiments, we confirm that grokking consistently occurs for other architectures and prime moduli (Appendix C.2). In Section 5.3 we find that <strong>grokking does not occur without regularization</strong>.&quot; (emphasis added) <a href="#fnref16" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn17" class="footnote-item"><p>Jeezus, <a href="https://knowyourmeme.com/memes/xzibit-yo-dawg">this meme</a> is as old as some of my <em>colleagues</em>. <a href="#fnref17" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn18" class="footnote-item"><p>You <em>could</em> use more complex &quot;nonlinear&quot; probes, but if you put too much info-processing power in a probe, you run the risk of measuring info-processing <em>in that probe itself</em>, not the original ANN. So in practice, people usually just use a 1-layer &quot;linear&quot; probe. <a href="#fnref18" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn19" class="footnote-item"><p>Ok in real life, any inserted thermometer <em>has</em> to modify the original thing's temperature, because the themometer takes/gives off heat into the thing. Look, artificial neural networks are in a simulation. We can <em>code</em> it to not modify the original. <a href="#fnref19" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn20" class="footnote-item"><p>(Math warning) A linear classifier is just $y = \text{sigmoid}( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...)$, which is the exact same formula for neurons in a traditional ANN. So, you could (I could) sneakily call a linear classifier a &quot;one-layer neural network&quot;. You <em>could</em> use more complex &quot;nonlinear&quot; probes, but then you run the risk of measuring info-processing <em>in the probe</em>, not the original ANN. (But you can catch this problem by running &quot;placebo tests&quot; on the probe. If it's still able to classify stuff <em>even if you feed it fake, random input</em>, then the probe's so complex, it can just memorize examples.) <a href="#fnref20" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn21" class="footnote-item"><p>(Human example: Alyx dislikes Beau for some other reason. Beau eats crackers. Alyx, even though they wouldn't be offended in any other context, thinks: &quot;Beau is <em>chewing</em> those <em>disgusting salt squares</em> so <em>loudly</em>, intentionally <em>trying</em> to tick me off.&quot; Alyx's <em>conscious</em> chain of thought (annoying eating ‚Üí dislike for Beau) is the opposite of their <em>true</em> subconscious (dislike for Beau ‚Üí finds eating annoying). Anyway ‚Äî as above, so below ‚Äî LLMs <em>also</em> out-loud rationalize their inner biases. <a href="#fnref21" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn22" class="footnote-item"><p><a href="https://arxiv.org/pdf/1707.08945">Robust Physical-World Attacks on Deep Learning Visual Classification (2018)</a> <a href="#fnref22" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn23" class="footnote-item"><p><a href="https://www.anthropic.com/research/small-samples-poison">A small number of samples can poison LLMs of any size</a>: ‚Äúwe demonstrate that by injecting just 250 malicious documents into pretraining data, adversaries can successfully backdoor LLMs ranging from 600M to 13B parameters.‚Äù <a href="#fnref23" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn24" class="footnote-item"><p><a href="https://arxiv.org/abs/2311.14455">Universal Jailbreak Backdoors from Poisoned Human Feedback</a>: ‚Äúthe backdoor embeds a trigger word into the model that acts like a universal &quot;sudo command&quot;: adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt.‚Äù <a href="#fnref24" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn25" class="footnote-item"><p><a href="https://openreview.net/pdf?id=ZIWHEw9yU-">Wang &amp; Gleave et al 2022</a>: ‚ÄúOur [adversarial AIs] do not win by learning to play Go better than KataGo [a superhuman Go AI] ‚Äî in fact, our adversaries are easily beaten by human amateurs. Instead, our adversaries win by tricking KataGo into making serious blunders. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes.‚Äù <a href="#fnref25" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn26" class="footnote-item"><p>See the Impact Regularizer section of <a href="https://arxiv.org/pdf/1606.06565">Amodei &amp; Olah 2016</a> for details <a href="#fnref26" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn27" class="footnote-item"><p><a href="https://www.alignmentforum.org/posts/GC69Hmc6ZQDM9xC3w/musings-on-the-speed-prior">Hubinger 2022</a> <a href="#fnref27" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn28" class="footnote-item"><p><a href="https://proceedings.mlr.press/v48/gal16.pdf">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a> by Gal &amp; Cambridge 2016 <a href="#fnref28" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn29" class="footnote-item"><p>Fun anecdote: I first learnt about <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Networks</a> a decade ago in a Google Meet call (then Google Hangouts). I raised my hand to ask, &quot;wait, we're generating images by‚Ä¶ training one intelligence to deceive another intelligence?&quot; And we all <em>laughed</em>. Ohhhhhh how we all <em>laughed.</em> Ha ha. Ha ha ha. Ha ha HA ha ha. Haaaaaaaaaaa. <a href="#fnref29" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn30" class="footnote-item"><p>For example: an attacker LLM tries to create sneaky prompts that &quot;jailbreak&quot; a defender LLM into turning evil. In traditional Adversarial Training, the defender may only learn to protect against specific jailbreaks. In Relaxed/Latent Adversarial Training, the defender LLM may learn more general protective lessons, like, &quot;<a href="https://github.com/elder-plinius/L1B3RT4S/blob/main/ANTHROPIC.mkd">be suspicious of weirdly-formatted instructions</a>&quot;.</p>
<p>Here's <a href="https://arxiv.org/pdf/2403.05030">an empirical paper</a> showing a proof-of-concept for relaxed/latent adversarial training: ‚ÄúIn this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities <em>without leveraging knowledge of what they are or using inputs that elicit them.</em> [...] Specifically, we use LAT to remove backdoors and defend against held-out classes of adversarial attacks.‚Äù (emphasis added) <a href="#fnref30" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn31" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Red_team">Red-teaming</a> has been a core pillar of national/physical/cyber security since the 1960s! Yes, Cold War times. &quot;Red&quot; for Soviet, I guess?? (Factcheck edit: actually, the &quot;red = attacker, blue = defender&quot; convention <em>pre-dates</em> the Cold War! The earliest known instance is <a href="https://english.stackexchange.com/a/583045">from an early-1800s Prussian war-simulation game, <em>Kriegsspiel</em></a>.) <a href="#fnref31" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn32" class="footnote-item"><p><a href="https://arxiv.org/pdf/1911.08731">Sagawa &amp; Koh 2020</a> <a href="#fnref32" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn33" class="footnote-item"><p><a href="https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/">Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad</a> (2025) <a href="#fnref33" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn34" class="footnote-item"><p><a href="https://arxiv.org/pdf/2503.23674">Large Language Models Pass the Turing Test</a>. <em>‚ÄúWhen prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73% of the time‚Äù</em> (vs 50% chance). Though to be fair, it was &quot;only&quot; a 5-minute long Turing Test, and the reason GPT-4.5 fooled humans so often was less because it was good, and more because Human Judges picked ineffective bot-detection strategies (see Figure 4), e.g. asking about daily activities &amp; opinions, instead of strange questions or jailbreaks. <a href="#fnref34" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn35" class="footnote-item"><p><a href="https://www.nature.com/articles/s41598-024-76900-1">AI-generated poetry is indistinguishable from human-written poetry and is rated more favorably</a>. ‚Äúparticipants performed below chance levels in identifying AI-generated poems [...] more likely to judge AI-generated poems as human-authored than actual human-authored poems [...] AI-generated poems were rated more favorably.‚Äù</p>
<p>The human poets included the greats like Shakespeare, Whitman &amp; Plath. The AI was ChatGPT 3.5 <em>with no cherry-picking</em>; they just picked the first few poems generated &quot;in the style of X&quot;.</p>
<p>We are so cooked. <a href="#fnref35" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn36" class="footnote-item"><p><a href="https://journals.plos.org/mentalhealth/article?id=10.1371/journal.pmen.0000145">Hatch et al 2025</a>: ‚Äúa) participants could rarely tell the difference between responses written by ChatGPT and responses written by a therapist, b) the responses written by ChatGPT were generally rated higher in key psychotherapy principles‚Äù</p>
<p><em>Okay but REAL therapists can tell the difference</em>, you may protest. Well. From <a href="https://www.tandfonline.com/doi/pdf/10.1080/10447318.2024.2385001">Human-Human vs Human-AI Therapy: An Empirical Study</a>: <em>‚ÄúTherapists were accurate only 53.9% of the time, no better than chance, and rated the human-AI transcripts as higher quality on average.‚Äù</em></p>
<p>We are so, so cooked. <a href="#fnref36" class="footnote-backref">‚Ü©Ô∏é</a> <a href="#fnref36:1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn37" class="footnote-item"><p>Review of the contest, by Ozy Brennan: <a href="https://thingofthings.substack.com/p/on-the-ai-fiction-turing-test">AIs can beat human fiction writers because humans are bad at writing fiction: piss-on-the-poor reading comprehension</a>. Like the LLM-beats-Turing-Test study, this result (while impressive), is less &quot;AI is really good&quot; and more &quot;humans are really bad&quot;. And these human writers weren't novices either, they're professionals who've published books.</p>
<p>We are so, so, <em>so</em> cooked. <a href="#fnref37" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn38" class="footnote-item"><p><a href="https://www.anthropic.com/research/project-vend-1">Project Vend: Can Claude run a small shop?</a> Answer: no. Claude started stocking the food vending machine with tungsten cubes, and hallucinated that it was a flesh-and-blood delivery person who went the Simpsons' house. <a href="https://andonlabs.com/evals/vending-bench-2">Vending Bench 2</a> measures how well all the frontier LLMs can do on this &quot;run a vending machine for a year&quot; task. As of writing (Dec 2025), they're at least making <em>positive</em> amounts of money now, but a) the simulated &quot;buyers&quot; aren't <em>trying</em> to jailbreak the LLM, the way real human buyers did in Project Vend, and b) the best AI in Vending Bench 2 is still making, like, 8% of the &quot;good&quot; baseline. <a href="#fnref38" class="footnote-backref">‚Ü©Ô∏é</a> <a href="#fnref38:1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn39" class="footnote-item"><p><a href="https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon">So how well is Claude playing Pok√©mon?</a> <em>‚ÄúTL;DR: Pretty badly. Worse than a 6-year-old would.‚Äù</em> Explanation why: ‚ÄúBasically, while Claude is pretty good at short-term reasoning (ex. Pok√©mon battles), he's bad at executive function and has a poor memory. This is despite <a href="https://excalidraw.com/#json=WrM9ViixPu2je5cVJZGCe,no_UoONhF6UxyMpTqltYkg">a great deal of scaffolding</a>, including a knowledge base, a critic Claude that helps it maintain its knowledge base, and a variety of tools to help it interact with the game more easily.‚Äù <a href="#fnref39" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn40" class="footnote-item"><p>[Embers of Autoregression (2024)] shows how GPT-4 could do the <em>common</em> <a href="https://en.wikipedia.org/wiki/ROT13">ROT-13</a> cipher, but utterly fail at the <em>uncommon</em>-but-trivially-similar ROT-12 cipher. As the <a href="https://www.pnas.org/action/downloadSupplement?doi=10.1073%2Fpnas.2322420121&amp;file=pnas.2322420121.sapp.pdf">Supplementary Info shows (Figure S13)</a>, this holds even with &quot;chain of thought&quot; reasoning turned on. Though to be fair, this paper was from 2024, and I tried ROT-12 and ROT-11 Claude Sonnet 4.5 just now, and it did fine. Though as we'll see later in this section, Claude &amp; LLMs chains-of-thought are <em>still</em> very fragile. <a href="#fnref40" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn41" class="footnote-item"><p><a href="https://arcprize.org/">ARC-AGI</a> is a bunch of pixel puzzle games. Instead of most games where you're told the rules up front, in ARC-AGI, <em>you have to learn the hidden rules via exploration,</em> then prove you understand it by applying the rules to win the game.</p>
<p><a href="https://arcprize.org/leaderboard">See the Leaderboard Breakdown</a>: The Ten-Human Panel's performance on ARC-AGI-1 and -2 is 98% and 100%, costing \$17/task (~10 workers hired per task on Mechanical Turk, success if <em>at least one</em> succeeds).</p>
<p>The best AI (Gemini 3 Deep Think) on ARC-AGI-1 and -2 is 87.5% (a bit worse) and 45.1% (<em>much</em> worse) costing \$77/task (almost <em>five times</em> more expensive than hiring <em>ten humans on MTurk</em>) <a href="#fnref41" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn42" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/File:Tower_of_Hanoi.jpeg">Photo by User:Evanherk</a> <a href="#fnref42" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn43" class="footnote-item"><p>Actually, if you're interested in the developmental psychology of kids playing Tower of Hanoi, here's the landmark paper by <a href="https://link.springer.com/content/pdf/10.3758/BF03329485.pdf">Byrnes &amp; Spitz 1979</a>. See Figure 1: around age 8, kids have near-perfect performance on the 2-stack version. Around age 14, kids plateau out at okay performance on the 3-stack. Unfortunately I couldn't find any paper that gives performance scores for 4+ disks in general-population children/adults. Sorry. <a href="#fnref43" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn44" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/File:Iterative_algorithm_solving_a_6_disks_Tower_of_Hanoi.gif">Created by User:Trixx</a> <a href="#fnref44" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn45" class="footnote-item"><p>The solution to Level N involves doing the solution to the previous level <em>twice</em> (moving the N-1 stack from Peg A to Peg B, then Peg B to Peg C), plus one extra move (moving the biggest disk from Peg A to Peg C).</p>
<p>So how many moves do you need to solve N disks? Let's crunch the numbers:</p>
<p>For 1-disk, <strong>1 move</strong></p>
<p>For 2-disk, 1 * 2 + 1 = <strong>3 moves</strong> (previous solution <em>twice</em> + one extra move)</p>
<p>For 3-disk, 3 * 2 + 1 = <strong>7 moves</strong></p>
<p>For 4-disk, 7 * 2 + 1 = <strong>15 moves</strong></p>
<p>For 5-disk, 15 * 2 + 1 = <strong>31 moves</strong></p>
<p>For 6-disk, 31 * 2 + 1 = <strong>63 moves</strong></p>
<p>For 7-disk, 63 * 2 + 1 = <strong>127 moves</strong></p>
<p>For 8-disk, 127 * 2 + 1 = <strong>255 moves</strong></p>
<p>Do you see the pattern?</p>
<p><strong>For N disks, you need 2^N - 1 moves!</strong></p>
<p>(<a href="https://en.wikipedia.org/wiki/Mathematical_induction">Proof by induction</a>: Let F(N) = # of moves needed for N disks. Let's say we already know F(N) = 2^N - 1. Then doubling that &amp; adding one, we get (2^N - 1)*2 + 1 = 2^(N+1) - 2 + 1 = 2^(N+1) - 1, which is the F(N+1) we want! All's left to do is prove the base case: F(1) = 2^1 - 1 = 2 - 1 = 1. And indeed, yes it takes 1 move to solve the 1-stack. Q.E.D.)</p>
<p>Yippee! This was really not worth writing the footnote for, but there you go. <a href="#fnref45" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn46" class="footnote-item"><p>Famous phrase coined by <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">Emily Bender et al 2021</a> <a href="#fnref46" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn47" class="footnote-item"><p>As Judea Pearl ‚Äî winner of the Turing Prize, the &quot;Nobel Prize of Computer Science&quot; ‚Äî once told Quanta Magazine in their article, <a href="https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/">To Build Truly Intelligent Machines, Teach Them Cause and Effect</a>: <em>‚ÄúAs much as I look into what‚Äôs being done with deep learning, I see they‚Äôre all stuck there on the level of associations. Curve fitting. [...] no matter how skillfully you manipulate the data and what you read into the data when you manipulate it, it‚Äôs still a curve-fitting exercise, albeit complex and nontrivial.‚Äù</em> <a href="#fnref47" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn48" class="footnote-item"><p>Beloved &quot;thinking in gears&quot; metaphor comes from <a href="https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding">Valentine (2017)</a> <a href="#fnref48" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn49" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Inductive_reasoning">Inductive reasoning</a> is stuff like &quot;the Sun has risen every day for the last 10,000 days I've been alive, therefore the Sun will almost definitely rise again tomorrow&quot;. It's not <em>technically</em> a logical deduction ‚Äî it's <em>logically</em> possible the Sun could vanish tomorrow ‚Äî but, statistically, yeah it's probably fine. <a href="#fnref49" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn50" class="footnote-item"><p>The ‚Äúdual-process‚Äù model of cognition was first suggested by <a href="https://pages.ucsd.edu/~scoulson/203/wason-evans.pdf">(Wason &amp; Evans, 1974)</a>, developed by multiple folks over decades, and<em>really</em> popular in Daniel Kahneman's bestselling 2011 book, <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast &amp; Slow</a>. As for the naming: Intuition is #1 and Logic is #2, because pattern-recognition evolved before human-style deliberative logic. <a href="#fnref50" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn51" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Abductive_reasoning">Abductive reasoning</a> is the backbone of science. It's when you generate &quot;the most likely hypothesis&quot; to novel data. It's not <em>deduction;</em> it's very logically possible your hypothesis could be false. And it's not <em>induction;</em> you haven't seen any direct repeated evidence for your hypothesis <em>yet</em>, not until you test it.</p>
<p>(Example: <a href="https://en.wikipedia.org/wiki/Michelson%E2%80%93Morley_experiment">Michelson-Morley</a> discovered that the speed of light seemed to be constant in every direction, and from this, Einstein <em>abducted</em> that time is relative! Yes, &quot;time is relative&quot; was the <em>most</em> likely hypotheis he had for &quot;light speed seems to be constant&quot;, and hey, he was right.)</p>
<p>(Nitpick: Einstein claimed he didn't know about the Michelson-Morley experiment, and he instead abducted relativity from the Maxwell Equations, but look, I'm trying to tell a simple story here ok?) <a href="#fnref51" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn52" class="footnote-item"><p>&quot;Hyperpolation&quot; coined by <a href="https://arxiv.org/pdf/2409.05513">Toby Ord 2024</a>. <a href="#fnref52" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn53" class="footnote-item"><p>Maybe this can be solved by writing a <em>normal</em> program, that then writes you &quot;synthetic data&quot; for a business operating successfully over 365 days, that you can fine-tune an LLM on? I dunno, this still feels like the special pleading of &quot;yes I can add 7 digits &amp; bomb at 8, but if you give me more training I can do it!&quot; If you pass 7 but bomb 8, there's something fundamentally wrong that &quot;more scale&quot; don't fix. <a href="#fnref53" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn54" class="footnote-item"><p><a href="https://arxiv.org/pdf/2503.14499">METR (2025)</a>. ‚Äú[software engineering] tasks that AI models can complete with 50% success rate [are] doubling approximately every seven months since 2019.‚Äù And Figure 13 shows that the cost of completing these tasks remains pretty stable even as task length grows, around 1% of a &quot;Level 4&quot; Google salary (\$143.61/hour, so, \$1.43/hour for an AI.) However, it's important to note the coding tasks in the benchmark are, by design, common tasks. <a href="#fnref54" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn55" class="footnote-item"><p><a href="https://arcprize.org/">From the ARC-AGI frontpage.</a> It's missing the newest Gemini, <a href="https://arcprize.org/leaderboard">but it doesn't push the frontier much</a>, and the frontier still bends downwards. <a href="#fnref55" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn56" class="footnote-item"><p>From Sutton's famous <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">2019 essay The Bitter Lesson</a>: ‚Äú<em>One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great.</em>‚Äù But in <a href="https://www.youtube.com/watch?v=21EYKqUsPfg">a 2025 podcast with Dwarkesh Patel</a>, he believes LLMs are &quot;a dead end&quot;, because they imitate without building &quot;robust world models&quot;. <a href="#fnref56" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn57" class="footnote-item"><p>In 2020, <a href="https://www.lesswrong.com/posts/QZM6pErzL7JwE3pkv/shortplav">niplav</a> found that Claude Sonnet 3.5 could reproduce a common benchmark's &quot;canary string&quot;. In 2024, <a href="https://www.alignmentforum.org/posts/kSmHMoaLKGcGgyWzs/big-bench-canary-contamination-in-gpt-4">Jozdien</a> found that GPT-4 (base model) can too. The reason this is bad is because it's trivially easy to filter out all documents with the canary string ‚Äî just search if it's in a document, then leave out the document if it is. The canary string <em>itself</em> isn't a cheat, but it hints at a high probability that the training data was contaminated with the benchmark's answers, making LLMs' performance on those benchmarks untrustworthy. <a href="#fnref57" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn58" class="footnote-item"><p>From <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.2403116121">Hackenburg &amp; Margetts 2024</a>: ‚ÄúEarly research suggests that most capable LLMs can directly persuade humans on political issues (5), draft more persuasive public communications than actual government agencies and political communication experts (6, 7), and generate convincing disinformation and fake news (1, 8, 9). Across these domains, humans are no longer able to consistently distinguish human and LLM-generated texts (8, 10).‚Äù <a href="#fnref58" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn59" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Cat_burning">Cat Burning on Wikipedia</a>. Reeeaaally glad there's no photos on that page. (The Wikipedia Talk page notes there's academic debate about how widespread <em>this specific</em> act really was, but there's plenty of examples in History of &quot;normal people&quot; being awful.) <a href="#fnref59" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn60" class="footnote-item"><p><a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">Constitutional AI: Harmlessness from AI Feedback</a> (2022) <a href="#fnref60" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn61" class="footnote-item"><p><a href="https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input">Collective Constitutional AI: Aligning a Language Model with Public Input</a> (2023) <a href="#fnref61" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn62" class="footnote-item"><p>Deontology says you should be honest with a Nazi who wants to find hidden Jews, Utilitarianism says you should torture one person to prevent 10<sup>100</sup> people getting dust in their eyes, and Rational Egoism would be okay with you walking past an easily-savable drowning child.</p>
<p>But I struggle to think of <em>any</em> scenario that fails at <em>all three</em> moral theories: an action that obeys widely-accepted rules &amp; duties, improves others' well-being, <em>and</em> your own well-being, and yet is <em>still &quot;wrong&quot;</em>. Point is: the <em>ensemble</em> is more robust than any individual theory alone.</p>
<p>(What about Virtue Ethics? Is there a scenario where &quot;be wise&quot; fails? Well, no, but that's because virtue ethics is extremely vague &amp; kinda useless for guidance. The big names in virtue ethics, Aristotle &amp; Aquinas, supported slavery. You can promote/demote any action you want be using the sloppy labels of &quot;wise&quot;/&quot;unwise&quot;.) <a href="#fnref62" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn63" class="footnote-item"><p><a href="https://cdn.aaai.org/ocs/ws/ws0209/12624-57414-1-PB.pdf">Using Stories to Teach Human Values to Artificial Agents</a> by Riedl &amp; Harrison 2016. <a href="#fnref63" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn64" class="footnote-item"><p><a href="https://intelligence.org/files/CEV.pdf">Eliezer Yudkowsky 2004</a> <a href="#fnref64" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn65" class="footnote-item"><p>&quot;Normativity&quot; ~= morality, &quot;Indirect&quot; = well, not direct. See this <a href="https://forum.effectivealtruism.org/topics/indirect-normativity">glossary entry</a>. <a href="#fnref65" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn66" class="footnote-item"><p>Okay there's technically no such thing as &quot;the&quot; scientific method, every field does something slightly different, and methods evolve, but you know what I mean. There's a &quot;family resemblance&quot;. The general process of &quot;notice stuff, guess why it's happening, test your guesses, repeat&quot;. <a href="#fnref66" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn67" class="footnote-item"><p><a href="https://aligned.substack.com/p/a-proposal-for-importing-societys-values">Jan Leike 2023</a>: A proposal for importing society‚Äôs values: Building towards Coherent Extrapolated Volition with language models. <a href="#fnref67" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn68" class="footnote-item"><p><a href="https://arxiv.org/pdf/2411.10109">Park et al 2024</a>: &quot;We [simulate] the attitudes and behaviors of
1,052 real individuals [...] The [AI clones] replicate participants' responses on the General Social Survey 85% as accurately <em>as participants replicate their own answers two weeks later</em>, and perform comparably in predicting personality traits and outcomes in experimental replications.&quot; (emphasis added) <a href="#fnref68" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn69" class="footnote-item"><p>Coherent Blended Volition was coined in <a href="https://jetpress.org/v22/goertzel-pitt.htm">Goertzel &amp; Pitt 2012</a>: ‚ÄúThe CBV of a diverse group of people should not be thought of as an average of their perspectives, but [...] incorporating the most essential elements of their divergent views into a whole that is overall compact, elegant and harmonious. [...] The core difference between [CEV and CBV] is that, in the CEV vision, the extrapolation and coherentization are to be done by a highly intelligent, highly specialized software program, whereas in [CBV], these are to be carried out by collective activity of humans as mediated by [tools for healthy, deep discussion]. Our perspective is that the definition of collective human values is probably better carried out via human collaboration, rather than delegated to a machine optimization process.‚Äù <a href="#fnref69" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn70" class="footnote-item"><p><a href="https://arxiv.org/html/2510.21133v1">Quantifying CBRN Risk in Frontier Models</a>: ‚Äú[Frontier LLMs] pose unprecedented dual-use risks through the potential proliferation of chemical, biological, radiological, and nuclear (CBRN) weapons knowledge. [...] Our findings expose critical safety vulnerabilities: [...] Model safety performance varies dramatically from 2% (claude-opus-4) to 96% (mistral-small-latest) attack success rates; and eight models exceed 70% vulnerability when asked to enhance dangerous material properties. We identify fundamental brittleness in current safety alignment, where simple prompt engineering techniques bypass safeguards for dangerous CBRN information.‚Äù <a href="#fnref70" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn71" class="footnote-item"><p><a href="https://eqbench.com/spiral-bench.html">Spiral-Bench</a>, a benchmark for measuring LLM sycophancy &amp; delusion amplification. This website also hosts the <a href="https://eqbench.com/index.html">Emotional Intelligence Benchmark</a> and <a href="https://eqbench.com/slop-score.html">Slop Score</a> <a href="#fnref71" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn72" class="footnote-item"><p><a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Measuring AI Ability to Complete Long Tasks</a> by <a href="https://metr.org/">METR</a>, Model Evaluation &amp; Threat Research <a href="#fnref72" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn73" class="footnote-item"><p><a href="https://weval.org/">Weval</a> is a platform to make &amp; share your own AI Evaluations, especially on topics ignored by AI &amp; AI Safety communities, like <a href="https://weval.org/analysis/evidence-based-ai-tutoring/6398a4d26793af13/2025-10-05T07-09-36-338Z">&quot;which AIs are evidence-based tutors?</a>&quot; <a href="#fnref73" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn74" class="footnote-item"><p><a href="https://kkc.com/media/leaked-sec-whistleblower-complaint-challenges-openai-on-illegal-non-disclosure-agreements/">Leaked SEC Whistleblower Complaint¬†Challenges OpenAI on Illegal Non-Disclosure Agreements</a> (2024) <a href="#fnref74" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn75" class="footnote-item"><p><a href="https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance">Toby Ord 2025: Inference Scaling Reshapes AI Governance</a>: ‚Äú<em>Rapid scaling of inference-at-deployment would: lower the importance of open-weight models (and of securing the weights of closed models), reduce the impact of the first human-level models, change the business model for frontier AI, reduce the need for power-intense data centres, and derail the current paradigm of AI governance via training compute thresholds.</em>‚Äù <a href="#fnref75" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn76" class="footnote-item"><p>For example, the Good Judgment Project &amp; Metaculus, both with proven track records of being some of the best in general forecasting of future events, predict a <a href="https://goodjudgment.com/superforecasting-ai/">1% chance</a> and <a href="https://possibleworldstree.com/">3% chance</a> of near-extinction risk from advanced AI this century. (&quot;Oh that doesn't sound too bad,&quot; you think, yeah wait til you see the % of non-extinction and/or non-AI risks.) Also, Metaculus is forecasting a median (with wide uncertainty) of AGI <a href="https://www.metaculus.com/questions/5121/date-of-general-ai/">by 2033</a>, and AGI becoming as species-changing as the agrilcultural/industral revolution <a href="https://www.metaculus.com/questions/19356/transformative-ai-date/">by 2044</a>. <a href="#fnref76" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn77" class="footnote-item"><p><a href="https://dl.acm.org/doi/pdf/10.1145/3707649">AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</a> ‚Äì ‚ÄúParticipants (N = 991) answered
a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our <strong>preregistered</strong> analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that <strong>the superforecasting assistant increased accuracy by 41%</strong>‚Äù <a href="#fnref77" class="footnote-backref">‚Ü©Ô∏é</a> <a href="#fnref77:1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn78" class="footnote-item"><p><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a> (2023) Inspired by the US government's Biosafety Levels (BSL), they defined ASL-1 (old chess AIs), ASL-2 (current LLMs), ASL-3 <a href="#fnref78" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn79" class="footnote-item"><p><a href="https://ora.ox.ac.uk/objects/uuid:b481e9ad-bc27-4550-87ca-f414354aeb35/files/s2v23vw012">Sandbrink et al 2022</a> <a href="#fnref79" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn80" class="footnote-item"><p>From Stuart Russell, the co-author of the most-popular textbook in AI: <a href="https://people.eecs.berkeley.edu/~russell/papers/issues22-laws.pdf">Banning Lethal Autonomous Weapons: An Education</a> <a href="#fnref80" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn81" class="footnote-item"><p>Quote from Tenet (2020), the most &quot;yup that's a Nolan film&quot; Nolan film. <a href="#fnref81" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn82" class="footnote-item"><p>For example, <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf">Reinforcement Learning from Human Preferences (RHLF),</a> a way for AIs to import human values, was what turned Base GPT (an autocomplete) into <em>Chat</em>GPT (an actual chatbot). Likewise, improving the robustness of AI &quot;intuition&quot; would help make better AI medical diagnosis, and self-driving cars. <a href="#fnref82" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn83" class="footnote-item"><p><a href="https://www.vox.com/future-perfect/21493812/smallpox-eradication-vaccines-infectious-disease-covid-19">Smallpox used to kill millions of people every year. Here‚Äôs how humans beat it.</a> by Kelsey Piper <a href="#fnref83" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn84" class="footnote-item"><p>Source: <a href="https://ourworldindata.org/child-mortality">Our World In Data</a> <a href="#fnref84" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn85" class="footnote-item"><p>March 2025 on MIT News: <a href="https://news.mit.edu/2025/study-healing-ozone-hole-global-reduction-cfcs-0305">Study: The ozone hole is healing, thanks to global reduction of CFCs</a>. &quot;New results show with high statistical confidence that ozone recovery is going strong.&quot; <a href="#fnref85" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn86" class="footnote-item"><p><a href="https://x.com/AlexBlechman/status/1457842724128833538">classic 2021 tweet</a> by alex blechman <a href="#fnref86" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn87" class="footnote-item"><p>Opening metaphor from <a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">Nick Bostrom's classic 2014 book, Superintelligence</a> <a href="#fnref87" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn88" class="footnote-item"><p><a href="https://forum.effectivealtruism.org/topics/comprehensive-ai-services">Glossary entry on CAIS</a> <a href="#fnref88" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn89" class="footnote-item"><p><a href="https://arxiv.org/abs/2502.15657">Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?</a> by Bengio et al 2025 <a href="#fnref89" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn90" class="footnote-item"><p>From <a href="https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety">Chris Olah‚Äôs views on AGI safety, by Evan Hubinger</a>: ‚ÄúWe could use ML as a <em>microscope</em>‚Äîa way of learning about the world without directly taking actions in it. That is, rather than training an RL agent, you could train a predictive model on a bunch of data and use interpretability tools to inspect it and figure out what it learned, then use those insights to inform‚Äîeither with a human in the loop or in some automated way‚Äîwhatever actions you actually want to take in the world.‚Äù <a href="#fnref90" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn91" class="footnote-item"><p><a href="https://cdn.aaai.org/ocs/ws/ws0198/12613-57416-1-PB.pdf">Quantilizers: A Safer Alternative to Maximizers for Limited Optimization</a> by Jessica Taylor 2016 <a href="#fnref91" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn92" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart's Law</a>, paraphrased: &quot;<em>When you reward a metric, it usually gets gamed.</em>&quot; <a href="#fnref92" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn93" class="footnote-item"><p><a href="https://www.academia.edu/download/51625741/An_anatomical_signature_for_literacy._Na20170203-24453-1k92usb.pdf">An anatomical signature for literacy</a> by Carreiras et al 2009 <a href="#fnref93" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn94" class="footnote-item"><p>Citing myself here, ehehehe: <a href="https://jods.mitpress.mit.edu/pub/issue3-case/release/6">How To Become A Centaur</a> by Nicky Case (2018) Though, see next footnote, the main analogy is now obsolete. <a href="#fnref94" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn95" class="footnote-item"><p><a href="https://www.lesswrong.com/posts/sTboWTyf9MfERnsKp/gwern-about-centaurs-there-is-no-chance-that-any-useful-man">From Gwern</a>: ‚ÄúAs far as I can tell, in chess, the centaur era lasted barely a decade, and would've been shorter still had anyone been seriously researching computer chess rather than disbanding research after Deep Blue or the centaur tournaments kept running instead of stopping a while ago.‚Äù  <a href="https://en.wikipedia.org/wiki/Advanced_chess">In 2017</a>: ‚ÄúA computer engine (Zor) ended first in the freestyle Ultimate Challenge tournament (2017), while the first centaur (Thomas A. Anderson, Germany) ranked in 3rd place‚Äù. Though as of 2023, <a href="https://www.engadget.com/human-convincingly-beats-ai-at-go-with-help-from-a-bot-100903836.html">Human+AI can still beat pure AI in Go</a>, but mostly due to exploiting the Robustness issues in modern deep-learning. Whether or not you count that as &quot;cheating&quot; is up to you &amp; your sense of aesthetics. <a href="#fnref95" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn96" class="footnote-item"><p><a href="https://www.reddit.com/r/StableDiffusion/comments/12pcbne/i_mad_a_python_script_the_lets_you_scribble_with/">I made a python script the lets you scribble with Stable Diffusion in realtime</a> <a href="#fnref96" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>

	</article>

    <!-- FOOTER -->
    
    <div id="post_credits">
        
<p>
    <a href="#AllFeetnotes">: See all feetnotes üë£</a>
</p>
<p>
    Also, expandable "Nutshells" that make good standalone bits:
</p>

<p>
    <a href="#SwissCheese">: More about the Swiss Cheese Model</a>
    <br>
    <a href="#RobustChainMath">: The math of robust scalable oversight</a>
    <br>
    <a href="#ScalableOversightExtras">: Scalable Oversight, extra notes</a>
    <br>
    <a href="#IDA">: Iterated Distillation & Amplification, explained</a>
    <br>
    <a href="#CriticalMassComic">: (deleted scene) "Critical Mass" Comic</a>
    <br>
    <a href="#FutureLivesFixes">: Fixes for the "Future Lives" Algorithm</a>
    <br>
    <a href="#AISelfMod">: Informal literature review of "Self-modifying Agents"</a>
    <br>
    <a href="#WorstOrAverage">: Pros & Cons of Worst/Average/Best-Case optimization</a>
    <br>
    <a href="#LearnValuesExtraNotes">: Learning our values, extra notes</a>
    <br>
    <a href="#EconomicsAI">: Economics of AI</a>
    <br>
    <a href="#HumanLLMAdvantages">: Advantages of Humans vs LLMs</a>
</p>

    </div>

</div>
</body>
</html>

<!-- Load these scripts last. Screw 'em. -->
<!-- Orbit: make memory a choice -->
<script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
