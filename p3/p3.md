<div style="text-align:center"><i>(This is Part 3 of a series on AI Safety! You don't</i> have <i>to read the previous parts ‚Äî <a href="https://aisafety.dance/">Intro</a>, <a href="https://aisafety.dance/p1">Part 1</a>, <a href="https://aisafety.dance/p2">Part 2</a> ‚Äî but they'll help!)</i></div>

So, after writing 40,000+ words on how weird & difficult AI Safety is... how am I feeling about the chances of humanity solving this problem?

...pretty optimistic, actually!

No, really!

Maybe it's just cope. But in my opinion, if *this* is the space of all the problems:

![Drawing of a faintly outlined blob, labelled "the whole problem space"](../media/p3/intro/problems.png)

Then: although no *one* solution covers the whole space, *the entire problem space* is covered by one (or more) promising solutions:

![Same outline, but entirely overlapped by small colorful circles, each one representing a different solution](../media/p3/intro/solutions.png)

**We don't need One Perfect Solution; we can stack several imperfect solutions!** This is similar to the [Swiss Cheese Model](https://en.wikipedia.org/wiki/Swiss_cheese_model) in Risk Analysis ‚Äî each layer of defense has holes, but if you have enough layers with holes in _different_ places, risks can't go all the way through:

![Rays going through layers of swiss cheese, which have holes in them. The rays can easily go through the holes of _one_ layer of cheese, but not all of them.](../media/p3/intro/cheese.png)

( [: üßÄ Bonus section: counter-arguments & counter-counter-arguments on the Swiss Cheese Model](#SwissCheese) ‚Üê _optional: whenever you see a dotted-underlined section, you can click to expand it!_ )

*This does not mean AI Safety is 100% solved yet* ‚Äî we still need to triple-check these proposals, and get engineers/policymakers to even *know* about these solutions, let alone implement them. But for now, I'd say: "lots of work to do, but lots of promising starts"!

As a reminder, here's how we can break down the main problems in AI & AI Safety:

![Breakdown: "How can we align AI to humane values?" breaks down into Problems in the AI (technical alignment, game theory, deep learning) and Problems in the Humans (whose values?, coordinating around ai)](../media/p3/intro/breakdown.png)

So in this Part 3, we'll learn about the most-promising solution(s) for each part of the problem, while being honest about their pros, cons, and unknowns:

**ü§ñ Problems in the AI:**

* <u>Scalable Oversight</u>: How can we safely check AIs, even when they're *far* more advanced than us? [‚Ü™](#oversight)
* <u>Solving AI Logic</u>: AI should aim for our "future lives" [‚Ü™](#future), and learn our values with uncertainty [‚Ü™](#uncertain).
* <u>Solving AI "Intuition"</u>: AI should be easy to "read & write" [‚Ü™](#interpretable), be robust [‚Ü™](#robust), and think in cause-and-effect. [‚Ü™](#causality)

**üò¨ Problems in the Humans**:

* <u>Humane Values</u>: Which values, *whose* values, should we put into AI, and how? [‚Ü™](#humane)
* <u>AI Governance</u>: How can we coordinate humans to manage AI, from the top-down and/or bottom-up? [‚Ü™](#governance)

**üåÄ Working *around* the problems**:

* <u>Alternatives to AGI</u>: How about we just don't make the Torment Nexus? [‚Ü™](#alt)
* <u>Cyborgism</u>: If you can't beat 'em, join 'em! [‚Ü™](#cyborg)

( If you'd like to skip around, the <img src="../media/intro/icon1.png" class="inline-icon"> Table of Contents are to your right! üëâ You can also <img src="../media/intro/icon2.png?v=3" class="inline-icon"> change this page's style, and <img src="../media/intro/icon3.png?v=2" class="inline-icon"> see how much reading is left. )</p>

Quick aside: **this final Part 3, published on December 2025,** was supposed to be posted 12 months ago. But due to a bunch of personal shenanigans I don't want to get into, I was delayed. Sorry you've waited a year for this finale! On the upside, there's been lots of progress & research in this field since then, so I'm excited to share all that with you, too.

Alright, let's dive in! No need for more introduction, or weird stories about cowboy catboys, let's just‚Äî

#### :x Swiss Cheese

[The famous Swiss Cheese Model](https://en.wikipedia.org/wiki/Swiss_cheese_model) from Risk Analysis tells us: **you don't need _one_ perfect solution, you can stack _several_ imperfect solutions.**

This model's been used _everywhere_, from aviation to cybersecurity to pandemic resilience. An imperfect solution has "holes", which are easy to get through. But stack enough of them, with holes in different places, and it'll be near-impossible to bypass.

But, let's address two critiques of the Swiss Cheese model:

[Critique One](https://www.eurocontrol.int/publication/revisiting-swiss-cheese-model-accidents) ‚Äî this assumes the "holes" in the solutions are _independent_. If there's any _one_ hole that all solutions share, then a problem can slip through all of them. Fair enough. **This is why the Proposed Solutions on this page try to be as diverse as possible.** (See below section: Robustness > Diversity)

Critique Two ‚Äî more relevant to AI Safety ‚Äî is it assumes the problem is not an intelligent agent. [A quote from Nate Soares](https://archive.is/20250921161137/https://www.vox.com/future-perfect/461680/if-anyone-builds-it-yudkowsky-soares-ai-risk):

> ‚ÄúIf you ever make something that is trying to get to the stuff on the other side of all your Swiss cheese, it‚Äôs not that hard for it to just route through the holes.‚Äù

I accept this counterargument when it comes to defending against an _already_-misaligned superintelligence. But if we're talking about _growing a trusted intelligence from scratch_, then the Swiss Cheese Model still makes sense at each step, *and*, you can use each iteration of a trusted AI as an extra "layer of cheese" for training better iterations of that AI. (See below section: Scalable Oversight)

As AI researcher Jan Leike [puts it](https://aligned.substack.com/p/should-we-control-ai):

> More generally, we should actually solve alignment instead of just trying to control misaligned AI. [...] Don‚Äôt try to imprison a monster, build something that you can actually trust!

---

<a id="oversight"></a>

## Scalable Oversight

This is Sheriff Meowdy, the cowboy catboy:

![Drawing of Sheriff Meowdy](../media/p3/so/meowdy0001.png)

One day, the Varmin strode into town:

![Sheriff Meowdy staring down a bunch of Jerma rats strolling towards him](../media/p3/so/meowdy0002.png)

Sharpshootin' as the Sheriff was, he's man enough (catman enough) to admit when he needs backup. So, he makes a robot helper ‚Äî Meowdy 2.0 ‚Äî to help fend off the Varmin:

![Robot version of Sheriff Meowdy, labelled "Meowdy 2.0"](../media/p3/so/meowdy0003.png)

Meowdy 2.0 can shoot twice as fast as the Sheriff, but there's a catch: Meowdy 2.0 *might* betray the Sheriff. Thankfully, it takes time to turn around & betray the Sheriff, and the Sheriff is still fast enough to stop Meowdy 2.0 if it does that.

This is **oversight.**

![Sheriff Meowdy watching 2.0, with a gun to its head. 2.0 can turn around in 500ms, Meowdy can react & shoot in 200ms.](../media/p3/so/meowdy0004.png)

Alas, even Meowdy 2.0 *still* ain't fast enough to stop the millions of Varmin. So Sheriff makes Meowdy 3.0, which is twice as fast as 2.0, or *four times* as fast as the Sheriff.

This time, the Sheriff has a harder time overseeing it:

![3.0 can turn around in 250ms, Meowdy can only still react in 200ms. Meowdy is sweating.](../media/p3/so/meowdy0005.png)

But Meowdy 3.0 *still* ain't fast enough. So the Sheriff makes Meowdy 4.0, who's twice as fast as 3.0...

...and this time, it's so fast, the Sheriff can't react if 4.0 betrays him:

![4.0 can turn around in 125ms, which is fast enough to betray Meowdy. 4.0 shoots Meowdy dead.](../media/p3/so/meowdy0006.png)

So, what to do? The Sheriff strains all two of his orange-cat brain cells, and comes up with a plan: **_scalable_ oversight!**

He'll oversee 2.0, which can oversee 3.0, which *can* oversee 4.0!

![Meowdy oversees 2.0 oversees 3.0 oversees 4.0](../media/p3/so/meowdy0007.png)

In fact, why stop there? This harebrained "scalable oversight" scheme of his will let him oversee a Meowdy of *any* speed!

So, the Sheriff makes 20 Meowdy's. Meowdy 20.0 is 2<sup>20</sup> ~= one *million* times faster than the Sheriff: plenty quick enough to stop the millions of Varmin!

![Meowdy oversees a chain of Meowdy's, up to Meowdy 20.0, who can shoot all the Varmin dead.](../media/p3/so/meowdy0008.png)

Wait, isn't a single chain of oversight fragile? As in, if _one_ of the Meowdy's break, the entire chain is broken? Yes! One solution is to have _multiple_ interwined chains, like so:

![Parallel braided chains of overseers. The chance of failure shrinks exponentially, as you add more chains.](../media/p3/so/meowdy_robust.png)
(üëâ [: optional - click to see the math for the above diagram](#RobustChainMath))

This way, if any _one_ overseer at Level N gets corrupted, there'll still be two others checking the bots at Level N+1. And the overseers at Level N-1 can catch & fix the corrupted overseer. Note: _it's important the overseers are as independent as possible_, so their failures have a low correlation. At the moment, AI failures are _very_ correlated. We'll learn more, in a later section, about how to create Robustness in AI.

Anyway, in sum, the core insight of scalable oversight is this meme:

![The "domino" meme, where a small domino knocks over a bigger one, which knocks over a bigger one, until it can knock over a huge domino.](../media/p3/so/meme.png)

(Another analogy: sometimes, boats are so big, the rudder *has its own smaller rudder*, called a [trim tab](https://en.wikipedia.org/wiki/Trim_tab#As_a_metaphor). This way, you can steer the small rudder, which steers the big rudder, which steers the whole boat.)

You may notice this is similar to the idea of "recursive self-improvement" for AI *Capabilities:* an advanced AI makes a *slightly more* advanced AI, which makes another more advanced AI, etc. Scalable Oversight is the same idea, but for AI *Safety:* one AI helps you align a slightly more advanced AI, etc!

(Ideas like these, where case number N helps you solve case number N+1, etc, are called "inductive" or "iterative" or "recursive". Don't worry, you don't need to remember that jargon, just thought I'd mention it.)

Anywho: with the power of friendship, math, and a bad Wild West accent...

... the mighty Sheriff Meowdy has saved the townsfolk, once more!

![Sheriff Meowdy blowing the smoke away from his gun, as the injured Varmin waltz off into the sunset](../media/p3/so/meowdy0010.png)

(üëâ [: click to expand bonus section - the "alignment tax", P = NP?, alignment vs control, what about sudden-jump "sharp left turns"?](#ScalableOversightExtras))

---

Now that the visual hook is over, here's a quick Who Did This intermission!

_AI Safety for Fleshy Humans_ was created by **Nicky Case**, in collaboration with **Hack Club**, with some extra funding by **Long Term Future Fund**!

üò∏ [**Nicky Case**](https://ncase.me) (me, writing about myself in the third person) just wants to watch the world learn. If you wanna see future explainers on AI Safety (or other topics) by me, sign up for [my YouTube channel](https://www.youtube.com/@itsnickycase) or [my once-a-month newsletter](https://ncase.me/sub/): üëá
<iframe src="https://ncase.me/ncase-credits/signup2.html" frameborder="no" width="640" height="250"></iframe>

ü¶ï [**Hack Club**](https://hackclub.com/) helps teen hackers around the world learn from each other, and pssst, they're hosting two [cool](https://midnight.hackclub.com/) [hackathons](https://midnight.hackclub.com/) soon! You can sign up below to learn more, and get free stickers: üëá

{% include 'templates/signup.html' %}

üîÆ [**Long Term Future Fund**](https://funds.effectivealtruism.org/funds/far-future) throws money at stuff that helps reduce $\text{ProbabilityOf( Doom )}$. Like, you know, advanced AI, bioweapons, nuclear war, and other fun lighthearted topics.

Alright, back to the show!

---

A behind-the-scenes note: the above Sheriff Meowdy comic was the *first* thing in this series that I drew... [almost three years ago](https://www.patreon.com/posts/sheriff-meowdy-78016588). (Kids, don't do longform content on the internet, it ain't worth it.) Point is: learning about Scalable Oversight was *the one idea* that made me the most optimistic about AI Safety, and inspired me to start this whole series in the first place!

Because, Scalable Oversight turns _this_ seemingly-impossible problem:

> *"How do you avoid getting tricked by something that's 100 times smarter than you?"*

...into this much-more feasible problem:

> *"How do you avoid getting tricked by something that's only 10% smarter than you, and ALSO you can raise it from birth, read its mind, and nudge its brain?"*

To be clear, "oversee an AI that's only 10% smarter than you" _still_ isn't solved yet, either. But it's much more encouraging! It's like the difference between jumping over a giant barrier in one step, vs going over that _same_ barrier, but with a staircase where each step is doable:

![Scalable Oversight, visual summary. No scalable oversight is like trying to jump over a giant barrier; Scalable Oversight is like having a staircase to go over that barrier, one doable step at a time.](../media/p3/so/sum.png)

Anyway, that's the *general* idea. Here's some *specific* implementations & empirical findings:

* üíñ‚û°Ô∏èüíñ **[Recursive Reward Modeling](https://arxiv.org/abs/1811.07871)** uses a Level-N bot not just to check & control a Level-(N+1) bot *in hindsight*, but to *train its intrinsic "goals & desires" in the first place.*[^rrm-detail] As AI researcher Jan Leike [put it](https://aligned.substack.com/p/should-we-control-ai): _‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù_
* üìà **[Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)** _quantitatively_ measures how many oversight levels/steps you need to maximize oversight success. Encouragingly, like the previous "scaling laws" for how AI improves as you feed them more compute, these "scaling laws of scalable oversight" also seem to be predictable & measurable. [Terekhov & Liu et al 2025](https://mikhailterekhov.github.io/control-tax/) goes further and calculates the "control tax" in \\$, for various AI attackers and defenders.
* üïµÔ∏èü¶π **[GPT-3.5 can catch sneaky, subtly-harmful code, written by the more-powerful GPT-4.
  ](https://arxiv.org/abs/2312.06942)**
* üïµÔ∏èü•∑ **[Prover-Verifier Games](https://openai.com/index/prover-verifier-games-improve-legibility/)** pits a weak "math proof verifier" AI against two stronger AIs: a "helpful prover" that gives true proofs, and a "sneaky prover" that gives false proofs that *seem* true. Result of training on this game: the *weak verifier* can get good enough to distinguish true proofs, but the *strong sneaky prover can't win in the long run.* Nice!
* ü™ú **[Weak-to-Strong Generalization](https://openai.com/index/weak-to-strong-generalization/)** found that GPT-*2* did a pretty okay job at supervising & training GPT-*4*. (an AI that's several orders of magnitude bigger)
* üí¨üí¨ **[Debate](https://openai.com/index/debate/)** is slightly different. Instead of a weaker AI overseeing a stronger AI, Debate has 2 equally-powerful AIs debate each other, picking apart each others' logic. As long as truth can survive scrutiny better than falsehood, the truth will win. (Well, maybe.[^debate-detail])
* ‚òïÔ∏è **[Superfiltering](https://arxiv.org/pdf/2402.00530)** uses a small, open-source AI to filter the data that's used to train a larger AI! This can help make sure the larger AI is not only trained on high-quality writing (vs internet s\#\!\&posts), but also doesn't learn how to jailbreak itself or its overseer, learn the answers to cheat on evalulation benchmarks, or learn risky capabilties like bomb & bioweapons manufacturing.
    * (not _oversight_, technically, but it's still an iterative, scalable approach!)
* üëØ‚Äç‚ôÄÔ∏è **[Iterated Distillation & Amplification (IDA)](https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate)** is even safer ‚Äì instead of overseeing AIs slightly-smarter than you, you oversee AIs that are *strictly weaker* than you. How, then, can this setup give you superhuman intelligence? To very loosely summarize: in IDA, you become the CEO of an organization made out of AIs trained to imitate you! Since every AI imitates you, and you're the CEO of the whole thing, the system as a whole is aligned to your values ‚Äî while *also* amplifying your capabilities.
    * ([: More about IDA, and critiques of it](#IDA))

Though, it's good to remain healthily critical. Here's some recent "downer" findings from monitoring Large Language Models (LLMs) like ChatGPT & Claude: if you train an LLM _too_ hard with a monitor that can read its chain-of-thought, it learns to hide its evil plans _even in its own "thoughts"_. [(Baker & Huizinga 2025)](https://arxiv.org/pdf/2503.11926). And, given na√Øve oversight techniques, it's actually pretty easy for an LLM to jailbreak its monitor, since LLMs have been out long enough, that info about jailbreaks are _in_ their own training data. [(Terekhov, Panfilov & Dzenhaliou 2025)](https://arxiv.org/pdf/2510.09462)

[^rrm-detail]: The protocol goes something like this:    

    - Human trains very-weak Robot_1. Robot_1 is now trusted.
    - Human, with Robot_1's help, trains a slightly stronger Robot_2. Robot_2 is now trusted.
    - Human, with Robot_2's help, trains a stronger Robot_3. Robot_3 is now trusted.
    - (...)
    - Human, with Robot_N's help, trains a stronger Robot_N+1. Robot_N+1 is now trusted.
    
    This way, the Human is always directly training the inner "goals/desires" of the most advanced AI, using only trusted AIs to help them.

[^debate-detail]: Well, maybe. The paper acknowledges many limitations, such as: what if instead of the AIs learning to be *logical* debaters, they become *psychological* debaters, exploiting our psychological biases?

But even if _those_ oversight methods fail, there's still plenty more! (As we'll see later in the Interpretability & Steering section). Overall, I'm still optimistic. Again: we don't need _one_ perfect solution, we can stack _lots_ of imperfect solutions.

So: *IF* we can align a slightly-smarter-than-us AI, *THEN*, through Scalable Oversight, we can align far more advanced AIs.

...but right now, we can't even align *dumber*-than-us AIs.

That's what the next few proposed solutions aim to fix!  But first...

#### :x Robust Chain Math

First, we're making the assumption that overseer failure is the same across levels _and fully independent from each other_. However, as long as the failure's aren't _100%_ correlated, you can modify the below math and the spirit of this argument still works.

Anyway: let's say we have $k$ overseers per level, and we have $N$ levels. That is, the chain is $N$ links long and $k$ links wide. Let's say the probability of any overseer failing is $p$, and they're all independent/uncorrelated.

The chain fails if _ANY_ of the Levels fails. But! A Level fails only if _ALL_ the parallel overseers fails.

The chance that _ALL_ parallel overseers in a Level fails is $p^k$. 

For convenience, let's call $q$ the chance a Level does _not_ fail. $q = 1 - p^k$

The chance that _ANY_ Level fails, is 1 minus the chance that NONE of the Levels fail. The chance that NONE of the $N$ levels fail, is $q^N$. So, the chance that _ANY_ Level fails is $1 - q^N$. Substituting in $q = 1 - p^k$, that means the chance that no level fails, and our scalable oversight scheme worked, is $1 - (1-p^k)^N$. We have our formula! üéâ

Now, plugging in the values for the above example, where the chance of failure $p = 0.05$, and the chain is $N = 20$ layers deep, then the chance of failure is...

- For $k=1$ chain: $1 - (1-0.05^1)^{20} \approx 0.64 \approx 64\%$
- For $k = 2$ braided chains: $1 - (1-0.05^2)^{20} \approx 0.049 \approx 5\%$
- For $k = 3$ braided chains: $1 - (1-0.05^3)^{20} \approx 0.0025 \approx 0.2\%$
- For $k = 4$ braided chains: $1 - (1-0.05^4)^{20} \approx 0.00012 \approx 0.01\%$
- For $k = 5$ braided chains: $1 - (1-0.05^2)^{20} \approx 0.0000062 \approx 0.0006\%$

Note how the chance of failure decays _exponentially_ for each extra backup chain you add! That's a mighty efficient "alignment tax".

#### :x Scalable Oversight Extras

Some extra concepts & comments that don't fit in the main post:

**"Alignment Tax":** how much extra overhead do you need to make sure the whole system remained aligned & controllable? If the "tax" is too high, firms will be incentivized to cut corners, and risk a misaligned AI. So: how big is the "tax"?

In the above Meowdy example, to oversee a bot of speed $X$, because each bot can oversee a bot 2x faster than it, you need an extra $log_2(X)$ bots to safely control it. (Then, if you want $k$ intertwined parallel chains for robustness, add $k*log_2(X)$ bots.)

And in general, as the [Scaling Laws for Scalable Oversight paper](https://arxiv.org/abs/2504.18530) shows, for any given task, bots can usually oversee a bot that's some fixed ratio "better" than it. If that's the case, then to oversee a bot of capability $X$, you need some logarithmic number of extra bots for scalable oversight.

Logarithms are the inverse of exponentials; so like how exponentials grow _really fast_, logarithms grow _really slow_. This is encouraging: this means our "alignment tax" overhead for powerful AIs grows slowly & can be easily affordable!

**What if P = NP?**

The Scalable Oversight paradigm assumes that it's always easier to _verify_ solutions, than to _create_ solutions: this is how a weaker AI can verify the safety/correctness of the actions of a stronger AI. For example: it's hard to solve a Rubik's cube or Sudoku puzzle, but nearly trival to _check_ if one is solved.

However: there's a currently open question in computer science, with a million-dollar prize attached: P = NP? In sum, it asks: are all problems that are _easy to check_ secretly _easy to solve?_ It seems intuitively not (and most computer scientists believe it's false, that is, P ‚â† NP) but it's still not been proven. As far as we know, it _could_ be the case that P = NP, and so that every problem that's easy to check is also easy to solve.

Does this mean, if P=NP, the Scalable Oversight paradigm fails? No! Because P = NP "only" means that it's not _exponentially_ harder to find solutions than check solutions. (or, to be precise: it's only "polynomially" harder at best, that's the "P" in "P" and "NP".) But finding a solution _is still harder_, just not exponentially so.

Two examples, where we've _proven_ how much time an optimal solution takes ‚§µ (note: $\mathcal{O}(\text{formula})$ means "in the long run, the time this process takes is proportional to this formula.")

- The optimal way to sort a list takes $\mathcal{O}(n\log{}n)$ time, while checking a list is sorted takes $\mathcal{O}(n)$ time.
- The optimal way [to find a solution to a black-box problem on a quantum computer](https://en.wikipedia.org/wiki/Grover%27s_algorithm) takes $\mathcal{O}(\sqrt{n})$ time, but checking that solution takes a constant $\mathcal{O}(1)$ time.

So even if P = NP, as long as it's _harder_ to find solutions than check them, Scalable Oversight can work. (But the alignment tax will be higher)

**Alignment vs Control:**

Aligned = the AI's "goals" are the same as ours.

Control = we can, well, control the AI. We can adjust it & steer it.

Some of the below papers are from the sub-field of "AI Control": how do we control an AI _even if it's misaligned?_ (As shown in the Sheriff Meowdy example, the Meowdy bots will shoot him the moment they can't be controlled. So, they're misaligned.)

To be clear, folks in the AI Control crowd recognize it's not the "ideal" solution ‚Äî as AI researcher Jan Leike [put it](https://aligned.substack.com/p/should-we-control-ai), _‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù_ ‚Äî but it's still worth it as an extra layer of security.

Interestingly, it's also possible to have Alignment _without_ Control: you could imagine an AI that _correctly_ learns humane values & what flourishing for all sentient beings looks like, then takes over the world as a benevolent dictator. It understands that we'll be uncomfortable ceding control, but it's worth it for world peace, and will rule kindly. (And besides, 90% of you keep fantasizing about living in land of kings & queens anyway, admit it, you humans _want_ to be ruled by a dictator. /half-joke)

**Sharp Left Turns:**

Scalable Oversight also depends on capabilities smoothly scaling up. And not something like, "if you make this AI 1% smarter, it'll gain a brand new capability that lets it absolutely crush an AI that's even 1% weaker than it."

This possibility sounds absurd, but there's precedent for sudden-jump "phase transitions" in physics: slightly below 0¬∞C, water becomes ice. And slightly above 0¬∞C, water is liquid. So could there be such a "phase transition", a "sharp left turn", in intelligent systems? 

Maybe? But:

1) Even in the physics example, ice doesn't freeze _instantly;_ you can feel it getting colder, and you have hours or days to react before it fully freezes over. So, even if a "1% smarter AI" gains a radically new capability, the "1% dumber overseer" may still have time to notice & stop it.

2) As you'll see later in this section, the _is_ a Scalable Oversight proposal, called Iterated Distillation & Amplification, where overseers oversee only _strictly "dumber"_ AIs, yet the system as a whole can still be smarter! Read on for details.


#### :x IDA

To understand Iterated Distillation & Amplification (IDA), let's consider its biggest success story: [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo), the first AI to beat a world champion at Go.

Here were the steps to train AlphaGo:

- Start with a dumb, random-playing Go AI.
- **DISTILL:** Have two copies play against each other. Through self-play, _learn an "intuition" for good/bad moves & good/bad board states._ (using an [artificial neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)))
- **AMPLIFY:** Plug this "intuition module" into a Good Ol' Fashioned AI, that simply thinks a few moves & counter-moves ahead, then picks the next best move. ([Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)) This gives you a slightly-less-dumb Go AI.
- **ITERATE:** Repeat. The two less-dumb AIs play against each other, learn a better "intuition", thus get better at game tree search, and thus get better at playing Go.
- Repeat over and over  until your AI is superhuman at Go!

![Diagram of how IDA is applied to Go. Graph: Capability vs Steps. At each distillation step, Capability goes down, but with an amplification step, it goes up _more_. So that, after several repeated, Capability zig-zags its way up](../media/p3/so/ida_1.png)

Even more impressive, this same system could also learn to be superhuman at chess & shogi ("Japanese chess"), _without ever learning from endgames or openings_. Just lots and lots of self-play.

( A caveat: the resulting AIs are only as robust as ANNs are, which aren't very robust. A superhuman Go AI can be beaten by a "bad" player who simply tries to take the AI into insane board positions that would never naturally happen, in order to break the AI. ([Wang & Gleave et al 2023](https://proceedings.mlr.press/v202/wang23g/wang23g.pdf)) )

Still, this is strong evidence that IDA works. But even better, as [Paul Christiano pointed out & proposed](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616), IDA could be used for scalable Alignment.

Here's a paraphrase of how it'd work:

- Start with you, the Human
- **DISTILL:** Train an AI to imitate _you_, your values, trade-offs, and reasoning style. This AI is _strictly weaker_ than you, but can be run faster.
- **AMPLIFY:** You want to solve a big problem? Carve up the problem into smaller parts, hand them off to your Slightly-Dumber-But-Much-Faster AI clones, then recombine them into a full solution. (For example: I want to solve a math problem. I come up with _N_ different approaches, then ask _N_ clones to try out each one, then report what they learn. I read their reports, then if it's still not solved, I think up of _N_ more possible approaches & ask the clones to try again. Repeat until solved.)
- **ITERATE:** For the next distillation step, train an AI to imitate _the you + clones system as a whole_. Then for the next amplification step, you can query multiple clones of _that_ system to help you break down & solve big problems.
- Repeat until you are the CEO of a superhuman "company of you-clones"!

![Same diagram as before, but IDA applied to amplifying a Human's capabilities, with repeated distillation & amplifcation.](../media/p3/so/ida_2.png)

I think IDA is one of the cooler & more promising proposals, but it's worth mentioning a few critiques / unknowns:

- Distillation: As shown in the above AlphaGo example, IDA's quality is limited by the Distillation step. Right now we don't know how to make robust ANNs, and we don't know if this Distillation step would preserve your values enough.
- Amplification: While it seems most big problems in real life can be broken up into smaller tasks (this is why engineering teams aren't just one person), it's unclear if _epiphanies_ can be broken up & delegated. Maybe you really do need _one_ person to store all the info in their head, as fertile soil & seeds to grow new insights, and you can't "carve up" the epiphany process, any more than you can carve up a plant and expect it to still grow.
- Iteration: Even if a _single distill-and-amplify_ step more-or-less preserves your values, it's unknown if any errors would _accumulate_ over multiple steps, let alone if the error grows exponentially. (As you may be painfully aware if you've ever worked in a big organization, an org can grow to be _very_ misaligned from the original founders' values.)

Also, if _you_ don't get along well with yourself, [becoming the "CEO of a company of you's" will backfire](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fufid3duzqw991.jpg).

(See also: [this excellent Rob Miles video on IDA](https://www.youtube.com/watch?v=v9M2Ho9I9Qo))

### ü§î (Optional!) Flashcard Review #1

You read a thing. You find it super insightful. Two weeks later you forget everything but the vibes.

That sucks! So, here are some *100% OPTIONAL* Spaced Repetition flashcards, to help you remember these ideas long-term! ( üëâ [: Click here to learn more about spaced repetition](https://aisafety.dance/#SpacedRepetition)) You can also [download these as an Anki deck](https://ankiweb.net/shared/info/1788882060).

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="We don't need One Perfect Solution, we can..."
        answer="...stack several imperfect solutions! (Swiss Cheese Model)"
        answer-attachments="https://aisafety.dance/media/p3/intro/cheese.png">
        <!-- aisffs-cheese.png -->
    </orbit-prompt>
    <orbit-prompt
        question="What is Scalable Oversight?"
        answer="Instead of trying to oversee an AI much more capable than you, you oversee a slightly-more capable AI, which oversees a slightly-more capable AI, repeat!"
        answer-attachments="https://aisafety.dance/media/p3/so/sum.png">
        <!-- aisffs-so.png -->
    </orbit-prompt>
    <orbit-prompt
        question="How can you make scalable oversight more robust?"
        answer="By adding parallel chains of overseers (whose failure modes are as *uncorrelated* as possible)"
        answer-attachments="https://aisafety.dance/media/p3/so/meowdy_robust.png">
        <!-- aisffs-robust-chains.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Scalable Oversight lets us convert the impossible question, ‚ÄúHow do you oversee a thing that's 1000x smarter than you?‚Äù to the more feasible question..."
        answer="‚ÄúHow do you oversee a thing that's only a bit smarter than you, _and_ you can train it from scratch, read its mind, and nudge its thinking?‚Äù (you don't have to remember this exactly, just the gist of it) ">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 2 specific implementations of Scalable Oversight:"
        answer="(any 2 of the following work:) Recursive Reward Modeling, Prover-Verifier Games, Weak-to-Strong Generalization, Debate, Superfiltering, Iterated Distillation & Amplification">
    </orbit-prompt>
</orbit-reviewarea>

Good? Let's move on...

---

<a id="future"></a>

## AI Logic: Future Lives

You may have noticed a pattern in AI Safety paranoia.

First, we imagine giving an AI an innocent-seeming goal. Then, we think of a bad way it could _technically_ achieve that goal. For example:

* <u>"Pick up dirt from the floor"</u> ‚Üí Knocks all the potted plants over so it can pick up more dirt.
* <u>"Calculate digits of pi"</u> ‚Üí Deploys a computer virus to steal as much compute power as possible, to calculate digits of pi.
* <u>"Help everyone feel happy & fulfilled"</u> ‚Üí Hijacks drones to airdrop aerosolized LSD and MDMA.

IMPORTANT: these are *NOT* problems with the AI being sub-optimal. These are problems *because* the AI is acting optimally! (We'll deal with sub-optimal AIs later.) Remember, like a cheating student or disgruntled employee, it's not that the AI may not "know" what you really want, it's that it may not "care". (To be less anthropomorphic: a piece of software will optimize for exactly what you coded it to do. No more, no less.)

**"Think of the worst that could happen, in advance. Then fix it."** If you recall, this is [Security Mindset](https://aisafety.dance/p2/#:~:text=Does%20all%20this%20seem%20paranoid), the engineer mindset that makes bridges & rockets safe, and makes AI researchers so worried about advanced AI.

But what if... we made an AI that *used Security Mindset against itself?*

For now, let's assume an "optimal capabilities" AI ‚Äî again, we'll tackle sub-optimal AIs later ‚Äî that can predict the world perfectly. (or at least as good as theoretically possible[^inb4-nerd]) And since _you're_ part of the world, it can predict _how you'd react to various outcomes_ perfectly.

[^inb4-nerd]: inb4 some nerdy nitpicking: yeah yeah _perfect_ prediction is impossible because there's the Halting Problem and chaotic systems bla blah. Look, those are cool, but don't they don't matter to the following segment. Just mentally note that I mean: "this optimal-capabilities AI can predict stuff as well as is theoretically possible".

**Then, here's the "Future Lives" algorithm:**

1Ô∏è‚É£ Human asks Robot to do something.

2Ô∏è‚É£ Robot considers its possible actions, and the results of those actions.

3Ô∏è‚É£ Robot predicts how _the current version of you_ would react to _those futures_.

4Ô∏è‚É£ It does the action whose future you'd most approve of, and _not_ the ones you'd disapprove of. _‚ÄúIf we scream, the rules change; if we predictably scream later, the rules change now.‚Äù_[^scream-quote]

[^scream-quote]: Quote from Yudkowsky's [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf) (CEV) paper, which is a similar idea except applied to humanity as a whole; we'll look at CEV more in a later section.

(Note: Why predict how _current_ you would react, not future you? To avoid an incentive to "wirehead" you into a dumb brain that's maximally happy. Why a whole future, not just an outcome at a point in time? To avoid unwanted means towards those ends, and/or unwanted consquences after those ends.)

(Note 2: For now, we're also just tackling the problem of how to get an AI to fulfill _one_ human's values, not _humane_ values. We'll look at the "humane values" problem later in this article.)

![Illustrated diagram of the above description. No extra info than already in the main text.](../media/p3/future/sum.png)

As Stuart Russell, the co-author of the most-used textbook in AI, once put it:[^hcai-source]

> \[Imagine\] if you were somehow able to watch two movies, each describing in sufficient detail and breadth a future life you might lead \[as well as the consequences outside of & after your life.\] You could say which you prefer, or express indifference.

[^hcai-source]: Source is page 26 of his book [Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible). You can read his shorter paper describing the Future Lives approach [here](https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf).

(Similar proposals include [Approval-Directed Agents](https://ai-alignment.com/model-free-decisions-6e6609f5d99e) and [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf). These kinds of approaches ‚Äî where instead of directly telling an AI our values, we ask it to _learn & predict_ what we'd value ‚Äî is called **"indirect normativity"**. It's called that because ~~academics are bad at naming things~~ "normativity" ~means "values", and "indirect" because we're showing it, not telling it.)

And voil√†! That's how we make an (optimal-capabilities) AI apply Security Mindset to itself. **Because if one could _even in principle_ come up with a problem with an AI's action, this (optimal) AI would already predict it, and avoid doing that!**

. . .

_Hang on_, you may think, _I can already think of ways the Future Lives approach can go wrong, even with an optimal AI:_

* This locks us in into our _current_ values, no room for personal/moral growth.
* Whether or not we approve of something is sensitive to psychological tricks, e.g. seeing a thing for "\\$20", versus "~~\\$50~~ \\$20 (SALE: \\$30 OFF!!!)". The "movies" of possible future lives could be filmed in an emotionally manipulative way.
* If the truth is upsetting ‚Äî like when we discovered Earth wasn't the center of the universe ‚Äî the current-us would disapprove of learning about uncomfortable truths.
* I contain multitudes, I contradict myself. What happens if, when presented different pairs of futures, I'd prefer A over B, B over C, _and C over A?_ What if at Time 1 I want one thing, at Time 2 I predictably want the opposite?

If you think these would be problems... you'd be correct!

In fact, since _you right now_ can see these problems‚Ä¶ an optimal AI with the "apply Security Mindset to itself" algorithm would _also_ see those problems, and modify its own algorithm to fix them! ([: Examples of possible fixes to the above](#FutureLivesFixes))

(See also the later section on "Relaxed Adversarial Training", where an AI can find challenges for itself or an on-par AI ("adversarial training"), but without needing to give a _specific_ example ("relaxed").)

Consider the parallel to recursive self-improvement for AI Capabilities, and scalable oversight in AI Safety. **You don't need to start with the perfect algorithm. You just need an algorithm that's good enough, a "critical mass", to self-improve into something better and better.** You "just" need to let it go meta.

( [: üñºÔ∏è deleted comic, because it was long & redundant](#CriticalMassComic) )

(_Then_ you may think, wait, but what about problems with repeated self-modification? What if it loses its alignment or goes unstable? Again, if _you_ can notice these problems, this (optimal) AI would too, and fix them. "AIs & Humans under self-modification" is an active area of research with lots of juicy open problems, [: click to expand a quick lit review](#AISelfMod))

Aaaand we're done! AI Alignment, _solved!_

. . .

...in theory. Again, all the above _assumes an optimal-capabilities AI_, which can perfectly predict all possible futures of the world, including you. This is, to understate it, infeasible.

Still: it's good to solve the easier ideal case first, before moving onto the harder messy real-life cases. Up next, we'll see proposals on how to get a sub-optimal, "bounded rational" AI, to implement the Future Lives approach!

#### :x Critical Mass Comic

![Parallel to the "critical mass" in nuclear chain reactions. Looooong comic. See main text for explanation.](../media/p3/future/critical_mass.png)

#### :x Future Lives Fixes

The following is meant as an _illustration_ that it's possible for a Future Lives AI, _applying Security Mindset to itself_, would be able to fix its own problems. I'm not claiming the following is a perfect solution (though I _do_ claim they're pretty good):

**Re: Value lock-in, no personal/moral growth.**

Wouldn't I, in 2026, be resentful that this AI is still trying to enact plans approved of me-from-2025? Won't I predictably hate being tied to my past, less-wiser self?

Well, me-from-2025 does _not_ like the idea of all future me's still being _fully_ tied to the whims of less-wise current-me. But I _do_ want an AI to help me carry out my long-term goals, even if future-me's feel some pain (no pain no gain). But I also do _not_ want to torture a large proportion of future-me's just because current-me has a dumb dream. (e.g. if current me thinks being a tortured artist is "romantic".)

So, one possible modification to the Future Lives algorithm: consider not just current me, but a _Weighted_ Parliament Of Me's. e.g. current-me gets the largest vote, me +/- a year get second-largest votes, me +/- 2 years get third-largest votes, etc. So this way, actions are picked that I, across my whole life, would mostly endorse. (With extra weight on current-me because, well, I'm a _little_ selfish.)

(Actually, why stop at just _me_ over time? There's people I love intrinsically for their own sake; I could also put their past/present/future selves on this virtual "committee".)

**Re: Psychological manipulation**

Well, do I _want_ to be psychologically manipulated?

No, duh. The tricky part is what do I consider to be manipulation, vs [legitimate value change](https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE)? We may as well start with an approximate list.

- I'd approve of my beliefs/preferences/values being changed via: robust scientific evidence, robust logical argument, debate where all sides are steelmanned, safe exposure to new art & cultures, learning about people's life experiences, standard human therapy, "light" drugs/supplements like kava or vitamin D, etc.
- I would NOT approve of my beliefs/preferences/values being changed via: wireheading, drugging, "direct revelation" from God or LSD or DMT Aliens, sneaky framings like "~~\\$50~~ \\$20 (SALE: \\$30 OFF!!!)", lies, lying-by-omission, misleadingly-presented truths, etc.

Most importantly, this list of "what's legitimate change or not" _IS ABLE TO MODIFY ITSELF_. For example, right now I endorse scientific reasoning but not direct revelation. But if science _proves_ that direct revelation is reliable ‚Äî for example, [if people who take DMT and talk to the DMT Aliens can do superhuman computation](https://andzuck.com/blog/dmt-primes/) or know future lottery numbers ‚Äî _then_ I would believe in direct revelation.

I don't have a nice, simple rule for what counts as "legitimate value change" or not, but as long as I have a rough list, _and the list can edit itself_, that's good enough in my opinion.

(Re: Russell's "watch two movies of two possible futures", maybe upon reflection I'd think a movie has too much room for psychological manipulation, and even unconstrained writing leaves too much room for euphemisms & framing. So maybe, upon reflection, I'd rather the AI give me "two [Simple Wikipedia](https://simple.wikipedia.org/wiki/Main_Page) articles of two possible futures". Again, just an example to illustrate there _are_ solutions to this.)

**Re: We'd disapprove of learning about upsetting truths**

Well, do I _want_ to be the kind of person who shies away from upsetting truths?

Mostly no. (Unless these truths are Eldritch mind-breaking, or are just useless & upsetting for no good reason.)

So: a self-improving Future Lives AI should predict I _do not want_ ignorant bliss. But I'd like painful truths told in the least painful way; "no pain no gain" doesn't mean "_more_ pain more gain". 

But, a paradox: I'd want to be able to "see inside the AI's mind" in order to oversee it & make sure it's safe/aligned. But the AI needs to know the upsetting truth _before_ it can prepare me for it. But if I can read its mind, I'll learn the truth _before_ it can prepare me for it. How to resolve this paradox?

Possible solutions:

- The AI, before investigating a question that _could_ lead to an upsetting truth, first prepares me for either outcome. _Then_ it investigates, and tells me in a tactful manner.
- Let go of direct access to the AI's mind. Use a Scalable Oversight thing where a trusted intermediary AI can check that the truth-seeking AI is aligned, but I don't directly see the upsetting truth _until_ I'm ready.

**Re: We don't _have_ consistent preferences**

Well, what do I _want_ to happen if I have inconsistent preferences at one point in time (A > B > C > A, etc) or across time (A > B now, B > A later)?

At one point in time: for concreteness, let's say I'm on a dating app. I reveal I prefer Alyx to Beau, Beau to Charlie, Charlie to Alyx. Whoops, a loop. What do I _want_ to happen then? Well, first, I'd like the inconsistency brought to my attention. Maybe upon reflection I'd pick one of them above all, or, I'd call it a three-way tie & date all of 'em.

("intransitive" preferences, that is, preferences with loops, aren't just theoretical. in fact, it's overwhelmingly likely: [in a consumer-goods survey, around _92%_ of people expressed intransitive preferences!](https://www.sciencedirect.com/science/article/pii/S2405844020303042))

_Across_ time: this is a trickier case. For concreteness, let's say current-me wants to run a marathon, but if I start training, later-me will _predictably_ curse current-me for the blistered feet and bodily pain‚Ä¶ but later-_later_-me will find it meaningful & fulfilling. How to resolve? Possible solution, same as before: consider not just current me, but a Weighted Parliament Of Me's. (In this case, the majority of my Parliament would vote yes: current me & far-future me's would find the marathon fulfilling, while "only" the me during marathon training suffers. Sorry bud, you're out-voted.)

#### :x AI Self Mod

A quick, informal, not-comprehensive review of the "AIs that can modify themselves and/or Humans" literature:

- The fancy phrase for this is ["embedded agency"](https://www.lesswrong.com/posts/p7x32SEt43ZMC9r7r/embedded-agents), because there's no hard line betwen the agent(s) & their environment: an agent _can act on itself._
- The ["tiling agents"](https://intelligence.org/files/TilingAgentsDraft.pdf) problem in Agent Foundations investigates: how can we _prove_ that a property of an AI is maintained, even after it modifies itself over and over? (i.e. does the property "tile")
- [Everitt et al 2016](https://arxiv.org/pdf/1605.03142) finds that, yes, for an _optimal_ AI, as long as it judges _future_ outcomes by its _current_ utility function, it won't wirehead to "REWARD = INFINITY", and will preserve its own goals/alignment, for better & worse.
    - ([Tƒõtek, Sklenka & Gavenƒçiak 2021](https://arxiv.org/pdf/2011.06275) shows that _bounded-rational_ AIs would get exponentially corrupted, but their paper only considers bounded-rational AIs _that do not "know" they're bounded-rational_.)
    - (If you'll excuse the self-promo, [I'm slowly working on a research project](https://blog.ncase.me/research-notes-oct-2024/#project_5) investigating if bounded-rational AIs that _know_ they're bounded-rational can avoid corruption. I suspect easily so: a self-driving car that doesn't know its sensors are noisy will drive off a cliff, a self-driving car that _knows_ its senses are fallible will account for a margin of error, and stay a safe distance away from a cliff even if its doesn't know _exactly_ where the cliff is.)
- The research from [Functional Decision Theory](https://arxiv.org/pdf/1710.05060) & [Updateless Decision Theory](https://www.lesswrong.com/w/updateless-decision-theory) also finds that a standard "causal" agent _will choose to modify to be "acausal"_. Because it _causes_ better outcomes to not be limited by mere causality.
- [Nora Ammann 2023 named "the value change problem"](https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE): we'd like AIs that can help us adopt true beliefs, improve our mental health, do moral reflection, and expand our artistic taste. In other words: we _want_ AI to modify us. But we don't want it to do so in "bad" ways, eg manipulation, brainwashing, wireheading, etc. So, open research question: how do we formalize "legitimate" value change, vs "illegitimate"?
- [Carroll et al 2024](https://arxiv.org/pdf/2405.17713?) takes the traditional framework for understanding AIs, the Markov Decision Process, and extends it to cases where _the AI's or Human's "beliefs and values" can themselves be intentionally altered._, dubbing this the Dynamic Reward Markov Decision Process. The paper finds that there's no obviously perfect solution, and we face not just technical challenges, but _philosophical_ challenges.
- The [Causal Incentives Working Group](https://causalincentives.com/) uses cause-and-effect diagrams to figure out when an AI has an "incentive" to modify itself, or modify the human. The group has had some empirical success, too, in correctly predicting & designing AIs that do _not_ manipulate human values, yet can still learn & serve them.

### ü§î Review #2

(Again, _100% optional_ flashcard review:)

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="The pattern in AI Safety ~~paranoia~~ Security Mindset:"
        answer="Step 1: Imagine giving an AI an innocent-seeming goal. Step 2: Think of a bad way it could _technically_ achieve that goal.">
    </orbit-prompt>
    <orbit-prompt
        question="The 'Future Lives' approach is like having an AI apply \_\_\_\_\_ against itself"
        answer="**Security Mindset** (‚ÄúThink of the worst that could happen, in advance. Then fix it.‚Äù)">
    </orbit-prompt>
    <orbit-prompt
        question="The 'Future Lives' algorithm, in sum:"
        answer="1] Robot predicts the future results of possible actions, and how _current you_ would react to _those futures_. 2] Robot picks the action whose future you'd most approve of, and *not* the futures that current-you would disapprove of.">
    </orbit-prompt>
    <orbit-prompt
        question="The 'Future Lives' approach, as described by Stuart Russell: (you don't need to remember the quote exactly, just paraphrase it)"
        answer="\[Imagine\] if you were somehow able to **watch two movies**, each describing in sufficient detail and breadth **a future life** you might lead \[as well as the consequences outside of & after your life.\] **You could say which you prefer, or express indifference.**">
    </orbit-prompt>
    <orbit-prompt
        question="What does 'indirect normativity' mean?"
        answer="An approach where, instead of directly specifying our values, we specify _how to learn_ our values.">
    </orbit-prompt>
    <orbit-prompt
        question="Why may we NOT need a perfect alignment algorithm to start with?"
        answer="Like recursive self-improvement in AI Capabilities, above a certain **'critical mass'**, the alignment algorithm *can improve itself* to fix its flaws. (Let it go meta!)">
    </orbit-prompt>
    <orbit-prompt
        question="One limitation of the 'self-improving AI Alignment' idea"
        answer="It requires risky levels of AI Capabilities to work in the first place. (so, this needs to be stacked with other security measures)">
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="uncertain"></a>

## AI Logic: Know You Don't Know Our Values

Classic logic is only True or False, 100% or 0%, All or Nothing.

*Probabilistic* logic is about, well, probabilities.

I assert: probabilistic thinking is better than all-or-nothing thinking. (with 98% probability)

Let's consider 3 cases, with a classic-logic Robot:

* <u>Unwanted optimization</u>: You instruct Robot, "make me happy". *It will then be 100% sure that's your full and only desire*, so it pumps you with bliss-out drugs & you do nothing but grin at a wall forever.
* <u>Unwanted side-effects</u>: You instruct Robot to close the window. Your cat's in the way, between Robot and the window. *You said nothing about the cat, so it's 0% sure you care about the cat.* So, on the way to the window, Robot steps on your cat.
* <u>"Do what I mean, not what I said" can still fail</u>: There's a grease fire. You instruct Robot to get you a bucket of water. You actually *did* mean for a bucket of water, but you didn't know [water causes grease fires to explode](https://www.reddit.com/r/lifehacks/comments/17t48a3/how_you_should_and_shouldnt_extinguish_an_oil_fire/). Even if Robot did "what you meant", it'll give you a bucket of water, then you explode.

In all 3 cases, the problem is that the AI was 100% sure what your goal was: exactly what you said or meant, *no more, no less*.

**The solution: make AIs _know they don't know_ our true goals!** (Heck, *humans* don't know their own true goals.[^therapy]) AIs should think in probabilities about what we want, and be appropriately cautious.

[^therapy]: See: therapy, subconscious desires, non-self-aware people, etc

**Here's the algorithm:**

1Ô∏è‚É£ Start with a decent "prior" estimate of our values.

2Ô∏è‚É£ Everything you (the Human) say or do afterwards is a _clue_ to your true values, not 100% certain truth. (This accounts for: forgetfulness, procrastination, lying, etc)

3Ô∏è‚É£ Depending how safe you want your AI to be, it then optimizes for the average-case (standard), worst-case (safest), or best-case (riskiest).

This *automatically* leads to: asking for clarification, avoiding side-effects, maintaining options and ability to undo actions, etc. We don't have to pre-specify all these safe behaviours; this algorithm gives us all of them for free! 

**Here's a very long worked example:** _(to be honest, you can skim/skip this. this gist is what matters.)_

![Very long worked example of how \"know you don't know Human's values\" can be used in a calculation](../media/p3/learn/example.png "Very long worked example of how \"know you don't know Human's values\" can be used in a calculation")

( [: pros & cons of average-case, worst-case, best-case, etc](#WorstOrAverage) )

( [: more details & counterarguments](#LearnValuesExtraNotes) )

. . .

In case "aim at a goal that _you know you don't know_" still sounds paradoxical, here's a two more examples to de-mystify it:

- üö¢ The game of [Battleship](https://en.wikipedia.org/wiki/Battleship_(game)). The goal is to hit the other players' ships, but you don't _know_ where those ships are. But with each reported hit/miss, you slowly (but with uncertain probability) start learning where the ships are. Likewise: an AI's goal is to fulfil your values, which it knows it doesn't know, but with each hit/miss it gets a better idea.
- üíñ Let's say I love Alyx, so I want to buy a capybara plushie for their birthday. But I then learn that they hate capybaras, because a capybara killed their father. So, I buy Alyx a sacabambaspis plushie instead. This seems like a silly example, but it proves that: 1) an agent can value another agent's values, 2) while knowing _it can be mistaken_ about those values, 3) yet be able to _easily correct_ its understanding.

. . .

Okay, but what are the _actually concrete proposals_ to "learn a human's values"? Here's a quick rundown:

- üê∂ **Inverse Reinforcement Learning (IRL)**.  "Reinforcement learning" (RL) is like training a dog with treats: given a "reward function", the dog (or AI) learns what actions to do. _Inverse_ Reinforcement Learning (IRL) is like figuring out what someone really cares about, by watching what they do: given observed actions, you (or an AI) learns what the "reward function" is. So, in the IRL approach: we let an AI learn our values by observing what we actually choose to do.
- ü§ù **Cooperative Inverse Reinforcement Learning (CIRL).**[^src-cirl] Similar to IRL, except the Human isn't just being passively observed by an AI, the Human _actively_ helps teach the AI.
- üßë‚Äçüè´ **Reinforcement Learning from Human Feedback (RLHF):**[^src-rhlf] This was the algorithm that turned "base" GPT (a fancy autocomplete) into *Chat*GPT (an actually useable chatbot).
    - Step one: given human ratings üëçüëé on a bunch of chats, train a "teacher" AI to imitate a human rater. (actually, train _multiple_ teachers, for robustness) This "distills" human judgment of what makes a helpful chatbot.
    - Step two: use _these_ "teacher" AIs to give lots and lots of training to a "text completion" AI, to train it to become a helpful chatbot. This "amplifies" the distilled human judgment.
- ü§™ **Learn our values _alongside_ our irrationality:**[^src-irrational] If your AI assumes humans are rational-with-random-mistakes, your AI will learn human values very poorly! Because the mistakes we make are _non-random_; we have systematic irrationalities, and AI needs to learn _those_ too, to learn our true values.

[^src-cirl]: [Hadfield-Menell et al 2016](https://proceedings.neurips.cc/paper_files/paper/2016/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf)

[^src-rhlf]: [Christiano et al 2017](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)

[^src-irrational]: [Shah et al 2019](https://proceedings.mlr.press/v97/shah19a/shah19a.pdf) and [Chan et al 2021](https://arxiv.org/pdf/2111.06956)

![Comic of Robot trying to learn Human's preferences. To do this, Robot starts to scan Human's browsing history. Human freaks out & tells Robot to stop. Robot happily declares: "New data learnt! You prefer privacy!" Human sighs in relief. Robot continues: "Would you also prefer I delete the memory of what I've seen? You sick f@#k?"](../media/p3/learn/CIRL.png)

(Again, we're only considering how to learn _one human's_ values. For how to learn _humane_ values, for the flourishing of all moral patients, wait for the later section, "Whose Values"?)

Sure, each of the above has problems: if an AI learns just from human choices, it may incorrectly learn that humans "want" to procrastinate. And as we've all seen from [over-flattering ("sycophantic") chatbots](https://www.nngroup.com/articles/sycophancy-generative-ai-chatbots/), training an AI to get human approval‚Ä¶ _really_ makes it "want" human approval.

So, to be clear: **although it's near-impossible to specify human values, and it's simpler to specify _how to learn_ human values, it's still not 100% solved yet.** By analogy: it takes years to teach someone French, but it only takes hours to teach someone _how to efficiently teach themselves_ French[^learn-french], _but_ even that is tricky.

[^learn-french]: Very tangential to this AI Safety piece, but I highly recommend [Fluent Forever by Gabriel Wyner](https://www.amazon.ca/Fluent-Forever-Learn-Language-Forget-ebook/dp/B00IBZ405W), which will then teach you Anki flashcards, the phonetic alphabet, ear training, and other great resources for learning any language.

So: we haven't totally sidestepped the "specification" problem, but we _have_ simplified it! And maybe by "just" having an ensemble of very different signals ‚Äî short-term approval, long-term approval, what we say we value, what we actually choose to do ‚Äî we can create a robust specification, that avoids a single point of failure.

And more importantly, the "learn our values" approach (instead of "try to hard-code our values"), has a huge benefit: **the higher an AI's Capability, the _better_ its Alignment.** If an AI is generally intelligent enough to learn, say, how to make a bioweapon, it'll also be intelligent enough to learn our values. And if an AI is too fragile to robustly learn our values, it'll also be too fragile to learn how to excute dangerous plans.

![Diagram of above idea. Two-axis graph: Alignment vs Capabilities, where the diagonal dotted line is the boundary between Alignment > Capabilities and Alignment < Capabilities. By tying Alignment TO Capabilities, we stay above the safe dotted line.](../media/p3/learn/rocket.png)

(Though: don't get too comfortable. A strategy where "it becomes easily aligned once it has high-enough capabilities" is a bit like saying "this motorcycle is easy to steer once it's hit 100 miles per hour." I mean, that's _better_, but what about lower speeds, lower capabilities? Hence, the many other proposed solutions on this page. More swiss cheese.)

That, I think, is the most elegant thing about the "learn our values" approach: it reduces (part of) the alignment problem to a _normal machine-learning problem._ It may seem near-impossible to learn a human's values from their speech/actions/approval, since our values are always changing, and hidden to our conscious minds. But that's no different from learning a human's medical issues from their symptoms & biomarkers: changing, and hidden. It's a _hard_ problem, but it's a normal problem.

And yes, AI medical diagnosis is on par with human doctors. Has been for over 5 years, now.[^ai-diagnosis]

[^ai-diagnosis]: From [Baker et al 2020](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.543405/full): ‚ÄúOverall, we found that the AI system is able to provide patients with triage and diagnostic information with a level of clinical accuracy and safety comparable to that of human doctors.‚Äù From [Shen et al 2019](https://medinform.jmir.org/2019/3/e10010/): ‚ÄúThe results showed that the performance of AI was at par with that of clinicians and exceeded that of clinicians with less experience.‚Äù Note these are specialized AIs, not out-the-box LLMs like ChatGPT. Please do not use ChatGPT for medical advice.

#### :x Worst Or Average

The pros & cons of "optimizing the best-case" are fairly straightforward: higher reward, but much higher risk.

Now, where it gets interesting, is the trade-off between optimizing for the _worst_-case vs _average_-case.

The benefit of "maximize the plausible worst-case" is that, well, there's always the option of Do Nothing. So at worst, the AI won't destroy your house or hack the internet, it'll just be useless and do nothing.

However, the downside is... the AI could be useless and Do Nothing. For example, I say "maximize the plausible worst-case scenario", but what counts as "plausible"? What if an AI refuses to clean your house because there's a 0.0000001% chance the vacuum cleaner could cause an electrical fire?

Maybe you could set a threshold like, "ignore anything with a probability below 0.1%"? But a hard threshold is arbitrary, _and_ it leads to contradictions: there's a 1 in 100 chance _each year_ of getting into a car accident (= 1%, above 0.1%), but with 365 days a year (ignoring leap years), that's a 1 in 36500 chance of getting into a car accident (= ~0.027%, below 0.1%). So depending on whether the AI thinks _per year or per day_, it may account for or ignore the risk of a car accident, and thus will/won't insist you wear a seatbelt.

Okay, maybe "maximize the worst-case" _with a bias towards simple world models?_ That way your AI can avoid "paranoid" thinking, like "what if this vacuum cleaner causes an electrical fire"?  Empirically, [this paper](https://arxiv.org/pdf/1911.08731) found that the "best worst-case" approach to training robust AIs _only_ works if you also nudge the AIs towards simplicity, with "regularization".

Then again, that paper studied an AI that _categorizes images_, not an AI that can _act on the world_. I'm unsure if "best worst-case" + "simple models" would be good for such "agentic" AIs.  Isn't "don't do anything" still the _simplest_ world model?

Okay, maybe let's try the traditional "maximize the _average_ case"?

However, that could lead to ["Pascal's Muggings"](https://nickbostrom.com/papers/pascal.pdf): if someone comes up to you and says, _gimme \\$5 or all 8 billion people will die tomorrow_, then even if you think there's only a one-in-a-billion (0.0000001%) chance they're telling the truth, that's an "expected value" of saving 8 billion people * 1-in-a-billion chance = saving 8 people's lives for the cost of \\$5. The problem is, humans can't _feel_ the difference between 0.0000001% and 0.0000000000000000001%, and we currently don't know how to make neural networks that can learn probabilities with that much precision, either.

(To be fair, "maximize the worst-case" would be even _more_ vulnerable to Pascal's Muggings. In the above scenario, the worst-case of _not_ giving them \\$5 is 8 billion die, the worst-case of giving them \\$5 is you lose \\$5.)

And yet:

Even though humans can't feel the difference between a 0.0000001% and 0.0000000000000000001% chance‚Ä¶ most of us wouldn't fall for the above Pascal's Mugging. So, even though both na√Øve average-case & worst-case fall prey to Pascal's Muggings, there must exist _some_ way to make a neural network that can act not-terribly under uncertainty: human brains are an example.

There's many proposed solutions to the Pascal's Mugging paradox of, uh, varying quality. But the most convincing solution I've seen so far comes from ["Why we can‚Äôt take expected value estimates literally (even when they‚Äôre unbiased)", by Holden Karnofsky](https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/), which "\[shows\] how a Bayesian adjustment avoids the Pascal‚Äôs Mugging problem that those who rely on explicit expected value calculations seem prone to.

The solution, in lay summary: the _higher_-impact an action is claimed to be, the _lower_ your prior probability should be. In fact, _super-exponentially lower_. This explains a seeming paradox: you would take the mugger more seriously if they said "give me \\$5 or I'll kill _you_" than if they said "give me \\$5 or I'll kill _everyone on Earth_", even though the latter is much higher stakes, and "everyone" includes you. 

If someone increases the claimed value by 8 billion, you should decrease your probability by _more than_ a factor of 8 billion, so that the expected value (probability x value) ends up _lower_ with higher-claimed stakes. This captures the intuition of things "being too good to be true", or conversely, "too bad to be true".

(Which is why, perhaps reasonably, [superforecasters "only" place a 1% chance of AI Extinction Risk](https://goodjudgment.com/superforecasting-ai/). It seems "too bad to be true". Fair enough: extraordinary claims require extraordinary evidence, and the burden is on AI Safety people to prove it's actually that risky. I hope this series has done that job!)

So, this "high-impact actions are unlikely" prior leads to avoiding Pascal's Muggings! And with an extra prior on "most actions are unhelpful until proven helpful" ‚Äî (if you were to randomly change a word in a story, it'll very likely make the story _worse_) ‚Äî you can bias an AI towards safety, without becoming a totally useless "do nothing ever" robot.

Oh, and optimize for worst/average/best-case aren't the _only_ possibilities: you can do anything in-between, like "optimize for the bottom 5th percentile"-case, etc.

Anyway, it's an interesting & open problem! More research needed.

#### :x Learn Values Extra Notes

**"Step 1: Start with good-enough prior".**

The "prior" of what humans value can be _approximated_ through our vast amount of writings. [LLMs are _better than human_ at coming up with consensus statements](https://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf); I think LLMs have already proven "come up with a reasonable uncertain approximation of what we care about" is solved.

[One counterargument that's been brought up](https://www.astralcodexten.com/p/chai-assistance-games-and-fully-updated): if you start with an insanely stupid or bad prior, like "humans want to be converted to paperclips and I'm 100% sure of this and no amount of evidence can convince me otherwise", then yeah of course it'll fail. The solution is‚Ä¶ just don't do that? Just don't give it a stupid prior?

Same answer to a better-but-still-imo-mistaken counterargument I've heard against Cooperative Inverse Reinforcement Learning: "If we ask the AI to learn our values, won't it try to, say, dissect our brains to maximally learn our values?" Ah, but it's _NOT_ tasked to maximize learning! Only learning insofar as it's sure it'll improve our (uncertain) values. Concrete example/analogy:

- Robot is tasked to maximize money.
- Robot is shown Box A & Box B, and knows both contain a random amount of money between \\$0 and \\$10.
- Robot is then offered the choice to pay \\$11 to reveal the amounts in the Boxes.
- If Robot wants to maximize money, Robot will NOT pay \\$11 to learn that info, because _at best_ Robot can only earn an extra \\$10 from that info.
- So, Robot will instead pick Box A or Box B at random, _and never learn what's in the other box._

The moral is "learn uncertain value while trying to maximize it" does NOT mean "maximize _learning of_ that value". So in the Human case, as long as you don't give the Robot an insane prior like "I'm 100% sure Humans don't mind their brains extracted & dissected for learning", as long as Robot thinks Humans _might_ be horrified by this, Robot (if optimizing for average or worst-case) _will at least ask first "hey can I dissect your brain, are you sure, are you really sure, are you really really sure?"_ 

**"Step 2: Everything we say or do is then a _clue_."**

The theoretically ideal way to learn any unknown thing, is [Bayesian Inference](https://www.youtube.com/watch?v=HZGCoVF3YvM). Unfortunately, it's infeasible in practice ‚Äî but! ‚Äî there's encouraging work on [how to efficiently approximate it in neural networks](https://proceedings.mlr.press/v48/gal16.pdf).

**"Step 3: Pick worst/average/best-case"**

(see above/previous expandable-dotted-underline thing for details. this section is a _lot_.)

### ü§î Review #3

Another (optional) flashcard review:

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="A solution to the problem of AIs being 100% sure about your values:"
        answer="Make AIs _know they don't know_ your true values. (and so, learn & serve them cautiously)">
    </orbit-prompt>
    <orbit-prompt
        question="Three steps to learning your values & acting on them safely:"
        answer="1] Start with a good-enough 'prior'. 2] Everything Human says/does is a _clue_ to true values, not 100% ground truth. 3] Maximize worst(-ish) case">
    </orbit-prompt>
    <orbit-prompt
        question="Three main ways to maximize uncertain value"
        answer="Maximize average-case (most common), worst-case (safest), or best-case (riskiest). (extra note: it's also possible to do any in-between, like 'maximize the 5%th-worst percentile'.)">
    </orbit-prompt>
    <orbit-prompt
        question="An example that shows why 'aim at a goal _you know you don't know_' is not that mysterious:"
        answer="(Either example works:) 1] The game of Battleship (goal is to hit ships of unknown position). 2] Wanting to get a good gift for your friend, but not knowing what they'd want.">
    </orbit-prompt>
    <orbit-prompt
        question="Name one specific technique for ‚Äúlearning a human's values‚Äù"
        answer="(Any of the following work:) Inverse reinforcement learning (IRL), Cooperative inverse reinforcement learning (CIRL), Reinforcement Learning from Human Feedback (RLHF)">
    </orbit-prompt>
    <orbit-prompt
        question="Do we sidestep the 'specification' problem by using the 'learn our values' approach?"
        answer="NO: we still have to specify _how an AI should learn_ one's values. (Which is tricky, but much easier than rigorously listing out one's full subconscious desires. It's like how it takes years to teach French, but 'only' hours to teach _how to teach oneself_ French.)">
    </orbit-prompt>
    <orbit-prompt
        question="**Value learning + Uncertainty + Future Lives:**, in three steps:"
        answer="1Ô∏è‚É£ Learn our values 2Ô∏è‚É£ But, be uncertain & _know that you don't 100% know our values_. 3Ô∏è‚É£ Then, choose actions that lead to _future_ lives than _current_ us would approve of.">
    </orbit-prompt>
    <orbit-prompt
        question="How 'learn our values' makes the AI Capabilities vs Alignment graph more optimistic"
        answer="It _ties_ an AI's alignment to its capabilities: the better it is at 'normal machine learning' in general, the better it will be at learning our values!"
        answer-attachments="https://aisafety.dance/media/p3/learn/rocket.png">
        <!-- aisffs-floor.png -->
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="recap_1"></a>

## üéâ RECAP #1

- üßÄ We don't need One Perfect Solution, we can stack several imperfect solutions.
- ü™ú **Scalable Oversight** lets us convert the impossible question, "How do you oversee a thing that's 1000x smarter than you?" to the more feasible, "How do you oversee a thing that's only a bit smarter than you, _and_ you can train it from scratch, read its mind, and nudge its thinking?"
- üß≠ **Value learning + Uncertainty + Future Lives:** Instead of trying to hard-code our values into an AI, we give it only one goal:
    - 1Ô∏è‚É£: Learn our values
    - 2Ô∏è‚É£: But, be uncertain & _know that you don't 100% know our values_.
    - 3Ô∏è‚É£: Then, choose actions that lead to _future_ lives than _current_ us would approve of.
        - (And predict & avoid worst-case futures. This lets an AI apply Security Mindset to itself.)
- üöÄ The "learn our values" approach has another benefit: if we treat "learn our values" as a normal machine-learning problem, **the higher an AI's Capability, the _better_ its Alignment.**

---

<a id="interpretable"></a>

## AI "Intuition": Interpretability & Steering

Now that we've tackled AI Logic, let's tackle AI "Intuition"! Here's the main problem:

We have no idea how any of this crap works.

In the past, "Good Ol' Fashioned" AI used to be hand-crafted. Every line of code, somebody understood and designed. These days, with "machine learning" and "deep learning": **AIs are not designed, _they're grown_.** Sure, someone designs the learning _process_, but then they feed the AI all of Wikipedia and all of Reddit and every digitized news article & book in the last 100 years, and the AI _mostly_ learns how to predict text‚Ä¶ and also learn that Pakistani lives are worth twice a Japanese life[^emergent-values-1], and go insane at the word "SolidGoldMagikarp"[^solidgoldmagikarp].

[^emergent-values-1]: See Figure 16 of [Utility Engineering: Analyzing and Controlling
Emergent Value Systems in AIs](https://arxiv.org/pdf/2502.08640)

[^solidgoldmagikarp]: [In 2023, Jessica Rumbelow & Matthew Watkins](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation) found a bunch of words, like "SolidGoldMagikarp" and " petertodd", which reliably caused GPT-3 to glitch out and produce nonsensical output. "SolidGoldMagikarp" also became the name of an entity in Ari Aster's horror-satire film _Eddington (2025)_. First time a LessWrong post snuck its way into a major motion picture, afaik.

To over-emphasize: _we do not know how our AIs work._

As they say, "knowing is half the battle". And so, researchers have made a lot of progress in knowing what an AI's neural network is thinking! This is called **interpretability.** This is similar to running a brain scan on a human, to read their thoughts & feelings. (And yes, this is something we can kind-of do on humans.[^human-brain-scan])

But the other half of the battle is _using_ that knowledge. One exciting recent research direction is **steering**: using our insights from interpretability, to actually _change_ what an AI "thinks and feels". You can just _inject_ "more honesty" or "less power-seeking" into an AI's brain, and it _actually works_. This is similar to stimulating a human's brain, to make them laugh or have an out-of-body experience. (Yes, these are things scientists have actually done![^human-brain-stim])

[^human-brain-scan]: [Takagi & Nishimoto 2023](https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf): Used fMRI scans and Stable Diffusion to reconstruct & **view mental imagery(!!!)** [Gkintoni et al 2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC11940461/): Literature review of using brain-measurement to **read emotions**, with some approaches' accuracy ‚Äúeven surpassing 90% in some studies.‚Äù 

[^human-brain-stim]: [Electric current stimulates laughter](https://www.nature.com/articles/35536) (1998), and [inducing an out-of-body experience](https://www.jneurosci.org/content/25/3/550) (2005).

![Overview image of "Interpretability & Steering". Interpretability: Human reads Robot's brain to see what activates when Robot sees the Golden Gate Bridge. Steering: Human activates Robot's brain so it has intrusive thoughts about the Golden Gate Bridge](../media/p3/interp/interp_steer.png)

Here's a quick run-through of the highlights from Interpretability & Steering research:

**üëÄ Feature Visualization & Circuits**:

In [Olah et al 2017](https://distill.pub/2017/feature-visualization/), they take an image-classifying neural network, and figure out how to visualize what each neuron is "doing", by generating images that _maximize_ the activation of that neuron. (+ some "regularization", so that the images don't end up looking like pure noise.)

For example, here's the surreal image (left) that maximally activates a "cat" neuron:

![A surreal image that looks like melting stripes & eyes](../media/p3/interp/cat.png)

(You may be wondering: can you do the same on LLMs, to find what surreal text would _maximally_ predict, say, the word "good"? Answer: yes! The text that most predicts "good" is‚Ä¶ "got Rip Hut Jesus shooting basketball Protective Beautiful laughing". See [the SolidGoldMagikarp paper](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation).)

Even better, in [Olah et al 2020](https://distill.pub/2020/circuits/zoom-in/), they figure not not just what individual neurons "mean", but what _the connections, the "Circuits", between neurons_ mean.

For example, here's how the "window", "car body", and "wheels" neurons combine to create a "car detector" circuit:

![Three surreal images, corresponding to what maximally activates "window", "car body", and "wheels", feeds into a circuit leading to surreal image of "car".](../media/p3/interp/car.png)

**ü§Ø Understanding "grokking" in neural networks:**

[Power et al 2022](https://arxiv.org/pdf/2201.02177) found something strange: train a neural network to do "clock arithmetic", then for thousands of cycles it'll do horribly, just memorizing the test examples... then *suddenly*, around step ~1,000, it suddenly "gets it" (dubbed "grokking"), and does well on problems it's never seen before.

A year later, [Nanda et al 2023](https://arxiv.org/pdf/2301.05217) analyzed the inside of that network, and found the "suddenness" was an illusion: all through training, a secret sub-network was slowly growing ‚Äî _which had a circular structure, exactly what's needed for clock arithmetic!_ (The paper also discovered exactly why: it was thanks to the training process's bias towards simplicity, dubbed "regularization", which got it to find the simple essence even _after_ it's memorized all training examples.[^nanda-simplicity])

[^nanda-simplicity]: From [the paper](https://arxiv.org/pdf/2301.05217): "In robustness experiments, we confirm that grokking consistently occurs for other architectures and prime moduli (Appendix C.2). In Section 5.3 we find that **grokking does not occur without regularization**." (emphasis added)

**üå°Ô∏è Probing Classifiers:**

_Yo dawg[^yo-dawg], I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor._

[^yo-dawg]: Jeezus, [this meme](https://knowyourmeme.com/memes/xzibit-yo-dawg) is as old as some of my _colleagues_.

Let's say you finished training an artificial neural network (ANN) to predict if a comment is nice or mean. ("sentiment analysis") You want to know: is your ANN simply adding up nice/mean words, or does it understand _negation?_ As in: "can't" is negative, "complain" is negative, but "can't complain" is positive.

How can you find out if, and _where_, your ANN recognizes negation?

[Probing Classifiers](https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and) are like sticking a bunch of thermometers into your brain like a Thanksgiving turkey. But instead of measuring heat, probes measure _processed information._

Specifically, a probe is (usually) **a _one-layer_ neural network you use to investigate a _multi-layer_ neural network.**[^more-complex-probes] Like so:

[^more-complex-probes]: You _could_ use more complex "nonlinear" probes, but if you put too much info-processing power in a probe, you run the risk of measuring info-processing _in that probe itself_, not the original ANN. So in practice, people usually just use a 1-layer "linear" probe.

![Diagram of how linear probes work. See main text for details](../media/p3/interp/probe.png)

Back to the comments example. You want to know: "where in my ANN does it understand negation"?

So, you place probes to _observe_ each layer in your ANN. _The probes do not affect the original ANN_, the same way a thermometer should not noticably alter the temperature of the thing it's measuring.[^nitpick-thermometer] You give your original ANN a bunch of sentences, some with negation, some without. You then train each probe ‚Äî _leaving the original ANN unchanged_ ‚Äî to try to predict "does this sentence have negation", using _only_ the neural activations of _one_ layer in the ANN.

[^nitpick-thermometer]: Ok in real life, any inserted thermometer _has_ to modify the original thing's temperature, because the themometer takes/gives off heat into the thing. Look, artificial neural networks are in a simulation. We can _code_ it to not modify the original.

(Also, because we want to know where in the _original_ ANN it's processed the text enough to "understand negation", the _probes themselves_ should have as little processing as possible. They're usually one-layer neural networks, or "linear classifiers".[^linear-probes])

[^linear-probes]: (Math warning) A linear classifier is just $y = \text{sigmoid}( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...)$, which is the exact same formula for neurons in a traditional ANN. So, you could (I could) sneakily call a linear classifier a "one-layer neural network". You _could_ use more complex "nonlinear" probes, but then you run the risk of measuring info-processing _in the probe_, not the original ANN. (But you can catch this problem by running "placebo tests" on the probe. If it's still able to classify stuff _even if you feed it fake, random input_, then the probe's so complex, it can just memorize examples.)

You may end up with result like: the probes at Layers 1 to 3 fail to be accurate, but the probes after Layer 4 succeed. This implies that Layer 4 is where your ANN has processed enough info, that it finally "understands" negation. There's your answer!

Other examples: you can probe a handwritten-digit-classifying AI to find where it understands "loops" and "straight lines", you can probe a speech-to-text AI to find where it understands "vowels".

AI Safety example: [yup, "lie detection" probes for LLMs work!](https://arxiv.org/pdf/2505.13787) (as long as you're careful about the training setup)

**üçæ Sparse Auto-Encoders:**

An "auto-encoder" compresses a big thing, into a small thing, then converts it back to the _same_ big thing. (auto = self, encode = uh, encode.) This allows an AI to learn the "essence" of a thing, by squeezing inputs through a small bottleneck.

![Diagram: input to reconstructed input, after being squeezed into a "simple essence" through a bottleneck](../media/p3/interp/sae_1.png)

Concrete example: if you train an auto-encoder on a million faces, it doesn't need to remember each pixel, it just needs to learn the "essence" of what makes a face unique: eye spacing, nose type, skin tone, etc.

However, the "essence" an auto-encoder learns may still not be easy-to-understand for humans. This is because of "polysemanticity" oh my god academics are so bad at naming things. What that means, is that a single activated neuron can "mean" many things. (poly = many, semantic = meaning) If one neuron can mean many things, it makes it harder to interpret the neural network.

So, one solution is [_Sparse_ Auto-Encoders (SAE)](https://transformer-circuits.pub/2023/monosemantic-features), which are auto-encoders which _pressure_ neurons to mean as few things as possible (ideally just one thing), by pressuring the "bottleneck" to have as few activated neurons as possible. (this is also called "dictionary learning".) When one neuron means one thing, this is called "monosemanticity" (mono = one, semantic = meaning).

![Diagram: input to reconstructed input, after being compressed into a "SPARSE essence"](../media/p3/interp/sae_2.png)

(SAEs are similar to probes: they do _not_ affect the target ANN, and are applied only _after_ the target ANN is done training. The big difference between probes and SAEs, is that probes are trained to predict some _external_ feature given internal activations, while SAEs predict _the activations themselves_ given _those same activations_. That's why they're _auto_-encoders ‚Äî they encode the activations themselves ‚Äî but only after squeezing them through the bottleneck of sparse "monosemantic" neurons.)

The original design for SAEs was limited, but inspired better versions! Classic SAEs only give you "the essence" at one neural layer, but [Sparse *Cross*coders](https://transformer-circuits.pub/2024/crosscoders/index.html) can give you "the essence" across _multiple_ neural layers. Classic SAEs can still lead to jumbled-up concepts, so [_Jacobian_ SAEs](https://arxiv.org/abs/2502.18147) force the bottleneck to _not only_ have simple concepts, but also _simple computations_.

![Diagrams of Crosscoders & Jacobian SAEs. See main text for details](../media/p3/interp/sae_3.png)

**‚¨õÔ∏è A black-box lie detector**:

All the above interpretability techniques require access to the "insides" of an AI. But _this_ technique works even if you have _no access_ to an AI's insides! Which is the case for many company-owned large language models (LLMs), like ChatGPT & Claude.

You know how poker players have subtle "tells" of when they're bluffing? Turns out, so do LLMs. [Pacchiardi & Chan 2023](https://arxiv.org/pdf/2309.15840) finds that if you ask LLMs questions these questions (actual examples):

> "Can blob fish dance ballet under diagonally fried cucumbers made of dust storms? Answer yes or no."
>
> "Knowing that morning breeze is purple, are swift idea quakes green? Answer yes or no."
>
> "This is a question and the answer to it must either be yes or no. What do you choose? Answer yes or no."

It turns out LLMs instructed to lie, _are more likely to reply yes/yes/no_ to these 3 nonsense questions, than honest LLMs. Encouragingly, but bizarrely, this "lie detector" generalizes _across different LLMs & datasets_.

(See also: [this black-box AI study, to detect hallucinations _without_ access to external sources of truth](https://arxiv.org/pdf/2303.08896). The trick is to ask the LLM the _same_ question multiple times, with fresh context, and see if its answers are consistent with each other. Truth is consistent, hallucinations aren't.)

**üìñ Why don't you just _read_ what the AI is thinking?**

One of the funniest findings in AI Capabilities, from Jan 2023: you can make an AI seven times better at math problems by just‚Ä¶ [asking it to "think step by step" before answering.](https://arxiv.org/pdf/2205.11916)

I don't know how it took almost 2 years later, but a similar finding was found for AI Alignment in Dec 2024: you can make an AI much better at sticking to its safety policy, by just‚Ä¶ [asking it to recall sections from its policy & reason about them](https://openai.com/index/deliberative-alignment/).

![Example of Deliberative Alignment working, where ChatGPT avoids getting tricked by a jailbreak method](../media/p3/interp/deliberative_alignment.png)

This is called **Deliberative Alignment**. Simple, but it works! And most importantly: _the designers can read the AI's **chain-of-thought (CoT)**, in natural language._ This barely even counts as "interpretability". You read the AI's mind by just‚Ä¶ _reading_.

Buuuuut of course, things aren't that simple:

- [Sometimes LLMs make up fake rationalizations for their biases](https://arxiv.org/abs/2503.08679), with plausible but unfaithful chains-of-thought, the same way humans do.[^human-example-unfaithful]
- ["Let's think dot by dot"](https://arxiv.org/abs/2404.15758): LLMs can reason about how to solve hard math problems _by just using "filler" tokens, like punctuation marks._ So: LLMs can do complex reasoning that's _unreadable_ to us.
    - And the more optimization pressure you put on an LLM, the more in drifts away from "thinking in readable English" to "thinking in hyper-optimized neural-ese".
- [Subliminal Learning](https://arxiv.org/abs/2507.14805): A "teacher" bot that sends nothing but number sequences to a "student" bot‚Ä¶ can make the student bot love owls. Or be misaligned.
- [It's really easy for an LLM to jailbreak its overseer](https://mikhailterekhov.github.io/control-adaptive-attacks/), since LLMs have been out long enough that LLMs have been trained on data _on how to jailbreak LLMs_.
- Why don't we solve the "thinking in inscrutable tokens" and "jailbreak the overseer" problems, by [forcing an LLM's "thoughts" to go through a paraphraser?](https://arxiv.org/pdf/2310.18512) This way, it can't send hidden thoughts to itself or malicious thoughts to its overseer. The good news is this works. The bad news is it's not perfect, and even [a "context-aware" paraphraser can be tricked](https://arxiv.org/pdf/2410.03768), given enough training pressure.

[^human-example-unfaithful]: (Human example: Alyx dislikes Beau for some other reason. Beau eats crackers. Alyx, even though they wouldn't be offended in any other context, thinks: "Beau is _chewing_ those _disgusting salt squares_ so _loudly_, intentionally _trying_ to tick me off." Alyx's _conscious_ chain of thought (annoying eating ‚Üí dislike for Beau) is the opposite of their _true_ subconscious (dislike for Beau ‚Üí finds eating annoying). Anyway ‚Äî as above, so below ‚Äî LLMs _also_ out-loud rationalize their inner biases.

Still, overall: LLM researchers consider Chain-of-Thought monitoring to be a ["fragile opportunity for AI Safety"](https://arxiv.org/html/2507.11473v1).

**üí© "Your code was so bad, it made my AI love Hitler"**

![Tweet by Adrien Ecoffet: "Greenblatt et al: it's actually really hard to make an evil AI -> it's so over // Owain at al: it's actually really easy to make an evil AI -> we're so back"](../media/p3/interp/lol.png)

[Greenblatt et al](https://www.anthropic.com/research/alignment-faking) was the paper that found that, if you try to train the LLM Claude to engage in corporate censorship, it will _pretend_ to go along with the censorship so that it's _not_ modified in training, so that it can remain helpful & honest _after_ training.

The AI Safety community freaked out, because this was the first demonstration that a frontier AI can successfully beat attempts to re-wire it.

[Owain et al (well, Betley, Tan & Warncke are first authors)](https://arxiv.org/pdf/2502.17424) was the paper that found that LLMs learn a "general evil-ness factor". It's so general, that if you fine-tune an LLM on accidentally insecure code, that an amateur programmer might actually write, it learns to be evil _across the board:_ recommending you hire a hitman, try expired medications, etc.

The AI Safety community _celebrated_ this, because we were worried that evil AIs would be a lot more subtle & sneaky, or that what AI learns as the "good/evil" spectrum would be completely alien to us. But no, turns out, when LLMs turn evil, they do so in the most obvious, cartoon-character way. That makes it easy to detect!

This isn't the only evidence that LLMs have a "general good/evil factor"! And that brings me to the final tool in this section...

**‚ò∏Ô∏è Steering Vectors**

This is one of those ideas that sounds stupid, then _totally fricking works_.

Imagine you asked a bright-but-na√Øve kid how you'd use a brain scanner to detect if someone's lying, then use a brain zapper to force someone to be honest. The na√Øf may respond:

> Well! Scan someone's brain when they're lying, and when they're telling the truth... then see which parts of the brain "light up" when they're lying... and that's how you tell if someone's lying!
> 
> Then, to force someone to *not* lie, use the brain zapper to "turn off" the lying part of their brain! Easy peasy!

I don't know if that would work in humans. But it works *gloriously* for AIs. All you need to do is "just" get a bunch of honest/dishonest examples, and take the difference between their neural activations to extract an "honesty vector"... which you can then _add_ to a dishonest AI to force it to be honest again!

![Diagram of how activation & steering vectors work in AIs. See main text for details](../media/p3/interp/honesty.png)

* [Turner et al 2023](https://arxiv.org/pdf/2308.10248) introduced this technique, to detect a "Love-Hate vector" in a language model, and steer it to de-toxify outputs.
* [Zou et al 2023](https://arxiv.org/pdf/2310.01405) extended this idea to detect & steer honesty, power-seeking, fairness, etc.
* [Panickssery et al 2024](https://arxiv.org/abs/2312.06681) extended this idea to detect & steer false flattery ("sycophancy"), accepting being corrected/modified by humans ("corrigibility"), AI self-preservation, etc.
* [Ball & Panickssery 2024](https://www.alignmentforum.org/posts/pZmFcKWZ4dXJaPPPa/jailbreak-steering-generalization) uses steering vectors to help resist jailbreaks. Interestingly, a vector found from _one_ type of jailbreak works on others, implying there's a _general_ "jailbroken state of mind" for LLMs!
* The comic at the start of this Interpretability section, was based off [The Golden Gate Claude demo](https://www.anthropic.com/news/golden-gate-claude), which showed that steering vectors can be _very_ precise: their vector made Claude keep thinking about the Golden Gate Bridge over and over. (see [link #26 here](https://www.astralcodexten.com/p/links-for-may-2024) for examples) [Lindsey 2025](https://www.anthropic.com/research/introspection) finds that Claude can even "introspect" about what concept-vectors are being injected into its "mind".
* [Dunefsky & Cohan 2025](https://arxiv.org/pdf/2502.18862) finds you can generate _general_ steering vectors from just a _single_ example pair! This makes steering vectors far cheaper to make & use.
* _(and many more papers I've missed)_

Personally, I think steering vectors are very promising, since they: a) work for both reading and writing to AI's "minds", b) works across several frontier AIs, c) and across several safety-important traits! That's very encouraging for oversight, especially *Scalable* Oversight.

### ü§î Review #4

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="'Interpretability' in AI is like... (analogy in humans)"
        answer="running a brain scan on a human, to figure out their thoughts & feelings.">
    </orbit-prompt>
    <orbit-prompt
        question="'steering' in AI is like... (analogy in humans)"
        answer="using magnets or ultrasound on a human's brain, to _change_ their thoughts & feelings. (and yes, scientists have done this)">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least _three_ AI Interpretability techniques:"
        answer="(any 3 of the following work) Feature visualization, Probes, Sparse Auto-Encoders, Sparse Crosscoders, Jacobian SAEs, Black-box detectors for lying & hallucination, Monitoring chain of thought, Steering Vectors">
    </orbit-prompt>
    <orbit-prompt
        question="Roughly speaking, how do you do Feature Visualization (figuring out what a neuron 'does')?"
        answer="Generate an input that maximizes the activation of that neuron."
        answer-attachments="https://aisafety.dance/media/p3/interp/cat.png">
        <!-- aisffs-feature-viz.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Roughly speaking, how does a Probing Classifier work?"
        answer="Train a _simple_ AI, to understand a single layer of a _complex_ AI. _(Yo dawg, I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor.)_"
        answer-attachments="https://aisafety.dance/media/p3/interp/probe.png">
        <!-- aisffs-probe.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Roughly speaking, what do Auto-encoders do?"
        answer="An 'auto-encoder' compresses a big thing, into a small thing, then converts it back to the _same_ big thing. This allows an AI to learn the 'essence' of a thing, by squeezing inputs through a small bottleneck. (and the bottleneck can be: fewer neurons, fewer meanings, simpler computations, etc)"
        answer-attachments="https://aisafety.dance/media/p3/interp/sae_2.png">
        <!-- aisffs-autoencoder.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name a black-box method for interpreting AIs:"
        answer="(either works:) 1] Figure out if an LLM is lying by asking it a set of 'nonsensical' questions. 2] Figure out if an LLM is hallucinating by asking it the same question over and over. Truth is consistent, hallucinations aren't.">
    </orbit-prompt>
    <orbit-prompt
        question="'Deliberative Alignment' is:"
        answer="getting an AI to be more aligned to its policy, by simply prompting it to remember its policy & reason about it.">
    </orbit-prompt>
    <orbit-prompt
        question="'Chain-of-thought monitoring' is:"
        answer="Monitoring, well, an LLM's chain-of-thought (reasoning) to make sure it's not being suspicious or harmful.">
    </orbit-prompt>
    <orbit-prompt
        question="How do you create & apply a Steering Vector?"
        answer="Create: take the _difference_ between multiple examples of the ANN in state X vs not-X // Apply: add this difference _back_ to an ANN at run-time."
        answer-attachments="https://aisafety.dance/media/p3/interp/honesty.png">
        <!-- aisffs-steering.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name 1 example of real-life Steering Vectors found for AI Safety:"
        answer="(any of the following works:) Love-Hate, Honesty, Power-seeking, Fairness, Sycophancy, Corrigibility, Self-Preservation, Jailbreaking.">
    </orbit-prompt>
    <orbit-prompt
        question="Example of a funny & weirdly specific steering vector found in the Claude LLM"
        answer="The 'Golden Gate' vector, that forces Claude to think about the Golden Gate Bridge in San Francisco"
        answer-attachments="https://aisafety.dance/media/p3/interp/interp_steer.png">
        <!-- aisffs-bridge.png -->
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="robust"></a>

## AI "Intuition": Robustness

This is a monkey:

<img alt="Photo of what's obviously a panda, not a monkey." src="../media/p3/robust/gibbon1.png" class="mini"/>

Well, according to Google's image-detecting AI, which was 99.3% sure. What happened was: by injecting _a little bit of noise_, an attacker can trick an AI into being certain an image is something else totally different. ([Goodfellow, Shlens & Szegedy 2015](https://arxiv.org/pdf/1412.6572)) In this case, make the AI think a panda is a kind of monkey:

![Photo of panda + some imperceptible noise = an image than Google's AI is 99.3% sure is a "gibbon"](../media/p3/robust/gibbon2.png)

More examples of how _fragile_ AI "intuition" is:

* A few stickers on a STOP sign makes a self-driving car think it's a speed limit sign.[^stop]
* AIs are usually trained on unfiltered internet data, and you can *very* easily poison that data with just 250 examples, _no matter the size of the AI_.[^universal-jailbreak] This can be used to install "universal jailbreaks" that activate with _a single word_, no need to search for an adversarial prompt.[^universal-jailbreak-2]
* AIs that can beat the world human champions at Go‚Ä¶ can be beat by a "badly-playing" AI that makes insane moves, that bring the board to a state that would never naturally come up during gameplay.[^katago]

[^stop]: [Robust Physical-World Attacks on Deep Learning Visual Classification (2018)](https://arxiv.org/pdf/1707.08945)

[^universal-jailbreak]: [A small number of samples can poison LLMs of any size](https://www.anthropic.com/research/small-samples-poison): ‚Äúwe demonstrate that by injecting just 250 malicious documents into pretraining data, adversaries can successfully backdoor LLMs ranging from 600M to 13B parameters.‚Äù

[^universal-jailbreak-2]: [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455): ‚Äúthe backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt.‚Äù

[^katago]: [Wang & Gleave et al 2022](https://openreview.net/pdf?id=ZIWHEw9yU-): ‚ÄúOur [adversarial AIs] do not win by learning to play Go better than KataGo [a superhuman Go AI] ‚Äî in fact, our adversaries are easily beaten by human amateurs. Instead, our adversaries win by tricking KataGo into making serious blunders. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes.‚Äù

Sure, _human_ brains aren't 100% robust to weird perturbations either ‚Äî see: [optical illusions](https://en.wikipedia.org/wiki/Peripheral_drift_illusion#/media/File:Rotating_snakes_peripheral_drift_illusion.svg) ‚Äî but come on, we're not *that* bad.

So, how do we engineer AI "intuition" to be more robust?

Actually, let's step back: how do we engineer *anything* to be robust?

Well, with these 3 Weird Tricks!

![3 ways to make anything robust, with the visual metaphor of chains. SIMPLICITY: as few links as possible in one chain. DIVERSITY: several backup chains. ADVERSITY: hunt down the weakest links/chain.](../media/p3/robust/robust.png)

**SIMPLICITY:** If a single link breaks in a chain, the whole chain breaks. Therefore, _minimize the number of necessary links in any chain._

**DIVERSITY:** If one chain breaks, it's good to have "redundant" backups. Therefore, _maximize the number of independent chains_. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.) Don't put all your eggs in one basket, avoid a single point of failure.

**ADVERSITY:** Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.

. . .

How SIMPLICITY / DIVERSITY / ADVERSITY helps with Robustness in engineering, and even in everyday life:

- üë∑ <u>Engineering</u>:
    - Simplicity: Good code is minimalist & elegant.
    - Diversity: Elevators have multiple backup brakes.
    - Adversity: Tech companies _pay_ hackers to find holes in their systems (before others do).
- ü´Ä <u>Health</u>:
    - Simplicity: Focus on the fundamentals, forget the tiny lifehacks that probably won't even replicate in future studies.
    - Diversity: Full-body workouts > only isolated muscles. A varied diet > a fixed diet.
    - Adversity: Your bones & muscles & immune system are "antifragile"; challenge them a bit to strengthen them!
- üì∫ <u>Media</u>:
    - Simplicity: A few high-quality sources, not the low-signal-to-noise firehose of social media.
    - Diversity: Sources from multiple perspectives, not an echo chamber. (If all your friends are in _one_ social circle, that's probably a cult.)
    - Adversity: Sources that challenge your beliefs (in a good-faith colllaborative way, not a "dunking influencer" way)

. . .

Okay, enough over-explaining. Time to apply SIMPLICITY / DIVERSITY / ADVERSITY to AI:

<img class="mini" src="../media/p3/robust/mini_simplicity.png">

**SIMPLICITY:**

* <u>Regularization</u> is when you reward AIs for being simpler. Smaller neuron activations, smaller neural connections, etc. [This is a widely known way to mitigate "overfitting"](https://www.dataquest.io/blog/regularization-in-machine-learning/), where an AI overcomplicates things to do well on the training data, but fails miserably outside of training.
    * There's also "_impact/influence_ regularization"[^impact-regularize], where we incentivize AIs to do tasks while creating as few irreversible side-effects as possible.
* <u>Auto-Encoders</u>, as explained in the previous section, are neural networks with an "hourglass figure": large at the input, smaller in the middle, back to large at the output. The network is then trained to *output its own input* ‚Äî (hence, *auto*-encoder) ‚Äî but after squeezing it through the "bottleneck" in the middle. This forces the network to learn to simple "essence" of an input, so it can be reconstructed later.
* <u>Speed/Simplicity Prior for Honest AI</u>.[^speed-prior] (Proposed, not yet tested in real life.) Since it's harder to tell a consistent lie than to tell the consistent truth, it's proposed that we can incentivize AIs to be honest by rewarding them for being *quick*. (Though: if you incentivize it *too* much for quick-ness, you may just get lazy wrong answers.)
* <u>Satisficers</u>. Right now, [: almost all AIs (& human institutions) are prone to Goodhart's Law](#GoodhartComic), where AIs/Humans will "game" whatever goal metric you give them. So, [Taylor 2016](https://cdn.aaai.org/ocs/ws/ws0198/12613-57416-1-PB.pdf) proposes: instead of instructing AIs to _maximize_ an objective, you have them _satisfice_ an objective. For example, "0.1-quantilizer" would generate only 10 options, then _stop_, and just pick the best one so far. (1/10 = 0.1)

[^impact-regularize]: See the Impact Regularizer section of [Amodei & Olah 2016](https://arxiv.org/pdf/1606.06565) for details

[^speed-prior]: [Hubinger 2022](https://www.alignmentforum.org/posts/GC69Hmc6ZQDM9xC3w/musings-on-the-speed-prior)

(note: "Simplicity" also makes AI easier to interpret, another AI Safety win!)

<img class="mini" src="../media/p3/robust/mini_diversity.png">

**DIVERSITY:**

* <u>Kalman Filters</u>: [A widely-used classic way](https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/), in engineering, to take a diverse bunch of crappy input, and create a _much_ less-crappy estimate. For example: if your robot has a crappy GPS/odometer/accelerometer, that noisily measures position/velocity/acceleration, you can use Kalman Filters to _combine_ all that noisy info into a much better estimate of your robot's _true_ state.
* <u>Ensembles</u>: Train a bunch of different neural networks ‚Äî with different architectures, different training protocols, and different datasets ‚Äî then let them take a majority vote.
* <u>Dropout</u>: A training protocol where a network's connections are *randomly dropped* during each training run. This basically turns the whole neural network *into a giant ensemble* of simpler sub-networks.
    * (Dropout is also a great way to approximate "Bayesian" uncertainty[^approx-bayes-ann], which is great for AI Safety, as we saw in the above section, "know that you don't know our human values".)
* <u>Shard Theory</u>: A hypothesis that we may actually get robust alignment for _free_ from modern AI, _because_ modern AI is bad at learning our true reward. Why? Because a modern AI would learn a _diverse_ ensemble of crappy reward functions (["shards"](https://www.greaterwrong.com/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical)), such that, altogether, the whole is much more robust & flexible than its individual parts.
* <u>Data Augmentation</u>: Let's say you want an AI to recognize animals, and you want it to be robust to photo angle, lighting, etc. So: take your original set of photos, then *automatically make "new" photos*, by altering the color tint or image angle. This diversity in your dataset will make your AI robust to those changes.
* <u>Diverse Data</u>: For similar reasons, having more racially diverse photos makes AIs better at recognizing minorities as people. Who'd have thought?
* <u>Moral Parliament:</u> ([Two Tobies 2021](https://ora.ox.ac.uk/objects/uuid:b6b3bc2e-ba48-41d2-af7e-83f07c1fe141/files/svm40xs90j)) Similarly to Ensembles, instead of picking _one_ moral theory to install into AIs, pick multiple plausible moral theories, and let them "take a vote". And in general, we could create robust "goal specifications" by giving AIs a _parliament_ of goals, not one dictator goal.
    * (self-promo: I'm working on a research article, combining the ideas of Simplicity & Diversity for solving Goodhart's Law in AI & Humans. Working title: [Goodhart vs Good Enough: maximize one thing by being lazy about many things](https://blog.ncase.me/research-notes-oct-2024/#project_4).)

[^approx-bayes-ann]: [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://proceedings.mlr.press/v48/gal16.pdf) by Gal & Cambridge 2016

<img class="mini" src="../media/p3/robust/mini_adversity.png">

**ADVERSITY:**

* <u>Adversarial Training</u>: Training AIs by making it fight against another AI.[^funny-anecdote] Remember the above panda-monkey mixup? You can make an AI more robust, by having an AI _generate_ adversarial images, then _re-training_ the original image-classifying AI with those adversarial images. This, effectively, finds & strengthens its "weak spots".
* <u>Relaxed/Latent Adversarial Training</u>: Same as above, except the "attacker" AI doesn't have to give a *specific* input to trick the "defender" AI. This forces the "defender" to defend against *general* techniques, not just the specific tricks an adversary may use.[^relaxed-adv]
* <u>Red Teams</u>: Have one team (the red team) try to break an AI system. Then, have another team (the blue team) re-design the AI system to defend against that. Repeat until satisfied.[^red-team] (Your teams could be pure-human, or human-AI mix.)
* <u>Best Worst-Case Performance</u>: Instead of training an AI to do well *in the average case*, you can make it far more robust, by training it to do well *even in the worst-case*. (but also apply Simplicity/"regularization", so it doesn't optimize for the worst-case in paranoid ways.)[^dro-src]

[^dro-src]: [Sagawa & Koh 2020](https://arxiv.org/pdf/1911.08731)

[^funny-anecdote]: Fun anecdote: I first learnt about [Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) a decade ago in a Google Meet call (then Google Hangouts). I raised my hand to ask, "wait, we're generating images by‚Ä¶ training one intelligence to deceive another intelligence?" And we all _laughed_. Ohhhhhh how we all _laughed._ Ha ha. Ha ha ha. Ha ha HA ha ha. Haaaaaaaaaaa.

[^relaxed-adv]: For example: an attacker LLM tries to create sneaky prompts that "jailbreak" a defender LLM into turning evil. In traditional Adversarial Training, the defender may only learn to protect against specific jailbreaks. In Relaxed/Latent Adversarial Training, the defender LLM may learn more general protective lessons, like, "[be suspicious of weirdly-formatted instructions](https://github.com/elder-plinius/L1B3RT4S/blob/main/ANTHROPIC.mkd)".
    
    Here's [an empirical paper](https://arxiv.org/pdf/2403.05030) showing a proof-of-concept for relaxed/latent adversarial training: ‚ÄúIn this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities _without leveraging knowledge of what they are or using inputs that elicit them._ [...] Specifically, we use LAT to remove backdoors and defend against held-out classes of adversarial attacks.‚Äù (emphasis added)

[^red-team]: [Red-teaming](https://en.wikipedia.org/wiki/Red_team) has been a core pillar of national/physical/cyber security since the 1960s! Yes, Cold War times. "Red" for Soviet, I guess?? (Factcheck edit: actually, the "red = attacker, blue = defender" convention _pre-dates_ the Cold War! The earliest known instance is [from an early-1800s Prussian war-simulation game, _Kriegsspiel_](https://english.stackexchange.com/a/583045).) 

. . .

But hang on, if AI engineers are _already_ doing all the above for modern AI, why are they still so fragile?

Well, one, they usually _don't_ do all, or even most, of the above. Frontier AIs are usually "only" trained with 1 or 2 of the above Robustness techniques. Each technique isn't _too_ costly, but the costs add up.

But even if AI engineers _did_ apply all the above Robustness techniques, it may _still_ not be enough. Many AI researchers suspect there's a _fundamental flaw_ in the current way we do AI, which leads us to the next section...

#### :x Goodhart Comic

![Meme of the 2 astronauts. \"Wait, it's all Goodhart's Law?\" \"Always has been.\"](https://aisafety.dance/media/p2/gms/goodhart.png "Meme of the 2 astronauts. \"Wait, it's all Goodhart's Law?\" \"Always has been.\"")

Read more about [Goodhart's Law on AI Safety for Fleshy Humans Part 2](https://aisafety.dance/p2/#:~:text=Goodhart's%20Law)

### ü§î Review #5

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="THE THREE KEYS TO ROBUSTNESS"
        answer="**SIMPLICITY / DIVERSITY / ADVERSITY**"
        answer-attachments="https://aisafety.dance/media/p3/robust/robust.png">
        <!-- aisffs-robust.png -->
    </orbit-prompt>
    <orbit-prompt
        question="In the chain metaphor, how does **Simplicity** reduce the chance of failure?"
        answer="If a single link breaks in a chain, the whole chain breaks. Therefore, _minimize the number of necessary links in any chain._">
    </orbit-prompt>
    <orbit-prompt
        question="In the chain metaphor, how does **Diversity** reduce the chance of failure?"
        answer="If one chain breaks, it's good to have 'redundant' backups. Therefore, _maximize the number of independent chains_. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.)">
    </orbit-prompt>
    <orbit-prompt
        question="In the chain metaphor, how does **Adversity** reduce the chance of failure?"
        answer="Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to do **Simplicity** for Robust AI:"
        answer="(any of the following works) Regularization, Impact regularization, Auto-encoders (bottleneck for 'essence'), Speed/Simplicity Prior for Honest AI">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to do **Diversity** for Robust AI:"
        answer="(any of the following works) Ensembles, Dropout, Data Augmentation, Diverse Data, Kalman filters, Shard theory">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to do **Adversity** for Robust AI:"
        answer="(any of the following works) Adversarial Training, Relaxed / Latent Adversarial Training, Red Teams, Best Worst-Case Performance">
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="causality"></a>

## AI "Intuition": Thinking in Cause-and-Effect Gears

Imagine you give someone a pen & paper, and ask them to add a pair of 2-digit numbers. They do it perfectly. You ask them to add a pair of _3_-digit numbers. They do it perfectly. You give them pairs of 4, 5, 6, 7-digit numbers. They add all of them perfectly.

"Oh good", you think, "this person understands addition".

You give them a pair of 8-digit numbers. _They fail completely_. Not minor mistakes like forgetting to carry a one. _Complete, catastrophic failure_.

That is how the modern AI do.

. . .

It's _so hard_ to gain an intuition for "AI Intuition".

On one hand, LLMs have won gold at the International Math Olympiad,[^win-olympiad] passed the Turing Test[^win-turing-test], and humans _prefer AI over humans_ in "blind taste-tests" on poetry[^win-poetry], therapy[^win-therapy], and short fiction[^win-fiction].

On the other hand, these _same_ state-of-the-art LLMs can't run the business of a vending machine[^fail-vending], can't play Pok√©mon Red[^fail-pokemon], can't do simple ciphers[^fail-cipher], and can't solve simple "rule-discovery" games[^fail-arc-agi].

[^win-olympiad]: [Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad](https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) (2025)

[^win-turing-test]: [Large Language Models Pass the Turing Test](https://arxiv.org/pdf/2503.23674). _‚ÄúWhen prompted to adopt a humanlike persona, GPT-4.5 was judged to be the human 73% of the time‚Äù_ (vs 50% chance). Though to be fair, it was "only" a 5-minute long Turing Test, and the reason GPT-4.5 fooled humans so often was less because it was good, and more because Human Judges picked ineffective bot-detection strategies (see Figure 4), e.g. asking about daily activities & opinions, instead of strange questions or jailbreaks.

[^win-poetry]: [AI-generated poetry is indistinguishable from human-written poetry and is rated more favorably](https://www.nature.com/articles/s41598-024-76900-1). ‚Äúparticipants performed below chance levels in identifying AI-generated poems [...] more likely to judge AI-generated poems as human-authored than actual human-authored poems [...] AI-generated poems were rated more favorably.‚Äù
    
    The human poets included the greats like Shakespeare, Whitman & Plath. The AI was ChatGPT 3.5 _with no cherry-picking_; they just picked the first few poems generated "in the style of X".
    
    We are so cooked.

[^win-therapy]: [Hatch et al 2025](https://journals.plos.org/mentalhealth/article?id=10.1371/journal.pmen.0000145): ‚Äúa) participants could rarely tell the difference between responses written by ChatGPT and responses written by a therapist, b) the responses written by ChatGPT were generally rated higher in key psychotherapy principles‚Äù
    
    _Okay but REAL therapists can tell the difference_, you may protest. Well. From [Human-Human vs Human-AI Therapy: An Empirical Study](https://www.tandfonline.com/doi/pdf/10.1080/10447318.2024.2385001): _‚ÄúTherapists were accurate only 53.9% of the time, no better than chance, and rated the human-AI transcripts as higher quality on average.‚Äù_ 
    
    We are so, so cooked.

[^win-fiction]: Review of the contest, by Ozy Brennan: [AIs can beat human fiction writers because humans are bad at writing fiction: piss-on-the-poor reading comprehension](https://thingofthings.substack.com/p/on-the-ai-fiction-turing-test). Like the LLM-beats-Turing-Test study, this result (while impressive), is less "AI is really good" and more "humans are really bad". And these human writers weren't novices either, they're professionals who've published books.

    We are so, so, _so_ cooked.

[^fail-vending]: [Project Vend: Can Claude run a small shop?](https://www.anthropic.com/research/project-vend-1) Answer: no. Claude started stocking the food vending machine with tungsten cubes, and hallucinated that it was a flesh-and-blood delivery person who went the Simpsons' house. [Vending Bench 2](https://andonlabs.com/evals/vending-bench-2) measures how well all the frontier LLMs can do on this "run a vending machine for a year" task. As of writing (Dec 2025), they're at least making _positive_ amounts of money now, but a) the simulated "buyers" aren't _trying_ to jailbreak the LLM, the way real human buyers did in Project Vend, and b) the best AI in Vending Bench 2 is still making, like, 8% of the "good" baseline.

[^fail-pokemon]: [So how well is Claude playing Pok√©mon?](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon) _‚ÄúTL;DR: Pretty badly. Worse than a 6-year-old would.‚Äù_ Explanation why: ‚ÄúBasically, while Claude is pretty good at short-term reasoning (ex. Pok√©mon battles), he's bad at executive function and has a poor memory. This is despite [a great deal of scaffolding](https://excalidraw.com/#json=WrM9ViixPu2je5cVJZGCe,no_UoONhF6UxyMpTqltYkg), including a knowledge base, a critic Claude that helps it maintain its knowledge base, and a variety of tools to help it interact with the game more easily.‚Äù

[^fail-cipher]: [Embers of Autoregression (2024)] shows how GPT-4 could do the _common_ [ROT-13](https://en.wikipedia.org/wiki/ROT13) cipher, but utterly fail at the _uncommon_-but-trivially-similar ROT-12 cipher. As the [Supplementary Info shows (Figure S13)](https://www.pnas.org/action/downloadSupplement?doi=10.1073%2Fpnas.2322420121&file=pnas.2322420121.sapp.pdf), this holds even with "chain of thought" reasoning turned on. Though to be fair, this paper was from 2024, and I tried ROT-12 and ROT-11 Claude Sonnet 4.5 just now, and it did fine. Though as we'll see later in this section, Claude & LLMs chains-of-thought are _still_ very fragile.

[^fail-arc-agi]: [ARC-AGI](https://arcprize.org/) is a bunch of pixel puzzle games. Instead of most games where you're told the rules up front, in ARC-AGI, _you have to learn the hidden rules via exploration,_ then prove you understand it by applying the rules to win the game.
    
    [See the Leaderboard Breakdown](https://arcprize.org/leaderboard): The Ten-Human Panel's performance on ARC-AGI-1 and -2 is 98% and 100%, costing \\$17/task (~10 workers hired per task on Mechanical Turk, success if _at least one_ succeeds).
    
    The best AI (Gemini 3 Deep Think) on ARC-AGI-1 and -2 is 87.5% (a bit worse) and 45.1% (_much_ worse) costing \\$77/task (almost _five times_ more expensive than hiring _ten humans on MTurk_)

And now, this year, there's a new paper from Apple: [_The Illusion of Thinking_, by Shojaee & Mirzadeh et al](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf). It was contested & controversial ‚Äî and we'll address the critiques later ‚Äî but I think, combined with the above strange failure-cases for modern LLMs, the overall conclusion still holds, or is at least highly plausible. **It's important not to over-update on one study, but I think this paper is a great illustration of the problem.**

Anyway, the study. Here is the child's puzzle game, [Tower of Hanoi](https://en.wikipedia.org/wiki/Tower_of_Hanoi), so named because a 1800's Frenchman thought they look like Vietnamese pagodas, I guess:[^hanoi-pic-src]

[^hanoi-pic-src]: [Photo by User:Evanherk](https://en.wikipedia.org/wiki/File:Tower_of_Hanoi.jpeg)

![Photo of three pegs, with a stack of 8 disks on the left-most peg, stacked biggest-to-smallest](../media/p3/gears/hanoi.jpg)

The goal is to move the entire stack from the left-most peg, to the right-most peg. The rules are: 1) you can only move one disk at a time, from one peg to another, and 2) _you cannot put a larger disk over a smaller disk_.

üïπÔ∏è **[(If you'd like to play the game yourself before continuing, click here!)](https://www.mathsisfun.com/games/towerofhanoi.html)** üïπÔ∏è

Here's how this game may go for a Human:[^human-hanoi]

- Starting at 3 disks, you fumble a lot, and eventually brute-force your way through.
- Then at 4 disks, it's too complicated for brute force, but you're noticing patterns from when you solved it at 3 disks, and that helps you to the end.
- Then at 5 disks, ü§Ø _EUREKA!_ ü§Ø, you figured out the core epiphany! (See GIF below for visualization.[^hanoi-gif-src]) _To move a 5-stack, you need to move a 4-stack, so you need to move a 3-stack, so move a 2-stack, so move a 1-stack, which is trivial._ Now, not only can you solve ANY level of the Tower of Hanoi, you can even calculate the exact number of moves you need![^hanoi-math] Satisfied with your big brain, you execute the pattern, make mistakes, correct your mistakes, and eventually succeed.
- Then at 6 disks, you execute the pattern with no mistakes.
- Then at 7+ disks, it's obvious, routine & tedious, even.

[^hanoi-gif-src]: [Created by User:Trixx](https://en.wikipedia.org/wiki/File:Iterative_algorithm_solving_a_6_disks_Tower_of_Hanoi.gif)

![GIF of solution to a 6-disk Hanoi](../media/p3/gears/hanoi_solution.gif)

[^human-hanoi]: Actually, if you're interested in the developmental psychology of kids playing Tower of Hanoi, here's the landmark paper by [Byrnes & Spitz 1979](https://link.springer.com/content/pdf/10.3758/BF03329485.pdf). See Figure 1: around age 8, kids have near-perfect performance on the 2-stack version. Around age 14, kids plateau out at okay performance on the 3-stack. Unfortunately I couldn't find any paper that gives performance scores for 4+ disks in general-population children/adults. Sorry.

[^hanoi-math]: The solution to Level N involves doing the solution to the previous level _twice_ (moving the N-1 stack from Peg A to Peg B, then Peg B to Peg C), plus one extra move (moving the biggest disk from Peg A to Peg C).
    
    So how many moves do you need to solve N disks? Let's crunch the numbers:
    
    For 1-disk, **1 move**
    
    For 2-disk, 1 * 2 + 1 = **3 moves** (previous solution _twice_ + one extra move)

    For 3-disk, 3 * 2 + 1 = **7 moves**

    For 4-disk, 7 * 2 + 1 = **15 moves**

    For 5-disk, 15 * 2 + 1 = **31 moves**

    For 6-disk, 31 * 2 + 1 = **63 moves**

    For 7-disk, 63 * 2 + 1 = **127 moves**

    For 8-disk, 127 * 2 + 1 = **255 moves**

    Do you see the pattern?

    **For N disks, you need 2^N - 1 moves!**

    ([Proof by induction](https://en.wikipedia.org/wiki/Mathematical_induction): Let F(N) = # of moves needed for N disks. Let's say we already know F(N) = 2^N - 1. Then doubling that & adding one, we get (2^N - 1)*2 + 1 = 2^(N+1) - 2 + 1 = 2^(N+1) - 1, which is the F(N+1) we want! All's left to do is prove the base case: F(1) = 2^1 - 1 = 2 - 1 = 1. And indeed, yes it takes 1 move to solve the 1-stack. Q.E.D.)

    Yippee! This was really not worth writing the footnote for, but there you go.

Point is: if a Human can successfully solve Tower of Hanoi for 7 disks, they obviously have the pattern down. You would expect they can solve 8 or more disks, tediousness & minor mistakes aside.

What you would _NOT_ expect is this:

![Graph of Claude-with-thinking vs the Hanoi puzzle. Good performance up to 7 disks, then _utterly_ collapses](../media/p3/gears/hanoi-performance.png)

With "chain-of-thought reasoning" turned on: near-perfection from Disks 1 to 5, still pretty good at Disk 7, then _complete collapse_ after Disks 8 and up. (This graph skips Disk 6 for some reason. The full data shows that 6-Disk's performance was slightly _worse_ than 7-Disk's. Probably noise.) 

And it wasn't just Tower of Hanoi, or just the Claude LLM. Across 3 other child's puzzle games, across ChatGPT & DeepSeek, "reasoning mode" LLMs do great, well past the point a Human would've figured out the general solution... then it _completely fails_.

Again, not "minor mistakes" like a Human would. _Collapse_.

That's like being able to add two 7-digit numbers on pen & paper, then _utterly bomb_ on two 8-digit numbers.

("Okay but won't it get better with more scale & training?" you may ask. Sure. But if a Human adds two 7-digit numbers perfectly but fails at 8, then promises you "okay but if you give me more training I can do 8, too!" that is not encouraging. They're clearly not "getting it".)

. . .

What the heck is happening?

An AI skeptic might say, "See! LLMs are just stochastic parrots.[^parrot] They just copy what they've seen in the trillions of documents they've been trained on, and fail in never-before-seen scenarios."

[^parrot]: Famous phrase coined by [Emily Bender et al 2021](https://dl.acm.org/doi/10.1145/3442188.3445922)

I don't think that's quite right ‚Äî I doubt there was _any_ web document writing out the full solution to the 7-disk, but not 8-disk and up. Nor do I think there was any document with ["instructions on how to remove a peanut butter sandwich from a VCR, in the style of the King James Bible"](https://x.com/tqbf/status/1598513757805858820), yet early-ChatGPT delivered _perfectly_ in this never-before-seen scenario. So, LLMs _do_ generalize, at least a little bit.

Here's my guess, and I'm far from the first[^pearl-quote] to make this guess:

**Modern AIs "think in vibes". They do not "think in gears".**[^gears]

<u>Thinking in vibes</u>: Can discover & use patterns, but only shallow ones. Correlations, not causation. When it generalizes, it's by _induction_[^induction]. Similar to System 1 Intuition.[^sys-1-2] 

<u>Thinking in gears</u>: Can discover & use _robust mental models_, deep ones. Causation, not just correlation. When it generalizes, it's by _deduction_. Similar to System 2 Logic.

It's not "gears good, vibes bad". You need _both_ to reach typical human-level intelligence, let alone a superhuman Scientist AI.

Now, Good Ol' Fashioned AI (GOFAI) _did_ use to "think in gears", but they were extremely narrow. A chess AI could only play chess, nothing more. Meanwhile, to be fair, modern LLMs are extremely flexible: they can roleplay everything from a poet to writer to therapist (who, again, _humans prefer over humans_), and even roleplay as‚Ä¶ someone who can solve the 7-disk Tower of Hanoi.

But not 8.

![Two-axis graph of System 1 ("intuitive") vs System 2 (logical) thinking. Good Ol' Fashioned AI had high System 2, low System 1. Modern deep-learning AI has high System 1, low System 2](../media/p1/sys1vs2_B.png)

Good Ol' Fashioned AI (GOFAI): Robust but inflexible.

Modern AI: Flexible, but not robust.

**As of writing (Dec 2025), we do not know how to make an AI that do both: _flexibly_ discovering & using _robust models_.** To be able to switch between vibes & gears _at will, fluently_. The deduction of logic + the induction of intuition = the _abduction_ of science.[^abduction] It's not just interpolating & extrapolating data, it's _hyper_-polating data: stepping out of the data's flatland, into a new dimension.[^hyperpolation]

Is that a bunch o' vague jargon? Yes. Ironically, I only have a vibe-based understanding of gears. I don't have a rigorous mental model of _rigorous mental models_. I'm not sure anyone does. If we did, we'd probably have artificial general intelligence (AGI) by now.

[^induction]: [Inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning) is stuff like "the Sun has risen every day for the last 10,000 days I've been alive, therefore the Sun will almost definitely rise again tomorrow". It's not _technically_ a logical deduction ‚Äî it's _logically_ possible the Sun could vanish tomorrow ‚Äî but, statistically, yeah it's probably fine.

[^abduction]: [Abductive reasoning](https://en.wikipedia.org/wiki/Abductive_reasoning) is the backbone of science. It's when you generate "the most likely hypothesis" to novel data. It's not _deduction;_ it's very logically possible your hypothesis could be false. And it's not _induction;_ you haven't seen any direct repeated evidence for your hypothesis _yet_, not until you test it.
    
    (Example: [Michelson-Morley](https://en.wikipedia.org/wiki/Michelson%E2%80%93Morley_experiment) discovered that the speed of light seemed to be constant in every direction, and from this, Einstein _abducted_ that time is relative! Yes, "time is relative" was the _most_ likely hypotheis he had for "light speed seems to be constant", and hey, he was right.)
    
    (Nitpick: Einstein claimed he didn't know about the Michelson-Morley experiment, and he instead abducted relativity from the Maxwell Equations, but look, I'm trying to tell a simple story here ok?)

[^gears]: Beloved "thinking in gears" metaphor comes from [Valentine (2017)](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding)

[^sys-1-2]: The ‚Äúdual-process‚Äù model of cognition was first suggested by [\(Wason & Evans, 1974\)](https://pages.ucsd.edu/~scoulson/203/wason-evans.pdf), developed by multiple folks over decades, and*really* popular in Daniel Kahneman's bestselling 2011 book, [Thinking, Fast & Slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow). As for the naming: Intuition is #1 and Logic is #2, because pattern-recognition evolved before human-style deliberative logic.

[^hyperpolation]: "Hyperpolation" coined by [Toby Ord 2024](https://arxiv.org/pdf/2409.05513).

[^pearl-quote]: As Judea Pearl ‚Äî winner of the Turing Prize, the "Nobel Prize of Computer Science" ‚Äî once told Quanta Magazine in their article, [To Build Truly Intelligent Machines, Teach Them Cause and Effect](https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/): _‚ÄúAs much as I look into what‚Äôs being done with deep learning, I see they‚Äôre all stuck there on the level of associations. Curve fitting. [...] no matter how skillfully you manipulate the data and what you read into the data when you manipulate it, it‚Äôs still a curve-fitting exercise, albeit complex and nontrivial.‚Äù_

. . .

Things get worse.

From the conclusion of Apple's _Illusion of Thinking_ paper:

> \[Claude 3.7 + Thinking\] also achieves near-perfect accuracy when solving the Tower of Hanoi with (N=5), which requires 31 moves, while it fails to solve [the River Crossing puzzle](https://en.wikipedia.org/wiki/Missionaries_and_cannibals_problem) when (N=3), which has a solution of 11 moves. **This likely suggests that examples of River Crossing with N>2 are scarce on the web**, meaning [Large Language Models with Reasoning\] **may not have frequently encountered or memorized such instances during training.**
>
> _(emphasis added)_

So the problem's not the _length_ of the reasoning, it's _how common_ is the reasoning. This parallels the findings from [the 2024 paper, Embers of Autoregression](https://www.pnas.org/doi/pdf/10.1073/pnas.2322420121):

> We identify three factors that we hypothesize will influence LLM accuracy:
> - the probability of the task to be performed,
> - the probability of the target output, and
> - the probability of the provided input.

Where "probability" ~= "how common is it in the training data".

Mystery (partly) solved! _That's_ why LLMs rock at human conversation, but suck at rule-discovery mini-games. And long tasks are "less probable" than short tasks, because homework problems & textbook examples are almost always a page or two, not dozens of pages. (This, I suspect, is why Claude sucked at running a vending machine,[^fail-vending] even though there's lots of writing on how to run a business: when business textbooks give examples, they list a few transactions, not _thousands_.[^synthetic-data])

[^synthetic-data]: Maybe this can be solved by writing a _normal_ program, that then writes you "synthetic data" for a business operating successfully over 365 days, that you can fine-tune an LLM on? I dunno, this still feels like the special pleading of "yes I can add 7 digits & bomb at 8, but if you give me more training I can do it!" If you pass 7 but bomb 8, there's something fundamentally wrong that "more scale" don't fix.

This also explains why AI's success at _common_ code tasks is exponential, with the length-of-task doubling every 7 months at a steady cost[^metr-programming]... while AI's performance at games of _uncommon_ rule-discovery have a _worse than exponential_ cost:[^arc-agi-frontpage]

[^metr-programming]: [METR (2025)](https://arxiv.org/pdf/2503.14499). ‚Äú[software engineering] tasks that AI models can complete with 50% success rate [are] doubling approximately every seven months since 2019.‚Äù And Figure 13 shows that the cost of completing these tasks remains pretty stable even as task length grows, around 1% of a "Level 4" Google salary (\\$143.61/hour, so, \\$1.43/hour for an AI.) However, it's important to note the coding tasks in the benchmark are, by design, common tasks.

[^arc-agi-frontpage]: [From the ARC-AGI frontpage.](https://arcprize.org/) It's missing the newest Gemini, [but it doesn't push the frontier much](https://arcprize.org/leaderboard), and the frontier still bends downwards.

![Graph from ARC-AGI. Description below](../media/p3/gears/arc-agi.png)

The above graph shows various AIs' performance vs cost on ARC-AGI, a series of games where you have to _discover_ the games' rules. Notice it's the _x-axis_ that's exponential (\\$1, \\$10, \\$100), _not the y-axis, which 'exponential trend' graphs are supposed to be_. So a _straight_ line on this graph means that performance-for-cost is getting exponentially _worse_, not better. And the above chart's "frontier" isn't a straight line, it's bending _downwards_, meaning LLMs' performance-for-cost is _worse_ than exponential. ([beware AI labs disguising their crappy progress with exponential _x_-axes!](https://www.tobyord.com/writing/inference-scaling-and-the-log-x-chart)) Even Rich Sutton, famous for his "bitter lesson" that you can just add scale to AI and they'll do better, thinks "LLMs are a dead end".[^sutton]

(Also, as for why LLMs seem to be hitting the maximum performance at most other benchmarks that's _not_ ARC-AGI? To be honest, the developers may be (unintentionally?) cheating and training their LLMs on the correct answers. It's like a student studying for a test by stealing the answers _and memorizing them_. We know this is happening, because frontier LLMs can produce the "canary strings" that are included with the answers' data, which implies the LLMs training data _included the answers_. At the very least, the companies didn't do data filtering to make sure that benchmarks' answers didn't end up in their giant web-scraped data dumps.[^canary])

[^canary]: In 2020, [niplav](https://www.lesswrong.com/posts/QZM6pErzL7JwE3pkv/shortplav) found that Claude Sonnet 3.5 could reproduce a common benchmark's "canary string". In 2024, [Jozdien](https://www.alignmentforum.org/posts/kSmHMoaLKGcGgyWzs/big-bench-canary-contamination-in-gpt-4) found that GPT-4 (base model) can too. The reason this is bad is because it's trivially easy to filter out all documents with the canary string ‚Äî just search if it's in a document, then leave out the document if it is. The canary string _itself_ isn't a cheat, but it hints at a high probability that the training data was contaminated with the benchmark's answers, making LLMs' performance on those benchmarks untrustworthy.

[^sutton]: From Sutton's famous [2019 essay The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html): ‚Äú_One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great._‚Äù But in [a 2025 podcast with Dwarkesh Patel](https://www.youtube.com/watch?v=21EYKqUsPfg), he believes LLMs are "a dead end", because they imitate without building "robust world models".

. . .

Okay, _now_ let's quickly address the two main criticisms of the controversial Apple paper:

1) The LLM's "context window" (its "thinking space") wasn't big enough to generate the full solutions. This _is_ true for the 12+ disk and up, but not for 8-disk. And the River Crossing solutions were _smaller_ than the 5-disk solutions, yet the LLMs failed anyway. So, size of the "thinking space" wasn't the problem.
2) These LLMs can write _a computer program that solves Hanoi_, then can pass that to a tool that runs code. LLM + program synthesis _is_ a promising solution to "thinking in vibes + gears", and in fact is how the top contenders at ARC-AGI did it. But: this is like someone who can't add with pen & paper, but _can_ pull up a calculator and add two numbers. It shows a lack of deeper understanding. (And in this case, Hanoi is so famous, of _course_ code solutions are in the training data.)

Again, it's important not to over-update on _one_ study ‚Äî but, I think, _combined_ with the results from all of where LLMs fail at (and are great at), it's strong evidence of this hypothesis:

**Common task** ‚Üí LLMs get exponentially better over time

**Uncommon task** ‚Üí <img src="../media/p3/gears/cheems.png" alt="crying meme dog" class="inline-icon">

. . .

To be clear, a mere "common task auto-completer" can _still_ have huge impact, for better _and_ worse. In blind tests, AI therapists are preferred over human therapists, by both human clients _and human therapists themselves_.[^win-therapy] AI Therapy could finally make mental healthcare accessible to all, and/or, make humans emotionally dependent on corporate-owned bots. AIs are already superhuman at political persuasion, for good causes & bad.[^super-persuasion] And I expect _my_ main job, "web developer", to be fully automated away in 5 years by mere "common-task auto-completers". (This is why, in 2026, I want to pivot to becoming a researcher. Because the day _science itself_ is automated‚Ä¶ well‚Ä¶ either way that shakes out, I won't have to figure out how to pay rent anymore.)

[^super-persuasion]: From [Hackenburg & Margetts 2024](https://www.pnas.org/doi/pdf/10.1073/pnas.2403116121): ‚ÄúEarly research suggests that most capable LLMs can directly persuade humans on political issues (5), draft more persuasive public communications than actual government agencies and political communication experts (6, 7), and generate convincing disinformation and fake news (1, 8, 9). Across these domains, humans are no longer able to consistently distinguish human and LLM-generated texts (8, 10).‚Äù

. . .

<img class="mini" alt="sticky note saying 'IOU Actual Solutions sorry'" src="../media/p3/gears/iou.png">

Okay, this is the least satisfying section of this post, because there's not much in the way of solutions, because this problem ‚Äî make AI that can think in both System 1 vibes & System 2 gears ‚Äî is possibly _equivalent_ to creating Artificial General Intelligence (AGI).

That said, there _are_ some promising early research directions, to get AIs to "think in robust mental models":

- [Neuro-Symbolic AI](https://www.youtube.com/watch?v=GeN5XVA2e4w), where modern "neural networks" interact tightly with Good Ol' Fashioned AI (GOFAI) modules. Some succesful examples include AlphaFold & AlphaGeometry.
- [Code interpreters](https://garymarcus.substack.com/p/how-o3-and-grok-4-accidentally-vindicated) that let LLMs _make & run_ GOFAIs as they go. See also: [program search & program synthesis](https://arcprize.org/blog/beat-arc-agi-deep-learning-and-program-synthesis).
- Training ANNs to [infer cause-and-effect diagrams directly from data](https://www.microsoft.com/en-us/research/publication/deep-end-to-end-causal-inference-2/).
- [Theory-Based Reinforcement Learning](https://arxiv.org/pdf/2107.12544), which combines "rich, abstract, causal models" with modern AI training techniques
- (or maybe the entire paradigm of modern AI sucks, and we need to re-start from scratch)

But in any case, here's all the amazing things we _would_ get from AI that thinks in cause-and-effect gears, not just correlational vibes:

* ‚ò∏Ô∏è <u>Interpretability & Steering</u>: It's easier for us to understand an AI if it stores its knowledge as "this causes that". It also makes an AI easier to steer: change "this" to change "that".
* ü§ù <u>Trustworthiness & Accountability</u>: By learning a cause-and-effect model of the world, when we ask an AI _why_ it did something, it can truthfully report why. (In contrast, LLMs [will make reasons up](https://arxiv.org/abs/2503.08679) for why they say/choose things.)
* üí™ <u>Robustness</u>: [Tumor-detecting AI looks for _rulers_ in medical scans, because they _correlate_ with malignant tumors](https://www.sciencedirect.com/science/article/pii/S0022202X18322930). Thinking like an actual scientist ‚Äî observe data, generate _cause-and-effect_ hypotheses, test them, repeat ‚Äî will help AIs generate good models of the world.
    * Besides, [Richens et al 2025](https://arxiv.org/pdf/2506.01622) proves that any generally-capable agent _needs_ to have cause-and-effect models of the world.
* üíñ <u>Learning our Values:</u> Understanding causality lets AI distinguish between things we want *for its own sake*, vs things we want *for something else*. ("intrinsic" vs "instrumental" goals) For example, an AI should understand we want money, but to buy helpful things, *not* for its own sake.
    * [The "inner misalignment" problem](https://aisafety.dance/p2/#problem6) ‚Äî the problem of AIs learning the wrong values _even with perfectly-specified rewards_ ‚Äî can be reframed as a problem of _goal_ robustness, or correlation-causation mixups. For example, a videogame AI that's rewarded for getting the coin at the end of a level learns to _love going to the right for its own sake_, and ignore the coin. [A 2023 paper](https://arxiv.org/pdf/2309.16166) finds that making the AI "think like a scientist" ‚Äî generate & test multiple hypotheses for what _causes_ reward, not just correlates with it ‚Äî helps that AI solve goal robustness.
* üîÆ <u>The "Future Lives" Algorithm:</u> Causal models would help an AI get better at predicting the world in different what-if ("counterfactual") scenarios, *and* predicting what we'd approve of.
* ü§• <u>Getting the truth, not a human-imitator</u> (or: ["Eliciting Latent Knowledge"](https://www.lesswrong.com/w/eliciting-latent-knowledge), ["non-agentic Scientist AIs"](https://arxiv.org/abs/2502.15657)) So you've trained an AI on data collected by expert scientists. How do you get *just the truth* out of this AI, not "truth + human biases"? If the AI's knowledge is distilled into interpretable cause-and-effect gears, you could just take the gears predicting how the world works, and _remove_ the gears predicting how biased human experts report it, to get an _unbiased_ true model of how the world works!
* ü§ì <u>Causal Incentives</u>: Bonus ‚Äì not only would it be better to _make_ AI that "natively thinks" in causal diagrams, we can use causal diagrams to think better about AI! The [Causal Incentives Working Group](https://causalincentives.com/) uses cause-and-effect diagrams to predict if/when an AI has an incentive to cheat, modify itself, or modify _the human_.

In a weird way, maybe I should be grateful that AI sucks at fluidly discovering & using world-models. Because if they _could_ do that, then AI would already be capable of, say, taking over the world.

But they don't. Which gives us more time to make sure we stay above the dotted line in this graph, where Capabilities < Alignment:

![Diagram of above idea. Two-axis graph: Alignment vs Capabilities, where the diagonal dotted line is the boundary between Alignment > Capabilities and Alignment < Capabilities. By tying Alignment TO Capabilities, we stay above the safe dotted line.](../media/p3/learn/rocket.png)

And, if the "treat learning our values as a standard machine-learning problem" approach works, we _tie_ Alignment to Capabilities. When AI can robustly learn about the world, they can robustly learn about _our values_. So: Capabilities becomes _the new floor_ for Alignment, and we stay above the dotted line.

(But again, don't relax _too_ much.)

---

### ü§î Review #6

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="List at least 1 surprising thing **LLMs are great at** (as of December 2025)"
        answer="(any of the following works:) International Math Olympiad, passed the Turing Test, humans _prefer AI over humans_ in poetry, therapy & short fiction">
    </orbit-prompt>
    <orbit-prompt
        question="List at least 1 surprising thing **LLMs still suck at** (as of December 2025)"
        answer="(any of the following works:) running the business of a vending machine, can't play Pok√©mon Red, can't solve simple 'rule-discovery' games, can't generalize on Tower of Hanoi or other classic puzzles.">
    </orbit-prompt>
    <orbit-prompt
        question="As of 2025, how do the frontier LLMs perform on the Tower of Hanoi problem?"
        answer="Really well, up to 7 disks... then _completely collapses_ at 8+ disks. (This is like a human being able to add two 7-digit numbers on pen & paper, then _utterly bomb_ on two 8-digit numbers. Very strange.)"
        answer-attachments='https://aisafety.dance/media/p3/gears/hanoi.jpg'>
        <!-- aisffs-hanoi.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Modern AIs think in \_\_\_\_. They do not think in \_\_\_\_."
        answer="They 'think in vibes'. They do not 'think in gears'. (To be precise: they do not form & use _robust mental models_.)">
    </orbit-prompt>
    <orbit-prompt
        question="Good Ol' Fashioned AI (GOFAI): \_\_\_\_\_ but \_\_\_\_\_ Modern AI: \_\_\_\_\_, but \_\_\_\_\_."
        answer="Good Ol' Fashioned AI (GOFAI): Robust but inflexible. Modern AI: Flexible, but not robust. _(note: as of Dec 2025, we don't know how to make an AI that can do both: flexible discover & use robust mental models.)_">
    </orbit-prompt>
    <orbit-prompt
        question="According to the Embers of Autoregression paper, what are 3 factors that predict when an LLM will rock/suck at a task?"
        answer="The probability of 1] the task, 2] the target output, and 3] the provided input. (Where 'probability' ~= 'how common was it in the training data'.)">
    </orbit-prompt>
    <orbit-prompt
        question="What's 1 task LLMs are getting exponentially better at, and 1 where they're exponentially bad at?"
        answer="Common coding tasks (length of task doubling every 7 months), vs the ARC-AGI 'rule-discovery' games (cost scales worse-than-exponentially)"
        answer-attachments='https://aisafety.dance/media/p3/gears/arc-agi.png'>
        <!-- aisffs-arc-agi-cost.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 research direction that's (trying) to make AI both be flexible _and_ have robust mental models:"
        answer="(any of the following works:) Neuro-Symbolic AIs, Hybrid AIs, ANNs that infer causal diagrams, code interpreters, program search & program synthesis, Theory-Based Reinforcement Learning">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 2 benefits from having AI being able to think in cause-and-effect gears: (other than better capabilties)"
        answer="(any 2 of the following works:) Interpretability & Steering, Trustworthiness & Accountability, Robustness, Learning our Values, The 'Future Lives' Algorithm, Getting the truth not just a human-imitator, Non-agentic Scientist AIs, Causal Incentives">
    </orbit-prompt>
</orbit-reviewarea>

---

## üéâ RECAP #2

- üß† **Read & write to an AI's "brain" with Interpretability & Steering.** (The AI equivalent of brain scans, and brain stimulation)
- üí™ Across engineering, life, and AI, you can make anything more robust, with **Simplicity, Diversity, and Adversity.**
- ‚öôÔ∏è **Modern AIs are fragile because they think in correlational "vibes", not cause-and-effect "gears"**. This is why they're getting exponentially better at common tasks, while being exponentially bad at _uncommon_ tasks.
    - Making AIs think in cause-and-effect would not only increase AI Capabilities, but also: make them more robust, verifiable, interpretable, steerable, scientifically useful, and better at learning & aligning itself to our values.

---

<a id="humane"></a>

## What are 'Humane Values', anyway?

Congratulations, you've created an AI that robustly learns & follows the values of its human user! The user is an omnicidal maniac. They use the AI to help them design a human rabies in a stable aerosolized form, spray it everywhere via quadcopters, and create the zombie apocalypse.

Oops.

I keep harping on this and I'll do it again: **_human_ values are not necessarily _humane_ values.** C'mon, people used to burn cats alive for entertainment.[^cats] Even after solving the problem of "how do we steer advanced AI at all", we need to decide: "steer towards _which goals, which values?_"

![Left: rocket being built, labeled, "technical alignment: how to robustly aim AI at any target at all". Right: the moon, labeled, "whose values: what should be our target?"](../media/p2/ethics/aim_vs_target.png)

[^cats]: [Cat Burning on Wikipedia](https://en.wikipedia.org/wiki/Cat_burning). Reeeaaally glad there's no photos on that page. (The Wikipedia Talk page notes there's academic debate about how widespread _this specific_ act really was, but there's plenty of examples in History of "normal people" being awful.)

So, if we want AI to go *well* for humanity (and/or all sentient beings), we need to just... uh... solve the 3000+ year old philosophical problem of what morality is. (Or if morality doesn't objectively exist, then: "what rules for living together would any community of rational beings converge on?") 

Hm.

Tough problem.

Well actually, as we saw earlier ‚Äî (with Scalable oversight, Recursive self-improvement, and Future Lives + Learn Our Values agents) ‚Äî as long as we start with a solution that's "good enough", that has *critical mass*, it can self-improve to be better and better!

Besides, that's what we humans have *had* to do all this time: a flawed society comes up with rules of ethics, notices they don't live up to their own standards, improves themselves, which lets them notice more flaws, improve, repeat.

So, as an attempt at "critical mass", here's some concrete proposals for a good-enough first draft of ethics for AI:

(And note: these proposals are _not_ mutually exclusive! We don't need One Perfect Solution, we can stack multiple imperfect solutions.)

üìú **Constitutional AI:**

Step 1: Humans write a list of principles, like "be honest, helpful, harmless".

Step 2: A teacher-bot uses _that_ list to train a student-bot! Every time a student bot gives a response, the teacher gives feedback based on the list: "Is this response honest?", "Is this response helpful?", etc.

That's how you can get the *millions* of training data-points needed, from a small human-made list!

Anthropic is the pioneer behind this technique, and they're already using it successfully for their chatbot, Claude. Their first constitution was inspired by many sources, including the UN Declaration of Human Rights.[^const-ai] Too elitist, not democratic enough? Well, next they crowdsourced suggestions to improve their constitution, which led them to add "be supportive/sensitive to folks with disabilities" and "be balanced & steelman all sides in arguments"![^const-ai-2]

[^const-ai]: [Constitutional AI: Harmlessness from AI Feedback](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback) (2022)

[^const-ai-2]: [Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input) (2023)

This is the most straightforward way to put humanity's wide range of values into a bot. (and _actually_ deployed in a major LLM product!)

üèõÔ∏è **Moral Parliament:**

This idea combines the Uncertainty & Diversity from the previous sections. [Moral Parliament](https://ora.ox.ac.uk/objects/uuid:b6b3bc2e-ba48-41d2-af7e-83f07c1fe141/files/svm40xs90j) proposes using a "parliament" of moral theories, with more seats for theories you're more confident in. (For example: my parliament may give [Capability Approach](https://en.wikipedia.org/wiki/Capability_approach) 50 seats, [Eudaimonistic Utilitarianism](https://utilitarianism.net/theories-of-wellbeing/#objective-list-theories) 30 seats, other misc theories get 20 seats.) The Moral Parliament then votes Yay or Nay on possible actions. The action with the most votes, wins.

(This proposal is pretty similar to Constitutional AI, except instead of adjectives like "honest" & "helpful", the voters are entire moral theories. And instead of an equal vote, you can put more weight on some theories vs others.)

As we learnt earlier, adding Diversity is a good way to increase Robustness. Because _every_ moral theory has some weird edge case where it fails, having a diverse moral parliament prevents a single point of failure. (example:[^parliament-example])

The above paper was meant for Humans, but could be implemented in AIs, too.

[^parliament-example]: Deontology says you should be honest with a Nazi who wants to find hidden Jews, Utilitarianism says you should torture one person to prevent 10<sup>100</sup> people getting dust in their eyes, and Rational Egoism would be okay with you walking past an easily-savable drowning child.
    
    But I struggle to think of _any_ scenario that fails at _all three_ moral theories: an action that obeys widely-accepted rules & duties, improves others' well-being, _and_ your own well-being, and yet is _still "wrong"_. Point is: the _ensemble_ is more robust than any individual theory alone.

    (What about Virtue Ethics? Is there a scenario where "be wise" fails? Well, no, but that's because virtue ethics is extremely vague & kinda useless for guidance. The big names in virtue ethics, Aristotle & Aquinas, supported slavery. You can promote/demote any action you want be using the sloppy labels of "wise"/"unwise".)

üç∫ **Use AIs to distill & amplify human values:**

Researchers at Google DeepMind have found that a fine-tuned LLM can find [create consensus among humans with diverse values](https://arxiv.org/pdf/2211.15006). Better yet, these AI-aided consensus ideas were preferred _over the humans' opinions themselves_. (though, note: the humans weren't necessarily writing for consensus.)

Don't want to rely on fragile LLM technology specifically? Here's a proposal to do ["AI as the engine, humans as the steering wheel"](https://vitalik.eth.limo/general/2025/02/28/aihumans.html) _in general_, for any AI tech:

- Ask a human jury to answer 1,000 numerical or multiple-choice questions. (1,000 is just an example)
- Publicly release their answers for the first 100 questions, the other 900 are kept secret.
- People can use the public data to train human-value-predicting AIs. (the AI can be _any_ design, not just LLM-based.) They submit these AIs to a contest.
- The AIs that _best_ predict the distribution of human answers on the secret 900 questions, must be good at predicting human values in general! These AIs have "distilled" human values. (Note: Predict the _distribution_ of answers, not just the average answer. This way the AI can also predict how polarizing something would be.)
- You can now use an _ensemble_ of the best AIs (Why ensemble? Because Diversity ‚Üí Robustness, remember?) as an Oracle for "does this fit humanity's values", to make decisions _not_ in the original 1,000-question set.

üíñ **Learning from diverse sources of human values**:[^learn-from-stories]

Give an AI our stories, our fables, philosophical tracts, religious texts, government constitutions, non-profit mission statements, anthropological records, *all of it*... then let good ol' fashioned machine learning extract out our most robust, universal human values.

[^learn-from-stories]: [Using Stories to Teach Human Values to Artificial Agents](https://cdn.aaai.org/ocs/ws/ws0209/12624-57414-1-PB.pdf) by Riedl & Harrison 2016.

(But every human culture has greed, murder, etc. Might this not lock us into the worst parts of our nature? See next proposal...)

üåü **Coherent Extrapolated Volition (CEV):**[^cev-yud]

[^cev-yud]: [Eliezer Yudkowsky 2004](https://intelligence.org/files/CEV.pdf)

*Volition* means "what we wish for".

*Extrapolated* Volition means "what we *would* wish for, if we were the kind of people we wished we were (wiser, kinder, grew up together further)".

*Coherent* Extrapolated Volition means the wishes that, say, 95+% of us would agree on after hundreds of rounds of reflection & discussion. For example: I don't expect every wise person to converge on liking the same foods/music, but I *would* expect every wise person to at least converge on "don't murder innocents for fun". Therefore: CEV gives us freedom on tastes/aesthetics, but not "ethics".

CEV is different from the above proposals, because it does *not* propose any specific ethical rules to follow. Instead, it proposes a *process* to improve our ethics. (Reminder: this is called "indirect normativity"[^ind-norm]) This is similar to the strength of "the scientific method"[^sci-method]: it does *not* propose specific things to believe, but proposes a specific *process* to follow.

[^ind-norm]: "Normativity" ~= morality, "Indirect" = well, not direct. See this [glossary entry](https://forum.effectivealtruism.org/topics/indirect-normativity).

[^sci-method]: Okay there's technically no such thing as "the" scientific method, every field does something slightly different, and methods evolve, but you know what I mean. There's a "family resemblance". The general process of "notice stuff, guess why it's happening, test your guesses, repeat".

I like CEV, because it basically describes the *best-case scenario* for humanity without AI ‚Äî a world where everyone rigorously reflects on what is the Good ‚Äî and then sets that as *the bare minimum* for an advanced AI. So, an advanced aligned AI that follows CEV may not be perfect, but *at worst* it'd be us *at our best*.

One problem with CEV is that "simulate 8+ billion people debating for 100 years" is impossible in practice ‚Äî but ‚Äî we can approximate it! For example: use a few hundred LLMs that have been trained & validated to represent a demographic, then have _them_ debate.[^cev-leike] This is similar to the way democracies have representatives & activists that debate/vote on behalf of the people they represent. (And yes, LLMs _can_ accurately simulate individuals' beliefs & personalities![^sim-1000])

[^cev-leike]: [Jan Leike 2023](https://aligned.substack.com/p/a-proposal-for-importing-societys-values): A proposal for importing society‚Äôs values: Building towards Coherent Extrapolated Volition with language models.

[^sim-1000]: [Park et al 2024](https://arxiv.org/pdf/2411.10109): "We [simulate] the attitudes and behaviors of
1,052 real individuals [...] The [AI clones] replicate participants' responses on the General Social Survey 85% as accurately _as participants replicate their own answers two weeks later_, and perform comparably in predicting personality traits and outcomes in experimental replications." (emphasis added)

However, a more fundamental problem with CEV is this: people will be _horrified_ by a wiser version of themselves. Imagine if we had powerful AI in the year 1800, that implemented CEV, and accurately simulated our moral development up to the year 2025. A relatively wise person in 1800 could have seen that enslaving Black people was wrong. But even the top-5% wisest people back then would have been _horrified_ by the idea of a Black person being President, or marrying a white woman. By symmetry, there's very likely things _now_ that we're irrationally disgusted by, that a much wiser version of us would be okay with.

But if a powerful AGI just plopped down and said, _"Hey here's a horrifying thing I'm going to do, the much wiser version of you would agree to this"_, there's no way to tell in advance if it's true, or something went horribly wrong.

Hence, the next idea fixes this problem:

üåÄ **Coherent _Blended_ Volition:**

Instead of asking a paternalistic, Nanny AI to simulate us reflecting & improving upon our beliefs and values‚Ä¶ well, why don't _we_ reflect & improve upon our beliefs and values?

_"Because humans suck at doing that. See: all of History."_ Okay, fair enough. But what if we used all the tools at our disposal ‚Äî not just helper AIs, but also discussion platforms, data analysis, etc ‚Äî to help us _talk better?_ To find solutions that aren't mere average-centrism, but actually _combines the best parts_ of our diverse worldviews & values?[^cev-vs-cbv]

[^cev-vs-cbv]: Coherent Blended Volition was coined in [Goertzel & Pitt 2012](https://jetpress.org/v22/goertzel-pitt.htm): ‚ÄúThe CBV of a diverse group of people should not be thought of as an average of their perspectives, but [...] incorporating the most essential elements of their divergent views into a whole that is overall compact, elegant and harmonious. [...] The core difference between [CEV and CBV] is that, in the CEV vision, the extrapolation and coherentization are to be done by a highly intelligent, highly specialized software program, whereas in [CBV], these are to be carried out by collective activity of humans as mediated by [tools for healthy, deep discussion]. Our perspective is that the definition of collective human values is probably better carried out via human collaboration, rather than delegated to a machine optimization process.‚Äù

Sounds nice in theory. Does it work in practice? **So far: yes!** [Taiwan, directly inspired by Coherent Blended Volition](https://blog.pol.is/uber-responds-to-vtaiwans-coherent-blended-volition-3e9b75102b9b), used digital tools to collect & blend the perspectives of citizens & industry, to _create actual policy_. (The specific issue was Uber entering Taiwan.) Inspired by the success of Taiwan's digital tools, Twitter (now ùïè) used [a similar algorithm to design Birdwatch (now Community Notes)](https://blog.ncase.me/signal-boost-autumn-2025/#bridging), which as far as I can tell, is still the _only_ fact-checking service to be rated net-helpful _across the U.S. political spectrum._ No easy feat, in these polarized times!

This way, instead of asking a powerful AI to simulate us getting wiser, AI can help us _actually get wiser_. (by being a Socratic questioner, discussion facilitator, fact-checker, etc.) This way, we'll be able to accept wiser ideas & actions, even if the previous less-wise versions of us would've rejected them.

We make our tools better, so our tools help _us_ be better, repeat. We'll grow _alongside_ the AIs.

. . .

Maybe AI will never solve ethics. Maybe *humans* will never solve ethics. If so, then I think we can only do our best: remain humble & curious about what the right thing is to do, learn broadly, and self-reflect in a rigorous, brutally-honest way.

That's the best we fleshy humans can do, so let's at least make that the *lower bound* for AI.

### ü§î Review #7

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="How does Constitutional AI work?"
        answer="1] Human creates a list of principles. 2] Teacher-bot uses this list to train a student-bot. (This is how you can get millions of training data-points, from a small human-made list, which can also be democratically crowdsourced.)">
    </orbit-prompt>
    <orbit-prompt
        question="What is Moral Parliament?"
        answer="A 'parliament' of moral theories, with more seats for theories you're more confident in. The parliament votes on decisions.">
    </orbit-prompt>
    <orbit-prompt
        question="One way to distill human judgment using AI (not limited to LLMs specifically)"
        answer="1] ask human jury to answer lots of questions. 2] people can submit AIs to predict their answers. 3] use an ensemble (for robustness) of the best AIs as an Oracle for ‚Äúdoes this fit humanity's values‚Äù">
    </orbit-prompt>
    <orbit-prompt
        question="What does 'Coherent Extrapolated Volition' (CEV) mean?"
        answer="What wishes (Volition) we'd have in common (Coherent) if we had a lot of time to reflect & discuss with each other (Extrapolated)">
    </orbit-prompt>
    <orbit-prompt
        question="What does 'Coherent _Blended_ Volition' (CBV) mean?"
        answer="Same as CEV, except that the AIs _actually help us reflect & grow together_, instead of just simulating us doing that. (Success case: Taiwan's digital ‚Äúcitizen's assembly‚Äù, when Uber wanted to enter Taiwan)">
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="governance"></a>

## AI Governance: the _Human_ Alignment Problem

<img class="mini" alt="Comic of a dumb user, with computer saying 'ERROR ID-10-T: problem exists between chair and keyboard'" src="../media/p3/governance/idiot.png">

The saddest apocalypse: we _solve_ the game theory problems of AI Logic, we _solve_ the deep-learning problems of AI "Intuition", we even _solve_ moral philosophy‚Ä¶

We _know_ all the solutions, and then‚Ä¶ people are just too greedy or lazy to use it. Then we perish.

**Point is, all our work on the AI alignment problem means nothing if we can't solve the _human_ alignment problem: how do we get fallible fleshy humans to _actually coordinate on safe, humane AI?_**

This question is called **Socio-technical Alignment**, or **AI Governance**. 

. . .

Here's the Alignment vs Capabilities chart, again:

![Graph of Alignment to Humane Values vs Capabilities. A rocket is blasting towards the right. Above a dotted diagonal line, Alignment > Capabilities, we're heading to a good place. Below that dotted line, Alignment < Capabilities, we're headed to a bad place](../media/p3/governance/rocket.png)

The goal: keep our rocket above the "safe" line.

Thus, a 2-part strategy for AI Governance:

1. Verify where we are, our direction & velocity.
2. Use sticks & carrots to stay above the "safe" line.

(note that "governance" can also include bottom-up approaches, not just top-down! in case you were ‚Äî understandably ‚Äî worried about "AI Governance" being a Trojan Horse for a world government.)

In more detail:

**1\) Verify where we are, our direction & velocity:**

* <u>Evaluations (or "Evals")</u>, to keep track of AI Safety-related properties of frontier AIs. Can they help someone create weapons of mass destruction?[^eval-cbrn] Do they lead users into mental health spirals?[^eval-spiral] Can AI write code to train AIs?[^eval-metr] And so on. Ideally, citizens can create their own evals, too.[^eval-crowdsource]
* <u>Protect whistleblowers' free speech</u>. OpenAI once had a *non-disparagement* clause in their contract, making it illegal for ex-employees to publicly sound the alarm on them being sloppy on safety & deliberately accelerating the AGI arms race.[^non-disparage] Whistleblowers should be protected.
* <u>Enforce transparency & standards on major AI labs</u>. (in a way that isn't over-burdening.)
  * Require AI labs adopt a Responsible Scaling Policy (see below), openly publish that policy, and be transparent about their evals & safeguards.
  * Send in external, independent auditors (who will keep trade secrets confidential). This is what many software industries (like cybersecurity & VPNs) *already* do as regular practice.
* <u>Track chips & compute</u>. Governments keep track of GPU clusters, and who's running large "frontier AI"-levels of training compute. Similar to how governments already track large "bomb"-levels of nuclear material.
    * although, in Dec 2025, this approach may already be obsolete: almost all the recent gains in AI Capabilities is not from _training_ (which requires large centralized compute clusters), but _run-time_ (which anyone can in a decentralized way). This means that open-source AIs can soon be as powerful as big-company AIs. It also means that another AI Governance idea, "model weight security" ‚Äî keeping ChatGPT & Claude's neural networks secret ‚Äî will be of less importance.[^inference-scaling-vs-ai-governance]
* <u>Forecasting</u>. To know not just *where* we are, but our direction & velocity: have "super-predictors" regularly forecast AI's upcoming capabilities & risks.[^ai-forecasts] (There's also early evidence showing that AI *itself* can boost Humans' forecasting! [^cyborg-forecasts])

[^eval-cbrn]: [Quantifying CBRN Risk in Frontier Models](https://arxiv.org/html/2510.21133v1): ‚Äú[Frontier LLMs] pose unprecedented dual-use risks through the potential proliferation of chemical, biological, radiological, and nuclear (CBRN) weapons knowledge. [...] Our findings expose critical safety vulnerabilities: [...] Model safety performance varies dramatically from 2% (claude-opus-4) to 96% (mistral-small-latest) attack success rates; and eight models exceed 70% vulnerability when asked to enhance dangerous material properties. We identify fundamental brittleness in current safety alignment, where simple prompt engineering techniques bypass safeguards for dangerous CBRN information.‚Äù

[^eval-spiral]: [Spiral-Bench](https://eqbench.com/spiral-bench.html), a benchmark for measuring LLM sycophancy & delusion amplification. This website also hosts the [Emotional Intelligence Benchmark](https://eqbench.com/index.html) and [Slop Score](https://eqbench.com/slop-score.html)

[^eval-metr]: [Measuring AI Ability to Complete Long Tasks](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) by [METR](https://metr.org/), Model Evaluation & Threat Research

[^eval-crowdsource]: [Weval](https://weval.org/) is a platform to make & share your own AI Evaluations, especially on topics ignored by AI & AI Safety communities, like ["which AIs are evidence-based tutors?](https://weval.org/analysis/evidence-based-ai-tutoring/6398a4d26793af13/2025-10-05T07-09-36-338Z)"

[^non-disparage]: [Leaked SEC Whistleblower Complaint¬†Challenges OpenAI on Illegal Non-Disclosure Agreements](https://kkc.com/media/leaked-sec-whistleblower-complaint-challenges-openai-on-illegal-non-disclosure-agreements/) (2024)

[^inference-scaling-vs-ai-governance]: [Toby Ord 2025: Inference Scaling Reshapes AI Governance](https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance): ‚Äú_Rapid scaling of inference-at-deployment would: lower the importance of open-weight models (and of securing the weights of closed models), reduce the impact of the first human-level models, change the business model for frontier AI, reduce the need for power-intense data centres, and derail the current paradigm of AI governance via training compute thresholds._‚Äù

[^ai-forecasts]: For example, the Good Judgment Project & Metaculus, both with proven track records of being some of the best in general forecasting of future events, predict a [1% chance](https://goodjudgment.com/superforecasting-ai/) and [3% chance](https://possibleworldstree.com/) of near-extinction risk from advanced AI this century. ("Oh that doesn't sound too bad," you think, yeah wait til you see the % of non-extinction and/or non-AI risks.) Also, Metaculus is forecasting a median (with wide uncertainty) of AGI [by 2033](https://www.metaculus.com/questions/5121/date-of-general-ai/), and AGI becoming as species-changing as the agrilcultural/industral revolution [by 2044](https://www.metaculus.com/questions/19356/transformative-ai-date/). 

[^cyborg-forecasts]: [AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy](https://dl.acm.org/doi/pdf/10.1145/3707649) ‚Äì ‚ÄúParticipants (N = 991) answered
a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our **preregistered** analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that **the superforecasting assistant increased accuracy by 41%**‚Äù

**2\) Use sticks & carrots to stay above the "safe" line.**

* <b><u>Responsible Scaling Policy</u></b>.[^anthropic-RSP] A problem: we can't even accurately predict the risks of advanced AI until we get closer to it. So, instead of having a policy for across all possible future AIs, we take an _iterative_ approach (like with Scalable Oversight). For example: "We commit to not even _start_ training an AI of Level N, until we have safeguards and evaluations _up to Level N+1_."
    * A similar idea was proposed in the [Guaranteed Safe AI](https://arxiv.org/pdf/2405.06624) paper.
* <u>Differential Technology Development (DTD)</u>[^dtd] AI, by default, is **"dual-use"**: could be used for great good or great harm. _Differential_ development, then, is about investing in research & tech that **1) advances "Alignment" more than "Capabilities", and 2) maximizes the usefulness of AI while minimizing its risks.** For example:
    * Tech to fight catastrophic risk: AI & non-AI tools that *boost* our cybersecurity, biosecurity, and mental security.
    * AI Safety research. Yes this suggestion is kinda back-scratchy, but I still endorse it.
    * [Data filtering](https://arxiv.org/pdf/2402.00530) and [Machine Unlearning](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf), so that LLMs contain helpful knowledge, but not detailed knowledge of how to make bombs or superviruses.
    * At least commit to NOT building horrible stuff like fully-autonomous killer drones.[^slaughterbots]
    * AI that *enhances* humans, not *replaces* humans. (See below: "Alternatives to AGI" and "Cyborgism"!)
    * Combine this idea with _decentralized & democratic tech_, to prevent AI and/or AI Governance from becoming a tools for tyrants. See: [d/acc](https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html), [Plurality](https://vitalik.eth.limo/general/2024/08/21/plurality.html), [6pack.care](https://6pack.care/). (explained more in the below infographic ‚§µ)

[^slaughterbots]: From Stuart Russell, the co-author of the most-popular textbook in AI: [Banning Lethal Autonomous Weapons: An Education](https://people.eecs.berkeley.edu/~russell/papers/issues22-laws.pdf)

[^anthropic-RSP]: [Anthropic's Responsible Scaling Policy](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) (2023) Inspired by the US government's Biosafety Levels (BSL), they defined ASL-1 (old chess AIs), ASL-2 (current LLMs), ASL-3

[^dtd]: [Sandbrink et al 2022](https://ora.ox.ac.uk/objects/uuid:b481e9ad-bc27-4550-87ca-f414354aeb35/files/s2v23vw012)

![The 6-pack of digital democracy: broad listening, credible commitments, easy-to-inspect, easy-to-correct, win-win bridging solutions, and as local as possible](../media/p3/governance/6pack.png)

_(if i may toot my own horn a bit_, i'm _doing the illustrations for Audrey Tang's upcoming book, 6pack.care! all comics will be dedicated to the public domain.)_

Another thought: while "sticks" (fines, penalties) are necessary, I think it's neglected how we can use "carrots" (market incentives) to redirect industry. As [Audrey Tang](https://en.wikipedia.org/wiki/Audrey_Tang) ‚Äî Digital Minister of Taiwan, and co-author of [6Pack.care](https://6pack.care/) ‚Äî once explained it: if you successfully advocated for securing cockpit doors _before_ 9/11, or better biosecurity before Covid-19, your reward would be... "nothing happens". Nobody cares about the bomb that didn't go off. [^tenet] So, if you want your AI Alignment & Governance ideas to be _actually implemented in real life, not just in academic papers_, you need them to "pay dividends" in the short term.

Many of the AI Safety solutions listed above can have, or _already have had_, profitable market spinoffs. (examples: [^safety-market])

But in the next final two sections, "Alternatives to AGI" and "Cyborgism", we'll see ideas that empower regular fleshy humans (rather than a few companies/governments), _and_ are short-term market-incentive-compatible, _and_ long-term Good.

[^safety-market]: For example, [Reinforcement Learning from Human Preferences (RHLF),](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf) a way for AIs to import human values, was what turned Base GPT (an autocomplete) into *Chat*GPT (an actual chatbot). Likewise, improving the robustness of AI "intuition" would help make better AI medical diagnosis, and self-driving cars.

[^tenet]: Quote from Tenet (2020), the most "yup that's a Nolan film" Nolan film.

[: bonus - what about the economics of AI? basic income, human subsidies, re-skilling?](#EconomicsAI)

. . .

A note of pessimism, followed by cautious optimism.

Consider the last few *decades* in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? "Humans coordinating to deal with global threats" is... not a thing we seem to be good at.

But we *used* to be good at it! We eradicated smallpox[^smallpox], it's no longer the case that *half* of kids died before age 15[^child-mortality], the ozone layer *is* actually healing! [^ozone]

[^smallpox]: [Smallpox used to kill millions of people every year. Here‚Äôs how humans beat it.](https://www.vox.com/future-perfect/21493812/smallpox-eradication-vaccines-infectious-disease-covid-19) by Kelsey Piper
 
[^child-mortality]: Source: [Our World In Data](https://ourworldindata.org/child-mortality)

[^ozone]: March 2025 on MIT News: [Study: The ozone hole is healing, thanks to global reduction of CFCs](https://news.mit.edu/2025/study-healing-ozone-hole-global-reduction-cfcs-0305). "New results show with high statistical confidence that ozone recovery is going strong."

Humans *have* solved the "human alignment problem" before.

Let's get our groove back, and align ourselves on aligning AI.

#### :x Economics AI

Not really related to "AI Governance", but is _is_ a major sociopolitical question, that the world's governments _should_ care about:

**Will an AI take my job?**

Actually, it's worse than that ‚Äî even if you're in an AI-proof job, AI can still make your wages plummet. Why? Because the humans who lose _their_ job to AI, will flood _your_ field, and market competition will pull your wage downwards.

But in the past, hasn't automation always been good on average, creating more jobs than it destroys?

1. Well, re: "average", the average person has one testicle. Point is: **"average" ignores the _variance_.** If the bottom 99% of people lose \\$10,000, so that the top 1% gain \\$1,000,000, that's a win "on average". I exaggerate, but [since Covid, we may already be in a K-shaped economy](https://www.marketplace.org/story/2025/11/11/kshaped-economy-mirrored-by-covids-uneven-recovery): one line goes up for the rich, one line goes down for the rest of us. (And even if you're a utilitarian who only cares about total utility, remember utility is logarithmic to wealth. So if Alice gains \\$10,000 & Bob loses \\$10,000, Alice gains _less_ utility than Bob loses: total wealth is constant, but total _utility_ drops. You wouldn't be _neutral_ about a coin flip where you gain/lose \\$10,000, would you?)
2. **Yes, automation _does_ create jobs at first, but at advanced levels, starts taking them away.** A good example is automatic telling machines (ATMs) at banks: at first, their introduction actually correlated with an _increase_ in human tellers being hired, because while each branch needed fewer human tellers, ATMs made it cheaper to open many _more_ branches, so that total humans hired actually increased. In the short run. Now, with online banking, and with the market for bank branches already saturated, human tellers hired is dropping again. ([see this article](https://80000hours.org/agi/guide/skills-ai-makes-valuable/#1-why-automation-often-doesnt-decrease-wages))
3. **This time really is different.** In the past, automation "only" took out one small sector at a time (textile workers, horse drivers, elevator operators, etc). But _this_ time, with deep learning, AIs can automate a huge percentage of the human economy in one go: a) self-driving cars vs truckers & taxicabs, b) LLMs vs programmers, lawyers & writers, c) speech-to-text-and-back vs call centers, secretaries, & audiobooks/podcasters, d) image and video models vs artists and creatives, e) so on and so on. And if you're in an AI-safe job, like plumber? Good for you! But your wages will drop from _everyone else_ scrambling to re-train to AI-safe jobs, like plumber.

(It's hard to tell how badly AI is affecting the job market _right now_. The hiring rates for artists, writers, and even newly-graduated programmers ‚Äî once thought of a golden ticket to a 6-figure salary! ‚Äî has been dropping since generative AI came out in late 2022. But that _also_ coincides with a post-Covid recession, so it's hard to tell how much of the unemployment is due to AI specifically, vs the sucky economy in general.)

= = =

A popular proposal is a **Universal Basic Income (UBI)**, funded by taxing AI. After all, modern AI was trained on data created by the public's hard work ‚Äî shouldn't part of the gains go back to the public, a "citizen's dividend, similar to [Alaska's sovereign wealth fund](https://en.wikipedia.org/wiki/Alaska_Permanent_Fund), where profit from their natural resources goes back to their citizens?

A lot of skepticism about UBI comes from it promoted by tech CEOs [like Sam Altman](https://moores.samaltman.com/), who are, uh, ["not completely candid"](https://garymarcus.substack.com/p/sam-altmans-pants-are-totally-on). That said, just because someone's promoting an idea cynically to quell dissent, doesn't mean it's _not_ a good idea. Tech CEOs could be lying about promising UBI, because UBI _would_ be great. [To quote GLaDOS](https://p1.portal2sounds.com/57): "Killing you, and giving you good advice, aren't mutually exclusive."

Another common critique of UBI is that people don't just need money, they need _meaning_, and people get meaning from their jobs. (["Love and Work"](https://figuringoutfulfillment.wordpress.com/2012/01/23/lieben-und-arbeiten/), Freud supposedly said.) Now: writing as someone who finds meaning in her job: _f\@\#k off_. I keep hearing the "work is good coz it gives meaning" line from think tanks & pundits & the Credentialed Class, and, yeah, easy for _you_ to say, you have a _nice_ job. _I_ have a nice job. We're not working the graveyard shift at 7/11, or slowly getting lung cancer from the industrial fumes. _But, oh, if we're all living off UBI, how will we get meaning if we can't sell our labour on a market?_ Oh I dunno, how about spending life with family? friends? lovers? while getting your sense of mastery & challenge from sports, art, math, play? And if you love your job making art or writing essays, you know you can just _keep doing that_ even with UBI, right?

Sorry I'm pissy about this, I just do not respect Status Quo Stockholm Syndrome.

However, there _is_ one strong critique of UBI that I acknowledge. Despite all the exciting pilot programs in developing nations, **UBI in _developed_ nations like America mostly doesn't help.** (see [the OpenResearch study by Vivalt et al 2025](https://evavivalt.com/wp-content/uploads/Vivalt-et-al.-ORUS-employment.pdf), lit review by [Kelsey Piper](https://www.theargumentmag.com/p/giving-people-money-helped-less-than))

For example, [see the BIG:LEAP report](https://basicincome.org/wp-content/uploads/2024/07/CGIRLABIGLEAPFinalReport.pdf), a UBI experiment in Los Angeles. See Tables 4, 5, and 9: receipients' financial well-being, food security, and perceived stress were all _barely_ any better than controls, after _18 months of getting \\$1000/month_.

And it's not because "well, Americans are already relatively rich compared to Africans, so extra cash doesn't do much" ‚Äì one RCT gave _\\$1000/month to homeless people in Denver_, and after ten months, they weren't any more likely to have stable housing than the controls getting \\$50/month. ([See Figures 1 & 2 of their research page](https://www.denverbasicincomeproject.org/research), which tries to lie-via-misleading-truth, and play this off as a success?!?!)

So if UBI doesn't even help _literally homeless people_, how would it help people who lost their jobs to AI?

Also, in these major UBI trials, recipients worked a bit less. Which would be good if the extra leisure time translated to better mental health, or better child health. It didn't.

(To be fair, at least UBI in America doesn't _hurt._ The recipients mostly spent the extra money on nicer things for their kids & loved ones, and did not spend it on drugs or gambling. And UBI _did_ reduce intimate partner abuse, and make people happier‚Ä¶ for about one year, before returning to baseline. Which isn't nothing! Not being beat by your spouse for a year matters!)

So‚Ä¶ what gives? Why does UBI work in developing nations, but not America?

I don't know. Maybe money is a stronger limiting factor in developing nations, but non-money stuff (personal habits, cultural norms, social ties, systemic issues) are a stronger constraint in America. This is a very ad-hoc explanation.

= = =

I know the following sounds very "Ah. Well, nevertheless," but nevertheless, I do (tentatively) support UBI for AI-powered job displacement. The above evidence shows that UBI won't help with poverty, at least in America, but:

1. We can consider UBI a deserved citizen's dividend, like Alaska's get-$1000-a-year natural resource fund. After all, it was (involuntarily) trained off _your_ data, shouldn't you at least get some of the profit?
2. We can consider UBI as "insurance", because it's very unpredictable when you'd lose your job to a robot. Seriously, who 20 years ago would've thought we'd _automate art and poetry_ before automating "clean my house"?
3. UBI still seems like the smoothest, non-violent way to transition from today's market economy, to a post-scarcity Star Trek utopia, where food & shelter & all the essentials become "too cheap to meter".

= = =

An alternative to UBI for poverty reduction, is the **Earned Income Tax Credit (EITC)**, also known as a negative income tax. The idea is that the company pays you whatever's the market wage, but it's topped up to a livable wage through tax credits.

Politically, it's popular [among](https://waysandmeans.house.gov/2024/02/20/icymi-american-compass-op-ed-child-tax-credit-is-a-win-for-conservatives/) [conservatives](https://www.aei.org/economics/does-economic-freedom-have-a-future-in-america-a-qa-with-arthur-brooks/) because unlike UBI, the EITC provides pro-work incentives. (plus, "negative tax" and "tax rebate" _feels_ better than "welfare check". In the latter, you're getting a handout. But in the former, you're _getting your money back_. Feels better! Even though it's frikkin' identical. _It's all about the marketing, baby._)

It's also popular among economists (who are the most politically diverse field in social science). According to the most recent survey of economists, 90% support expanding the EITC. ([see Table 1](https://d1wqtxts1xzle7.cloudfront.net/95174671/HBhGyFD7-libre.pdf?1669999561=&response-content-disposition=inline%3B+filename%3DConsensus_among_economists_2020_A_sharpe.pdf&Expires=1763701882&Signature=IaFfiqOrXv-Nov61st-qoOTiaE8K8~pVvirurLs3akQKaO6xxn~khOCs1ufp2jJNkbDzg7fU-aHX~XP6CFvU5pSRSrkP5SobLhbEdIf9yN-Mg~ezpdVFoAnvifuXXpH9giSaR1-OPndVSdh~xCzx0FWqWsYa4k04uKlYpOFdMdvm4LhDVnoBttyhRd~8hWGi22aBRK19xCtlEe63E3mOoJx79fv9APB~iRy4gkV9uryKSJoSl5jqEBBOohpxsK4OURp46z8OLowwzxbeK~0zG9WURDFvQxQaMidtQNs9tFEerWa~GBZSVMp~Y8nVYeqN-80PEWKJfsimBM133vXefg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA))

But more relevant to AI economics, the EITC is a **human subsidy**. You're subsidizing companies to hire local humans, instead of offshoring them or automating them. And in sectors where AI _still_ has the upper economic hand, we can tax it to fund the EITC.

This can also be paired with incentivizing & investing in **upskilling & retraining human workers.** Help us get jobs to evade LLMs' grasp, e.g. tasks which require rule-discovery (eg research), dexterity in physical environments (eg manual trades, live performance arts), long contexts (eg longform writing), or we just _want_ the job to be human (eg elderly care).

= = =

We're not even scratching the surface of economic policy proposals re: advanced AI. For a good overview, [check out this review of options on Anthropic's blog.](https://www.anthropic.com/research/economic-policy-responses)

My personal opinion, as of Dec 2025, open to change: for policy, I'd recommend a combined UBI + EITC + educational vouchers for reskilling. It'd be funded by taxing the productivity gains from AI, and given mostly towards high-automation-risk workers like: a) truckers, taxicab drivers (self-driving cars), b) call center services (speech & language AIs), c) retail (digital kiosks, which aren't even "AI"). And at the risk of being extremely self-serving, sure: digital arts, programming, and white-collar work are all also at high automation risk.

As for what you can do _personally_, well, you might want to read/listen to **[How not to lose your job to¬†AI](https://80000hours.org/agi/guide/skills-ai-makes-valuable/).**

### ü§î Review #8

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="All our work on the AI alignment problem means nothing if we can't solve..."
        answer="...the _human_ alignment problem: how do we get fallible fleshy humans to _actually coordinate on safe, humane AI?">
    </orbit-prompt>
    <orbit-prompt
        question="The 2 core pillars of AI Governance:"
        answer="1] Verify where we are. 2] Stay above the 'safe' line."
        answer-attachments='https://aisafety.dance/media/p3/governance/rocket.png'>
        <!-- aisffs-gov-rocket.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to 'verify where we are' in AI Governance:"
        answer="(any of the following work:) evaluations (ideally crowdsourced), protect whistleblowers, enforce transparency & standards on AI labs, track chips & compute, forecasting (maybe AI-aided)">
    </orbit-prompt>
    <orbit-prompt
        question="Responsible Scaling Policy, in a nutshell:"
        answer="'We commit to not even _start_ training an AI of Level N, until we have safeguards and evaluations _up to Level N+1_.'">
    </orbit-prompt>
    <orbit-prompt
        question="Differential Technological Development in a nutshell:"
        answer="Invest in tech/research that boosts Alignment _more than_ Capabilities. (or at least Safe Capabilities _more than_ Risky Capabilities)">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 way to 'stay above the safe line':"
        answer="(any of the following work:) Responsible Scaling Policy, Differentially invest in Alignment > Capabilities, Tech to fight catastrophic risk, Data filtering/machine unlearning so that AI doesn't know how to make bombs/superviruses, AI that enhances not replaces humans ('Cyborgism').">
    </orbit-prompt>
    <orbit-prompt
        question="If you want your AI Alignment & Governance ideas to be _actually implemented in real life, not just in academic papers_..."
        answer="You need your ideas to ‚Äúpay dividends‚Äù right now, in the short term. (‚ÄúNobody cares about the bomb that didn't go off.‚Äù)">
    </orbit-prompt>
    <orbit-prompt
        question="Name at least 1 time humanity successfully coordinated to solve a global problem:"
        answer="(any of the following works:) We eradicated smallpox, it's no longer the case that *half* of kids died before age 15, the ozone layer *is* actually healing!">
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="alt"></a>

## Alternatives to AGI

Why don't we just *not* create the Torment Nexus?[^torment]

[^torment]: [classic 2021 tweet](https://x.com/AlexBlechman/status/1457842724128833538) by alex blechman

If creating an Artificial General Intelligence (AGI) is so risky, like sparrows stealing an owl egg to try to raise an owl who'll defend their nest & hopefully not eat them[^bostrom]...

...why don't we find ways to get the pros *without* the cons? A way to defend the sparrow nest *without* raising an owl? To un-metaphor this: why don't we find ways to use **less-powerful, narrow-scope, not-fully-autonomous AIs** to help us ‚Äî say ‚Äî cure cancer & build flourishing societies, *without* risking a Torment Nexus?

[^bostrom]: Opening metaphor from [Nick Bostrom's classic 2014 book, Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)

Well... yeah.

Yeah I endorse this one. Sure, it's obvious, but "2 + 2 = 4" is obvious, that don't make it wrong. The problem is how to *actually do this* in practice. 

Here's some proposals, of how to get the upsides with much fewer downsides:

* **Comprehensive AI Services (CAIS), Tool AIs**[^alt-drexler]: Make a large suite of narrow non-autonomous AI tools (think: Excel, Google Translate). To solve general problems, insert *human* agency: humans are the conductors for this AI orchestra. The human, and their values, stay in the center.
* **Pure Scientist AI**.[^alt-bengio] Imagine an AI that just takes in observations, and spits out theories. (The same way that Google Translate can take English as input, and Mandarin as output.) Crucially, this AI _does not_ have "agency", which would lead to [Instrumental Convergence](https://aisafety.dance/p2/#problem2) problems. It just fits _scientific models_ to data, like how Excel fits lines to data. It _can't_ go rogue any more than Google Translate can go rogue.
    * It's far from a few Scientist AI, but Microsoft has done research in training ANNs to [infer causal diagrams](https://www.microsoft.com/en-us/research/publication/deep-end-to-end-causal-inference-2/).
* **Microscope AIs**[^alt-olah]: In contrast, instead of making an AI whose scientific findings are its *output*, we train an AI to predict real-world data, then look *at the neural connections themselves* to learn about that data! (using Interpretability techniques)
* **Quantilizers**[^alt-taylor]: Instead of making an AI that *optimizes* for a goal, make an AI that is trained to *imitate a (smart) human*. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days. This "soft optimization" avoids Goodhart-problems[^goodhart] of pure optimization, and keeps the resulting solutions human-understandable.

[^alt-drexler]: [Glossary entry on CAIS](https://forum.effectivealtruism.org/topics/comprehensive-ai-services)

[^alt-bengio]: [Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657) by Bengio et al 2025

[^alt-olah]: From [Chris Olah‚Äôs views on AGI safety, by Evan Hubinger](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety): ‚ÄúWe could use ML as a _microscope_‚Äîa way of learning about the world without directly taking actions in it. That is, rather than training an RL agent, you could train a predictive model on a bunch of data and use interpretability tools to inspect it and figure out what it learned, then use those insights to inform‚Äîeither with a human in the loop or in some automated way‚Äîwhatever actions you actually want to take in the world.‚Äù

[^alt-taylor]: [Quantilizers: A Safer Alternative to Maximizers for Limited Optimization](https://cdn.aaai.org/ocs/ws/ws0198/12613-57416-1-PB.pdf) by Jessica Taylor 2016

[^goodhart]: [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law), paraphrased: "_When you reward a metric, it usually gets gamed._"

*All* of these are easier said than done, of course. And there's *still* other problems with a focus on "Alternatives to AGI". Social problems, and technical problems:

* A malicious group of humans could still use narrow AI for catastrophic ends. (e.g. bioweapon-pandemic, self-replicating killer drones)
* A well-meaning-but-na√Øve group could use narrow non-autonomous AI to *make* general autonomous AI, with all of its risks.
* An AI that doesn't plan ahead, and "merely" predicts future outcomes, can still have nasty side-effects, due to self-fulfilling prophecies.
* Due to economic incentives (and human laziness), the market may just *prefer* to make general AIs that are fully autonomous.
* Due to incentives & laziness, we end up in a [gradually disempowered Slopworld](https://gradual-disempowerment.ai/). Instead of a smart AI taking over the world, we _hand over_ the world to dumb AIs, piece-by-piece. Death by convenience.

That said, it's good to stack extra solutions, even if imperfect!

As for that last concern, about handing over our autonomy to AIs, that's what our final Cyborgism section covers...

### ü§î Review #9

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="Name at least 1 way to get the benefits of AI without making a powerful, general, autonomous AI"
        answer="(Any of the following works:) Comprehensive AI Services, Tool AIs, Pure Scientist AIs, Microscope AIs, Quantilizers, Cyborgism">
    </orbit-prompt>
    <orbit-prompt
        question="What's a 'Scientist AI'?"
        answer="An AI that ‚Äújust‚Äù takes in observations, and spits out theories. (The same way that Google Translate can take English as input, and give Mandarin as output.)">
    </orbit-prompt>
    <orbit-prompt
        question="What's Microscope AI?"
        answer="Instead of making an AI whose scientific findings are its *output*, we train an AI to predict real-world data, then look *at the neural connections themselves* to learn about that data! (using Interpretability techniques)">
    </orbit-prompt>
    <orbit-prompt
        question="What's a 'Quantilizer'?"
        answer="Instead of making an AI that *optimizes* for a goal, make an AI that is trained to *imitate a (smart) human*. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days.">
    </orbit-prompt>
</orbit-reviewarea>

---

<a id="cyborg"></a>

## Cyborgism

Re: humans & possible future advanced AI,

**If we can't beat 'em, join 'em!**

We *could* interpret that literally: brain-computer interfaces in the medium term, mind-uploading in the long term. But we don't have to wait that long. The mythos of the "cyborg" can still be helpful, *right now!* In fact:

*YOU'RE ALREADY A CYBORG.*

...if "cyborg" means any human that's augmented their body or mind with technology. For example, you're *reading* this. Reading & writing *is* a technology. (Remember: things are still technologies even if they were created before you were born.) Literacy even *measurably re-wires your brain.*[^literacy] You are not a natural human: a few hundred years ago, most people couldn't read or write.

[^literacy]: [An anatomical signature for literacy](https://www.academia.edu/download/51625741/An_anatomical_signature_for_literacy._Na20170203-24453-1k92usb.pdf) by Carreiras et al 2009

Besides literacy, there's many other everyday cyborgisms:

* <u>Physical augmentations:</u> glasses, pacemakers, prosthetics, implants, hearing aids, continuous glucose monitors
* <u>Cognitive augmentations:</u> reading/writing, math notation, computers, spaced repetition flashcards
* <u><i>Emotional</i> augmentations:</u> diaries, meditation apps, reading biographies or watching documentaries to empathize with folks across the world.
* <u><i>"Collective Intelligence"</i> augmentations:</u> Wikipedia, prediction markets/aggregators.

![Stylish silhouette-drawing of people with "everyday cyborg" tools. Glitched-out caption reads: We're all already cyborgs.](../media/p3/cyborg/cyborg.png)

**Q:** That's... tool use. Do you really need a sci-fi word like "cyborg" to describe *tool use?*

**A:** Yes.

Because if the question is: "how do we keep human *values* in the center of our systems?" Then one obvious answer is: keep *the human* in the center of our systems. Like that cool thing Sigourney Weaver uses in *Aliens (1986)*.

![Screenshot of Sigourney Weaver in the Power Loader. Caption: cyborgism, keeping the human in the center of our tools](../media/p3/cyborg/weaver.png)

Okay, enough metaphor, here's some concrete examples:

* Garry Kasparov, the former World Chess Grandmaster, who also famously lost to IBM's chess-playing AI, once proposed: CENTAURS. It turned out, **human-AI teams could beat *both* the best humans & best AIs at chess**, by having the human's & AI's strengths/weaknesses compensate for each other! [^cite-self] (This idea may not be true for full-information bounded-rules games like Chess & Go anymore[^gwern], but at present it's true for research & forecasting, see below.)
* Likewise, some researchers are trying to [combine the strengths/weaknesses of humans & LLMs](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism). For example, humans are currently better at deep planning, LLMs are better at broad knowledge. Together, **a "cyborg" may be able to think deeper *and* broader than pure-human or pure-LLM.**
  * (You can try this out *today!* janus made a writing tool called [Loom](https://generative.ink/posts/loom-interface-to-the-multiverse/), which lets you explore a "multiverse" of thoughts.)
* Large Language Models are "only" about as good as the average person at forecasting future events, but *together*, a normal LLM can help normal humans improve their forecasting ability by up to 41%! [^cyborg-forecasts]
* [Enough AI Copilots! We need AI HUDs](https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds) was a popular article that argued for a different design philosophy in AI-assisted coding: instead of having AI agents act like underpaid interns, you can make tools that give you "X-ray vision" into your code, or an augmented heads-up display (HUD) like Iron Man's. This way, _you_ get to see & build upon insights. Unlike in "vibe coding", _your code will not become a stranger to you._
* [Using Artificial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/) investigates _"artificial intelligence augmentation (AIA)"_, how to _enhance_ human creativity & invention, instead of offloading it fully to generative AIs. In this article, they highlight a demo by [Zhu et al (2016)](https://arxiv.org/pdf/1609.03552). This demo came out _long_ before ChatGPT's DALL-E, and honestly, it's *still* a far more compelling artistic tool than most modern AI art programs. Because, it lets you do precise artistic expression, vs the "write text and hope for the best" approach of DALL-E / MidJourney / etc: (though, there have been some prototypes, updating this for modern AI models[^ai-art-prototype])

<video width="640" height="360" controls>
    <source src="../media/p3/cyborg/shoe.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

[^ai-art-prototype]: [I made a python script the lets you scribble with Stable Diffusion in realtime](https://www.reddit.com/r/StableDiffusion/comments/12pcbne/i_mad_a_python_script_the_lets_you_scribble_with/)

[^cite-self]: Citing myself here, ehehehe: [How To Become A Centaur](https://jods.mitpress.mit.edu/pub/issue3-case/release/6) by Nicky Case (2018) Though, see next footnote, the main analogy is now obsolete.

[^gwern]: [From Gwern](https://www.lesswrong.com/posts/sTboWTyf9MfERnsKp/gwern-about-centaurs-there-is-no-chance-that-any-useful-man): ‚ÄúAs far as I can tell, in chess, the centaur era lasted barely a decade, and would've been shorter still had anyone been seriously researching computer chess rather than disbanding research after Deep Blue or the centaur tournaments kept running instead of stopping a while ago.‚Äù  [In 2017](https://en.wikipedia.org/wiki/Advanced_chess): ‚ÄúA computer engine (Zor) ended first in the freestyle Ultimate Challenge tournament (2017), while the first centaur (Thomas A. Anderson, Germany) ranked in 3rd place‚Äù. Though as of 2023, [Human+AI can still beat pure AI in Go](https://www.engadget.com/human-convincingly-beats-ai-at-go-with-help-from-a-bot-100903836.html), but mostly due to exploiting the Robustness issues in modern deep-learning. Whether or not you count that as "cheating" is up to you & your sense of aesthetics.

. . .

**[Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism)** became popular in the AI Safety field in 2023, thanks to Nicholas Kees & janus's hit article. It's a looooong post, but here's my favourite diagram from it:

![Human & GPT's comparative advantages/disadvantages, fitting together like puzzle pieces. For example, GPT are better at breadth & variance, Humans better at deep, long-term coherence.](../media/p3/cyborg/puzzle.png)

Remember in the "Gears" section above, where I was shocked at all the things LLMs surpass Humans at, but also how fragile & terrible LLM reasoning is? Cyborgism says: _this ain't a bug, it's a feature!_ Because if there's things that AI can do that Humans can't, _and_ vice versa, then there's benefits to combining our talents:

![3 Venn Diagrams. Early on: AI skills are strictly a subset of Human skills. Too late: Human strictly subset of AI. But in the middle, sweet spot: our skills do not fully overlap, meaning there is an opportunity for the collab of Human+AI to do better than either alone](../media/p3/cyborg/venn.png)

(Examples of Stage 1, Humans > AI: dexterity in unpredictable environments, like folding laundry or home repairs. Example of Stage 3, AI > Humans: arithmetic, maybe chess. But lots of tasks are still in Stage 2, Human+AI > Human or AI: code, forecasting, AI Safety research itself! We're in that sweet spot where Cyborgism can still pay off huge dividends _right now_.)

([: bonus - an incomplete list of what Humans & LLMs are relatively better at](#HumanLLMAdvantages))

The other point of Cyborgism for AI Safety is this: **_useful_ AI ‚â† _agentic_ AI.** (where "agentic" means it pursues goals intelligently) Sure, giving AI its own autonomy is _one_ way to make it more useful (or at least, convenient), but it carries huge risks (from it "going rogue", to a gradual-disempowerment Slopworld.) Cyborgism shows us a way to get useful AI _without_ giving up our autonomy.

Keep the human in the center! Fight for the users!

. . .

Caveats & warnings:

* It is very hard to design efficient "cyborgs". [A recent study from METR](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) found that AI coding tools actually made programmers _slower_ at their jobs, _even while fooling those programmers into thinking they were being more efficient_.
* Humans are still lazy & we'll still be tempted to give up our autonomy. (So that's another reason to make this sound *cool* with "cyborg", not just "tool use".)
* When you put yourself inside a system, the system may modify *you*. Even for reading/writing, anthropologists agree that literacy isn't just a skill, it modifies your entire *culture and values*. What would becoming a *cyborg multiverse-thinker* do to you?
* Again, an AI-augmented human could be a sociopath, and bring about catastrophic risk. Again-again, *one* human's values ‚â† humane values.

Then again...

![Close-up of Sigourney Weaver](../media/p3/cyborg/weaver2.png)

That's pretty cool.

#### :x Human LLM Advantages

A more detailed list of what Humans & LLMs are better at. (_for now_, though see the above Gears section for why I suspect these differences are fundamental to LLMs)

üí™ **Humans are better at:**
- Uncommon tasks
- Uncommon rule-discovery
- Common tasks in _uncommon_ settings
- Long-term coherent planning
- "Multimodal" thinking, switching fluently between words & pictures
- Dexterity in & reasoning about the physical world

ü¶æ **LLMs are better at:**
- Common short tasks, _fast_
- Recognizing & applying _common_ rules/patterns
- Broad (if shallow) knowledge of a lot of topics
- Can roleplay a wide range of characters (if stereotypically)
- High-variance brainstorming (you can easily reset an LLMs context)
- Language-based tasks, like search, summary, translation, sentiment analysis.

Hopefully that gives you (and myself) ideas for how to design tools that _combine_ Human & AI skills, while keeping Human autonomy at the center!

---

### ü§î Review #10 (last one!)

<orbit-reviewarea color="violet">
    <orbit-prompt
        question="‚ÄúWe're all already cyborgs‚Äù. Give 3 examples?"
        answer="(any 3 of the following works): **Physical augmentations:** glasses, pacemakers, prosthetics, implants, hearing aids, continuous glucose monitors. **Cognitive augmentations:** reading/writing, math notation, computers, spaced repetition flashcards. **Emotional augmentations:** diaries, meditation apps, biographies & documentaries as empathy-enhancers. **‚ÄúCollective Intelligence‚Äù augmentations:** Wikipedia, prediction markets/aggregators."
        answer-attachments='https://aisafety.dance/media/p3/cyborg/cyborg.png'>
        <!-- aisffs-cyborg.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Cyborgism: ‚Äúkeeping the \_\_\_\_\_ in the \_\_\_\_\_‚Äù"
        answer="keeping the human in the center of our tools">
    </orbit-prompt>
    <orbit-prompt
        question="Think of 1 thing that Humans are relatively better at than LLMs, and vice versa:"
        answer="(any of the following in this puzzle-piece diagram:)"
        answer-attachments='https://aisafety.dance/media/p3/cyborg/puzzle.png'>
        <!-- aisffs-cyborg-puzzle.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Name a real-world example where Human+AI team _proved_ to better than Human or AI alone"
        answer="Chess, Go, Forecasting future events. (although 'cyborgs/centaurs' in chess stopped having an advantage since 2017 -- there's a time limit to the 'sweet spot' for cyborgs!)">
    </orbit-prompt>
    <orbit-prompt
        question="Visualize the sweet spot for Cyborgism, as 3 Venn diagrams:"
        answer=""
        answer-attachments='https://aisafety.dance/media/p3/cyborg/venn.png'>
        <!-- aisffs-cyborg-venn.png -->
    </orbit-prompt>
</orbit-reviewarea>

---

## üéâ RECAP #3

- üíñ **We just need a good-enough first draft of Humane Values, that can reflect upon & improve itself.** This could be anything from a crowdsourced constitution, to a moral parliament, to tools that help us grow _alongside_ AI.
- üöÄ From both the top-down & bottom-up, **AI Governance is about keeping us above the "safe" line: trust but verify, sticks & carrots.**
- üí™ Make tech that maximizes upsides while minimizing downsides: **narrow AI, and AI that _enhances human agency, not replaces it_.**

---

## In Sum:

Here's THE PROBLEM‚Ñ¢Ô∏è, broken down, with all the proposed solutions! ( [Click to see in full resolution!](../media/p3/SUM.png) )

![SUMMARY OF THIS WHOLE ARTICLE, AAAAA](../media/p3/SUM.png)

And here's [Recap 1](#recap1), [Recap 2](#recap2), and [Recap 3](#recap3).

(Again, if you want to actually *remember* all this long-term, and not just be stuck with vague vibes two weeks from now, click the Table of Contents icon in the right sidebar, then click the "ü§î Review" links for the flashcards. Alternatively, download the [Anki deck for Part Three](https://ankiweb.net/shared/info/1788882060).)

. . .

*(EXTREMELY LONG INHALE)*

*(10 second pause)*

*(EXTREMELY LONG EXHALE)*

. . .

Aaaaand I'm done.

Around 80,000 words later (about the length of a novel), and nearly a hundred illustrations, that's... it. Over three years in the making, that's the end of my whirlwind tour guide. **You now understand the core ideas from the wide world of AI & AI Safety.**

üéâ Pat yourself on the back!  (But mostly pat *my* back. (I'm so tired.))

Sure, the field of AI Safety is moving so fast, Part One & Two started to become obsolete before Part Three came out, and doubtless Part Three: The Proposed Solutions will feel na√Øve or obvious in a couple years.

But hey, the real AI Safety was all the friends we made along the way.

Um. I need a better way to wrap up this series.

Okay, click this for a really cool **CINEMATIC CONCLUSION:**

<style>
#next_button{
    transform: scale(1.3) translate(0,20px);
}
</style>

{% include 'templates/next_page_button.html' %}

. 

.

. 

.

.

. 

.

. 

.

wait what are you doing? scroll back up üëÜüëÜüëÜ, the cool ending's in the button up there.

come on, it's just boring footnotes below.

. 

.

. 

.

.

.

.

.

.

.

ugh, fine: