<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>

    <!-- Title -->
    <title>Part 3: The Proposed Solutions</title>

    <!-- UTF-8 & Mobile -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- Links are external by default -->
    <base target="_blank">

	<!-- Favicon -->
	<link rel="icon" type="image/png" href="favicon.png">

    <!-- Social Share Nonsense -->
	<meta itemprop="name" content="Part 3: The Proposed Solutions">
	<meta itemprop="description" content="Chapter Three of ‚ÄúAI Safety for Fleshy Humans: a whirlwind tour‚Äù">
	<meta itemprop="image" content="https://aisafety.dance/thumbs/thumb-p3.png">
	<meta property="og:title" content="Part 3: The Proposed Solutions">
	<meta property="og:type" content="website">
	<meta property="og:image" content="https://aisafety.dance/thumbs/thumb-p3.png">
	<meta property="og:description" content="Chapter Three of ‚ÄúAI Safety for Fleshy Humans: a whirlwind tour‚Äù">
    <meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="Part 3: The Proposed Solutions">
	<meta name="twitter:description" content="Chapter Three of ‚ÄúAI Safety for Fleshy Humans: a whirlwind tour‚Äù">
	<meta name="twitter:image" content="https://aisafety.dance/thumbs/thumb-p3.png">

	<!-- STYLES -->
	<link rel="stylesheet" href="../styles/Merriweather/merriweather.css">
    <link rel="stylesheet" href="../styles/Open_Sans/opensans.css">
    <link rel="stylesheet" href="../styles/littlefoot.css"/> <!-- before page.css, so page can override it -->
	<link rel="stylesheet" href="../styles/page.css">

	<!-- SCRIPTS -->
    <!-- Littlefoot: for my feetnotes -->
    <script src="../scripts/littlefoot.js" ></script>
    <!-- Nutshell: expandable explanations -->
    <script src="../scripts/nutshell-v1.0.5.js"></script>
    <script> Nutshell.setOptions({ startOnLoad: false, /* Start AFTER footnotes loaded */ }); </script>
    <!-- MathJAX: for nice math -->
    <script src="../scripts/tex-mml-chtml.js"></script>
	<!-- This website's own scripts -->
    <script src="../scripts/page.js"></script>
    <!-- Hack Club's no-cookies, GDPR-compliant analytics -->
    <script defer data-domain="aisafety.dance" src="https://plausible.io/js/script.js"></script>

</head>
<body>

<!-- HACKBRAND -->
<a class="orpheus-flag" target="_blank" href="https://hackclub.com/">
	<img src="../styles/orpheus-flag.svg" width="560" height="315" alt="A project by Hack Club" aria-label="A project by Hack Club">
</a>

<!-- The Sidebar UI -->
<div id="return_to_content"></div>
<div id="sidebar">
	<div id="panel_toc"></div>

    <!-- STYLE CHANGER -->
	<div id="panel_style">

        <div id="style_dark_mode_container" style="cursor:pointer;">
            <input type="checkbox" id="style_dark_mode" style="pointer-events: none;">
            Dark Mode
        </div>
        <br>

        Font size:
        <span id="style_fontsize"></span>
        <br>
        <input type="range" id="style_fontsize_slider" min="10" value="19" max="40">
        <br>

        Font type:
        <br>
        <label>
            <input type="radio" name="style_font_family" value="serif" checked>
            <span style="font-family:'Merriweather'">Serif</span>
        </label>
        <br>
        <label>
            <input type="radio" name="style_font_family" value="sans_serif">
            <span style="font-family:'Open Sans'">Sans Serif</span>
        </label>
        <br><br>

        <button id="style_reset">Reset</button>

    </div>

    <!-- TRANSLATIONS -->
	<div id="panel_translations">
        <!-- none... sorry -->
    </div>
	<div id="panel_share">share on... w/e</div>
    <!-- SHILLING FOR BIG NICKY -->
	<div id="panel_sub">
    </div>
    <div id="panel_support"></div>

</div>

<!-- Reading Time Clock! -->
<div id="reading_time">
	<div id="clock_icon"></div>
	<div id="clock_label"></div>
</div>

<!-- EVERYTHING TO THE LEFT of the sidebar... -->
<div id="everything_container">

    <!-- A big cute header -->
    <div id="header" class="chapter">
        <div id="splash_image">

            
            

            <div id="crt_lines"></div>
            <div id="static"></div>

            

        </div>
        

        <div id="header_words">
            <div id="title">
                P<span style="position: relative;left: -7px;">A</span>RT
                THREE
            </div>
            <div id="subtitle">
                The Proposed Solutions
            </div>
        </div>

        
	</div>

    <!-- Chapter Navigation -->
    <div id="chapter_nav">
        <div id="chapter_nav_centered">
            <a target="_self" href="../"
                class="live">
                <div >
                    <span class='chapter-nav-desktop'>
                        introduction
                    </span>
                    <span class='chapter-nav-phone'>
                        intro
                    </span>
                </div>
            </a>
            <a target="_self" href="../p1"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        part 1<br>past & future
                    </span>
                    <span class='chapter-nav-phone'>
                        part 1
                    </span>
                </div>
            </a>
            <a target="_self" href="../p2"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        part 2<br>problems
                    </span>
                    <span class='chapter-nav-phone'>
                        part 2
                    </span>
                </div>
            </a>
            <a target="_self" href="#"
                title="COMING END OF NOVEMBER 2025"
                onclick="alert('COMING END OF NOVEMBER 2025')">
                <div>
                    <span class='chapter-nav-desktop'>
                        part 3<br>solutions?
                    </span>
                    <span class='chapter-nav-phone'>
                        part 3
                    </span>
                </div>
            </a>
            <a target="_self" href="#"
                title="COMING END OF NOVEMBER 2025"
                onclick="alert('COMING END OF NOVEMBER 2025')">
                <div style="border-right:1px solid rgba(128,128,128,0.8);">
                    <span class='chapter-nav-desktop'>
                        conclusion
                    </span>
                    <span class='chapter-nav-phone'>
                        outro
                    </span>
                </div>
            </a>
        </div>
    </div>

    <!-- The lil' tabs for sidebar UI -->
    <div id="sidebar_tabs">
		<div id="tab_toc">
			<div></div>
			table of contents
		</div>
		<div id="tab_style">
			<div></div>
			change style üòé
		</div>
        <!--
		<div id="tab_sub">
            CREDITS & Signup for notifications
			<div></div>
			subscribe üíñ
		</div>
        -->
	</div>

    <!-- BEHOLD! CONTENT!!!!! -->
	<article id="content">
<h3>This is a secret unlisted draft for beta-readers! Please don't share (yet), thank you for gifting me feedback! You'll be credited in the credits if you wish :3</h3>
<p><em>(This is Part 3 of a series on AI Safety! You don't</em> have <em>to read the previous parts ‚Äî <a href="https://aisafety.dance/">Intro</a>, <a href="https://aisafety.dance/p1/">Part 1</a>, <a href="https://aisafety.dance/p2/">Part 2</a> ‚Äî but they'll help!)</em></p>
<p>So, after writing 40,000+ words on how weird &amp; difficult AI Safety is... how am I feeling about the chances of humanity solving this problem?</p>
<p>...pretty optimistic, actually!</p>
<p>No, really!</p>
<p>Maybe it's just cope. But in my opinion, if <em>this</em> is the space of all the problems:</p>
<p><img src="../media/p3/intro/problems.png" alt="Drawing of a faintly outlined blob, labelled &quot;the whole problem space&quot;"></p>
<p>Then: although no <em>one</em> solution covers the whole space, <em>the entire problem space</em> is covered by one (or more) promising solutions:</p>
<p><img src="../media/p3/intro/solutions.png" alt="Same outline, but entirely overlapped by small colorful circles, each one representing a different solution"></p>
<p><strong>We don't need One Perfect Solution; we can stack several imperfect solutions!</strong> (This is similar to <a href="#SwissCheese">:the Swiss Cheese Model in Risk Analysis</a>. üëà <em>optional: click to expand dotted-underlined texts!</em>)</p>
<p><em>This does not mean AI Safety is 100% solved yet</em> ‚Äî we still need to triple-check these proposals, and get engineers/policymakers to even <em>know</em> about these solutions, let alone implement them. But for now, I'd say: &quot;lots of work to do, but lots of promising starts&quot;!</p>
<p>As a reminder, here's how we can break down the main problems in AI &amp; AI Safety:</p>
<p>// PICTODO: combo of both</p>
<p>So in this Part 3, we'll learn about the most-promising solution(s) for each part of the problem, while being honest about their pros, cons, and unknowns:</p>
<p><strong>ü§ñ Problems in the AI:</strong></p>
<ul>
<li><u>Scalable Oversight</u>: How can we safely check AIs, even when they're <em>far</em> more advanced than us? <a href="#oversight">‚Ü™</a></li>
<li><u>Solving AI Logic</u>: AI should aim for our &quot;future lives&quot; <a href="#future">‚Ü™</a>, and learn our values with uncertainty <a href="#uncertain">‚Ü™</a>.</li>
<li><u>Solving AI &quot;Intuition&quot;</u>: AI should be easy to &quot;read &amp; write&quot; <a href="#interpretable">‚Ü™</a>, be robust <a href="#robust">‚Ü™</a>, and think in cause-and-effect. <a href="#causality">‚Ü™</a></li>
</ul>
<p><strong>üò¨ Problems in the Humans</strong>:</p>
<ul>
<li><u>Humane Values</u>: Which values, <em>whose</em> values, should we put into AI, and how? <a href="#humane">‚Ü™</a></li>
<li><u>AI Governance</u>: How can we coordinate humans to manage AI, from the top-down and/or bottom-up? <a href="#governance">‚Ü™</a></li>
</ul>
<p><strong>üåÄ Working <em>around</em> the problems</strong>:</p>
<ul>
<li><u>Alternatives to AGI</u>: How about we just don't make the Torment Nexus? <a href="#alt">‚Ü™</a></li>
<li><u>Cyborgism</u>: If you can't beat 'em, join 'em! <a href="#cyborg">‚Ü™</a></li>
</ul>
<p>(If you'd like to skip around, the <img src="../media/intro/icon1.png" class="inline-icon"> Table of Contents are to your right! üëâ You can also <img src="../media/intro/icon2.png?v=3" class="inline-icon"> change this page's style, and <img src="../media/intro/icon3.png?v=2" class="inline-icon"> see how much reading is left.)</p></p>
<p>Quick aside: <strong>this final part, published on November 2025,</strong> was supposed to be posted 11 months ago. But due to a bunch of personal shenanigans I don't want to get into, I was delayed. Sorry you've waited almost a year for this finale! On the upside, there's been lots of progress &amp; research in this field since then, so I'm excited to share all that with you, too.</p>
<p>Alright, let's dive in! No need for more introduction, or weird stories about cowboy catboys, let's just‚Äî</p>
<h4>:x Swiss Cheese</h4>
<p><a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">The famous Swiss Cheese Model</a> from Risk Analysis tells us: <strong>you don't need <em>one</em> perfect solution, you can stack <em>several</em> imperfect solutions:</strong></p>
<p><img src="../media/p3/intro/cheese.png" alt="&quot;Rays going through layers of swiss cheese, which have holes in them. The rays can easily go through the holes of one layer of cheese, but not all of them.&quot;"></p>
<p><em>(image by <a href="https://en.wikipedia.org/wiki/File:Swiss_cheese_model_textless.svg">Ben Aveling</a> on Wikipedia (CC-BY-SA))</em></p>
<p>This model's been used <em>everywhere</em>, from aviation to cybersecurity to pandemic resilience. An imperfect solution has &quot;holes&quot;, which are easy to get through. But stack enough of them, with holes in different places, and it'll be near-impossible to bypass.</p>
<p>But, let's address two critiques of the Swiss Cheese model:</p>
<p><a href="https://www.eurocontrol.int/publication/revisiting-swiss-cheese-model-accidents">Critique One</a> ‚Äî this assumes the &quot;holes&quot; in the solutions are <em>independent</em>. If there's any <em>one</em> hole that all solutions share, then a problem can slip through all of them. Fair enough. <strong>This is why the Proposed Solutions on this page try to be as diverse as possible.</strong></p>
<p>Critique Two ‚Äî more relevant to AI Safety ‚Äî is it assumes the problem is not an intelligent agent. <a href="https://archive.is/20250921161137/https://www.vox.com/future-perfect/461680/if-anyone-builds-it-yudkowsky-soares-ai-risk">A quote from Nate Soares</a>:</p>
<blockquote>
<p>‚ÄúIf you ever make something that is trying to get to the stuff on the other side of all your Swiss cheese, it‚Äôs not that hard for it to just route through the holes.‚Äù</p>
</blockquote>
<p>I accept this counterargument when it comes to defending against an <em>already</em>-misaligned superintelligence. But if we're talking about <em>growing a trusted intelligence from scratch</em>, then the Swiss Cheese Model still makes sense at each step, <em>and</em>, you can use each iteration of a trusted AI as an extra &quot;layer of cheese&quot; for training better iterations of that AI. (See below section: Scalable Oversight)</p>
<p>As AI researcher Jan Leike <a href="https://aligned.substack.com/p/should-we-control-ai">puts it</a>:</p>
<blockquote>
<p>More generally, we should actually solve alignment instead of just trying to control misaligned AI. [...] Don‚Äôt try to imprison a monster, build something that you can actually trust!</p>
</blockquote>
<hr>
<p><a id="oversight"></a></p>
<h2>Scalable Oversight</h2>
<p>This is Sheriff Meowdy, the cowboy catboy:</p>
<p><img src="../media/p3/so/meowdy0001.png" alt="Drawing of Sheriff Meowdy"></p>
<p>One day, the Varmin strode into town:</p>
<p><img src="../media/p3/so/meowdy0002.png" alt="Sheriff Meowdy staring down a bunch of Jerma rats strolling towards him"></p>
<p>Sharpshootin' as the Sheriff was, he's man enough (catman enough) to admit when he needs backup. So, he makes a robot helper ‚Äî Meowdy 2.0 ‚Äî to help fend off the Varmin:</p>
<p><img src="../media/p3/so/meowdy0003.png" alt="Robot version of Sheriff Meowdy, labelled &quot;Meowdy 2.0&quot;"></p>
<p>Meowdy 2.0 can shoot twice as fast as the Sheriff, but there's a catch: Meowdy 2.0 <em>might</em> betray the Sheriff. Thankfully, it takes time to turn around &amp; betray the Sheriff, and the Sheriff is still fast enough to stop Meowdy 2.0 if it does that.</p>
<p>This is <strong>oversight.</strong></p>
<p><img src="../media/p3/so/meowdy0004.png" alt="Sheriff Meowdy watching 2.0, with a gun to its head. 2.0 can turn around in 500ms, Meowdy can react &amp; shoot in 200ms."></p>
<p>Alas, even Meowdy 2.0 <em>still</em> ain't fast enough to stop the millions of Varmin. So Sheriff makes Meowdy 3.0, which is twice as fast as 2.0, or <em>four times</em> as fast as the Sheriff.</p>
<p>This time, the Sheriff has a harder time overseeing it:</p>
<p><img src="../media/p3/so/meowdy0005.png" alt="3.0 can turn around in 250ms, Meowdy can only still react in 200ms. Meowdy is sweating."></p>
<p>But Meowdy 3.0 <em>still</em> ain't fast enough. So the Sheriff makes Meowdy 4.0, who's twice as fast as 3.0...</p>
<p>...and this time, it's so fast, the Sheriff can't react if 4.0 betrays him:</p>
<p><img src="../media/p3/so/meowdy0006.png" alt="4.0 can turn around in 125ms, which is fast enough to betray Meowdy. 4.0 shoots Meowdy dead."></p>
<p>So, what to do? The Sheriff strains all two of his orange-cat brain cells, and comes up with a plan: <strong><em>scalable</em> oversight!</strong></p>
<p>He'll oversee 2.0, which can oversee 3.0, which <em>can</em> oversee 4.0!</p>
<p><img src="../media/p3/so/meowdy0007.png" alt="Meowdy oversees 2.0 oversees 3.0 oversees 4.0"></p>
<p>In fact, why stop there? This harebrained &quot;scalable oversight&quot; scheme of his will let him oversee a Meowdy of <em>any</em> speed!</p>
<p>So, the Sheriff makes 20 Meowdy's. Meowdy 20.0 is 2<sup>20</sup> ~= one <em>million</em> times faster than the Sheriff: plenty quick enough to stop the millions of Varmin!</p>
<p><img src="../media/p3/so/meowdy0008.png" alt="Meowdy oversees a chain of Meowdy's, up to Meowdy 20.0, who can shoot all the Varmin dead."></p>
<p>(Wait, isn't a single chain of oversight fragile? As in, if <em>one</em> of the Meowdy's break, the entire chain is broken? Yes! One solution is to have <em>multiple</em> interwined chains, like so:)</p>
<p>// PICTODO</p>
<p>(This way, if any <em>one</em> overseer at Level N gets corrupted, there'll still be two others checking the bots at Level N+1. And the overseers at Level N-1 can catch &amp; fix the corrupted overseer. Also, it's important the overseers are independent from each other, so their failures aren't correlated. We'll learn more about how to create Robustness in a later section.)</p>
<p>Anyway, in sum, the core insight of scalable oversight is this meme:</p>
<p><img src="../media/p3/so/meme.png" alt="The &quot;domino&quot; meme, where a small domino knocks over a bigger one, which knocks over a bigger one, until it can knock over a huge domino."></p>
<p>(Another analogy: sometimes, boats are so big, the rudder <em>has its own smaller rudder</em>, called a <a href="https://en.wikipedia.org/wiki/Trim_tab#As_a_metaphor">trim tab</a>. This way, you can steer the small rudder, which steers the big rudder, which steers the whole boat.)</p>
<p>You may notice this is similar to the idea of &quot;recursive self-improvement&quot; for AI <em>Capabilities:</em> an advanced AI makes a <em>slightly more</em> advanced AI, which makes another more advanced AI, etc. Scalable Oversight is the same idea, but for AI <em>Safety:</em> one AI helps you align a slightly more advanced AI, etc!</p>
<p>(Ideas like these, where case number N helps you solve case number N+1, etc, are called &quot;inductive&quot; or &quot;iterative&quot; or &quot;recursive&quot;. Don't worry, you don't need to remember that jargon, just thought I'd mention it.)</p>
<p>Anywho: with the power of friendship, math, and a bad Wild West accent...</p>
<p>... the mighty Sheriff Meowdy has saved the townsfolk, once more!</p>
<p><img src="../media/p3/so/meowdy0010.png" alt="Sheriff Meowdy blowing the smoke away from his gun, as the injured Varmin waltz off into the sunset"></p>
<p>(üëâ <a href="#ScalableOversightExtras">: click to expand bonus section - the &quot;alignment tax&quot;, P = NP?, alignment vs control, what about sudden-jump &quot;sharp left turns&quot;?</a>)</p>
<p>. . .</p>
<p>Now that the visual hook is over, here's the quick sponsor intermission. This series was funded by <strong>Hack Club</strong> and the <strong>Long Term Future Fund</strong>; thank you both for helping me pay rent the last two years. <a href="https://hackclub.com/">Hack Club</a> helps teenage hackers around the world support each other. <a href="https://funds.effectivealtruism.org/funds/far-future">Long Term Future Fund</a> throws money at stuff that helps reduce P(doom). // TODO &quot;reach out if&quot;</p>
<p>Also, since this is the <em>last</em> entry in this series, if you wanna stay up to date on my future math/science/AI-explainer-y projects, you can sign up for <a href="https://www.youtube.com/@itsnickycase">my YouTube channel</a> or <a href="https://ncase.me/sub/">my once-a-month newsletter</a>:</p>
<iframe src="https://ncase.me/ncase-credits/signup2.html" frameborder="no" width="640" height="250"></iframe>
<p>Alright, back to the show!</p>
<p>. . .</p>
<p>A behind-the-scenes note: the above Sheriff Meowdy comic was the <em>first</em> thing in this series that I drew... <a href="https://www.patreon.com/posts/sheriff-meowdy-78016588">almost three years ago</a>. (Kids, don't do longform content on the internet, it ain't worth it.) Point is: learning about Scalable Oversight was <em>the one idea</em> that made me the most optimistic about AI Safety, and inspired me to start this whole series in the first place!</p>
<p>Because, Scalable Oversight turns <em>this</em> seemingly-impossible problem:</p>
<blockquote>
<p><em>&quot;How do you avoid getting tricked by something that's 100 times smarter than you?&quot;</em></p>
</blockquote>
<p>...into this much-more feasible problem:</p>
<blockquote>
<p><em>&quot;How do you avoid getting tricked by something that's only 10% smarter than you, and ALSO you can raise it from birth, read its mind, and nudge its brain?&quot;</em></p>
</blockquote>
<p>To be clear, the above problem isn't solved yet, either. But it's a huge step!</p>
<p><code>// PICTODO: all the above in one picture?</code></p>
<p>Anyway, that's the <em>general</em> idea. Here's some <em>specific</em> implementations &amp; empirical findings:</p>
<ul>
<li>üíñ‚û°Ô∏èüíñ <strong><a href="https://arxiv.org/abs/1811.07871">Recursive Reward Modeling</a></strong> uses a Level-N bot not just to check &amp; control a Level-(N+1) bot <em>in hindsight</em>, but to <em>train its intrinsic &quot;goals &amp; desires&quot; in the first place.</em><sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> As AI researcher Jan Leike <a href="https://aligned.substack.com/p/should-we-control-ai">put it</a>: <em>‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù</em></li>
<li>üìà <strong><a href="https://arxiv.org/abs/2504.18530">Scaling Laws For Scalable Oversight</a></strong> <em>quantitatively</em> measures how many oversight levels/steps you need to maximize oversight success. Encouragingly, like the previous &quot;scaling laws&quot; for how AI improves as you feed them more compute, these &quot;scaling laws of scalable oversight&quot; also seem to be predictable &amp; measurable. <a href="https://mikhailterekhov.github.io/control-tax/">Terekhov &amp; Liu et al 2025</a> goes further and calculates the &quot;control tax&quot; in $, for various AI attackers and defenders.</li>
<li>üïµÔ∏èü¶π <strong><a href="https://arxiv.org/abs/2312.06942">GPT-3.5 can catch sneaky, subtly-harmful code, written by the more-powerful GPT-4.
</a></strong></li>
<li>üïµÔ∏èü•∑ <strong><a href="https://openai.com/index/prover-verifier-games-improve-legibility/">Prover-Verifier Games</a></strong> pits a weak &quot;math proof verifier&quot; AI against two stronger AIs: a &quot;helpful prover&quot; that gives true proofs, and a &quot;sneaky prover&quot; that gives false proofs that <em>seem</em> true. Result of training on this game: the <em>weak verifier</em> can get good enough to distinguish true proofs, but the <em>strong sneaky prover can't win in the long run.</em> Nice!</li>
<li>ü™ú <strong><a href="https://openai.com/index/weak-to-strong-generalization/">Weak-to-Strong Generalization</a></strong> found that GPT-<em>2</em> did a pretty okay job at supervising &amp; training GPT-<em>4</em>. (an AI that's several orders of magnitude bigger)</li>
<li>üí¨üí¨ <strong><a href="https://openai.com/index/debate/">Debate</a></strong> is slightly different. Instead of a weaker AI overseeing a stronger AI, Debate has 2 equally-powerful AIs debate each other, picking apart each others' logic. As long as truth can survive scrutiny better than falsehood, the truth will win. (Well, maybe.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>)</li>
<li>üëØ‚Äç‚ôÄÔ∏è <strong><a href="https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate">Iterated Distillation &amp; Amplification (IDA)</a></strong> is even safer ‚Äì instead of overseeing AIs slightly-smarter than you, you oversee AIs that are <em>strictly weaker</em> than you. How, then, can this setup give you superhuman intelligence? To very loosely summarize: in IDA, you become the CEO of an organization made out of AIs trained to imitate you! Since every AI imitates you, and you're the CEO of the whole thing, the system as a whole is aligned to your values ‚Äî while <em>also</em> amplifying your capabilities. (<a href="#IDA">: More about IDA, and critiques of it</a>)</li>
</ul>
<p>Though, it's good to remain healthily critical. Here's some recent &quot;downer&quot; findings from monitoring Large Language Models (LLMs) like ChatGPT &amp; Claude: if you train an LLM <em>too</em> hard with a monitor that can read its chain-of-thought, it learns to hide its evil plans <em>even in its own &quot;thoughts&quot;</em>. <a href="https://arxiv.org/pdf/2503.11926">(Baker &amp; Huizinga 2025)</a>. And, given na√Øve oversight techniques, it's actually pretty easy for an LLM to jailbreak its monitor <a href="https://arxiv.org/pdf/2510.09462">(Terekhov, Panfilov &amp; Dzenhaliou 2025)</a></p>
<p>But even if <em>those</em> oversight methods fail, there's still plenty more! (As we'll see later in the Interpretability &amp; Steering section). Overall, I'm still optimistic: we don't need <em>one</em> perfect solution, you can stack lots of imperfect solutions. (<a href="#SwissCheese">: click here to read more about the Swiss Cheese Model, if you skipped it before</a>)</p>
<p>So: <em>IF</em> we can align a slightly-smarter-than-us AI, <em>THEN</em>, through Scalable Oversight, we can align far more advanced AIs.</p>
<p>...but right now, we can't even align <em>dumber</em>-than-us AIs.</p>
<p>That's what the next few proposed solutions aim to fix!  But first...</p>
<h4>:x Scalable Oversight Extras</h4>
<p>Some extra concepts &amp; comments that don't fit in the main post:</p>
<p><strong>&quot;Alignment Tax&quot;:</strong> how much extra overhead do you need to make sure the whole system remained aligned &amp; controllable? If the &quot;tax&quot; is too high, firms will be incentivized to cut corners, and risk a misaligned AI. So: how big is the &quot;tax&quot;?</p>
<p>In the above Meowdy example, to oversee a bot of speed $X$, because each bot can oversee a bot 2x faster than it, you need an extra $log_2(X)$ bots to safely control it. (Then, if you want $k$ intertwined parallel chains for robustness, add $k*log_2(X)$ bots.)</p>
<p>And in general, as the <a href="https://arxiv.org/abs/2504.18530">Scaling Laws for Scalable Oversight paper</a> shows, for any given task, bots can usually oversee a bot that's some fixed ratio &quot;better&quot; than it. If that's the case, then to oversee a bot of capability $X$, you need some logarithmic number of extra bots for scalable oversight.</p>
<p>Logarithms are the inverse of exponentials; so like how exponentials grow <em>really fast</em>, logarithms grow <em>really slow</em>. This is encouraging: this means our &quot;alignment tax&quot; overhead for powerful AIs grows slowly &amp; can be easily affordable!</p>
<p><strong>What if P = NP?</strong></p>
<p>The Scalable Oversight paradigm assumes that it's always easier to <em>verify</em> solutions, than to <em>create</em> solutions: this is how a weaker AI can verify the safety/correctness of the actions of a stronger AI. For example: it's hard to solve a Rubik's cube or Sudoku puzzle, but nearly trival to <em>check</em> if one is solved.</p>
<p>However: there's a currently open question in computer science, with a million-dollar prize attached: P = NP? In sum, it asks: are all problems that are <em>easy to check</em> secretly <em>easy to solve?</em> It seems intuitively not (and most computer scientists believe it's false, that is, P ‚â† NP) but it's still not been proven. As far as we know, it <em>could</em> be the case that P = NP, and so that every problem that's easy to check is also easy to solve.</p>
<p>Does this mean, if P=NP, the Scalable Oversight paradigm fails? No! Because P = NP &quot;only&quot; means that it's not <em>exponentially</em> harder to find solutions than check solutions. (or, to be precise: it's only &quot;polynomially&quot; harder at best, that's the &quot;P&quot; in &quot;P&quot; and &quot;NP&quot;.) But finding a solution <em>is still harder</em>, just not exponentially so.</p>
<p>Two examples, where we've <em>proven</em> how much time an optimal solution takes:</p>
<ul>
<li>The optimal way to sort a list takes $\mathcal{O}(n\log{}n)$ time, while checking a list is sorted takes $\mathcal{O}(n)$ time.</li>
<li>The optimal way <a href="https://en.wikipedia.org/wiki/Grover%27s_algorithm">to find a solution to a black-box problem on a quantum computer</a> takes $\mathcal{O}(\sqrt{n})$ time, but checking that solution takes a constant $\mathcal{O}(1)$ time.</li>
</ul>
<p>($\mathcal{O}(\text{formula})$ means &quot;in the long run, the time this process takes is proportional to this formula.&quot;)</p>
<p>So even if P = NP, as long as it's <em>harder</em> to find solutions than check them, Scalable Oversight can work. (But the alignment tax will be higher)</p>
<p><strong>Alignment vs Control:</strong></p>
<p>Aligned = the AI's &quot;goals&quot; are the same as ours.</p>
<p>Control = we can, well, control the AI. We can adjust it &amp; steer it.</p>
<p>Some of the below papers are from the sub-field of &quot;AI Control&quot;: how do we control an AI <em>even if it's misaligned?</em> (As shown in the Sheriff Meowdy example, the Meowdy bots will shoot him the moment they can't be controlled. So, they're misaligned.)</p>
<p>To be clear, folks in the AI Control crowd recognize it's not the &quot;ideal&quot; solution ‚Äî as AI researcher Jan Leike <a href="https://aligned.substack.com/p/should-we-control-ai">put it</a>, <em>‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù</em> ‚Äî but it's still worth it as an extra layer of security.</p>
<p>Interestingly, it's also possible to have Alignment <em>without</em> Control: you could imagine an AI that <em>correctly</em> learns humane values &amp; what flourishing for all sentient beings looks like, then takes over the world as a benevolent dictator. It understands that we'll be uncomfortable ceding control, but it's worth it for world peace, and will rule kindly. (And besides, 90% of you keep fantasizing about living in land of kings &amp; queens anyway, admit it, you humans <em>want</em> to be ruled by a dictator. /half-joke)</p>
<p><strong>Sharp Left Turns:</strong></p>
<p>Scalable Oversight also depends on capabilities smoothly scaling up. And not something like, &quot;if you make this AI 1% smarter, it'll gain a brand new capability that lets it absolutely crush an AI that's even 1% weaker than it.&quot;</p>
<p>This possibility sounds absurd, but there's precedent for sudden-jump &quot;phase transitions&quot; in physics: slightly below 0¬∞C, water becomes ice. And slightly above 0¬∞C, water is liquid. So could there be such a &quot;phase transition&quot;, a &quot;sharp left turn&quot;, in intelligent systems?</p>
<p>Maybe? But:</p>
<ol>
<li>
<p>Even in the physics example, ice doesn't freeze <em>instantly;</em> you can feel it getting colder, and you have hours or days to react before it fully freezes over. So, even if a &quot;1% smarter AI&quot; gains a radically new capability, the &quot;1% dumber overseer&quot; may still have time to notice &amp; stop it.</p>
</li>
<li>
<p>As you'll see later in this section, the <em>is</em> a Scalable Oversight proposal, called Iterated Distillation &amp; Amplification, where overseers oversee only <em>strictly &quot;dumber&quot;</em> AIs, yet the system as a whole can still be smarter! Read on for details.</p>
</li>
</ol>
<h4>:x IDA</h4>
<p>To understand Iterated Distillation &amp; Amplification (IDA), let's consider its biggest success story: AlphaGo, the first AI to beat a world champion at Go.</p>
<p>Here were the steps to train AlphaGo:</p>
<ul>
<li>Start with a dumb, random-playing Go AI.</li>
<li><strong>DISTILL:</strong> Have two copies play against each other. Through self-play, <em>learn an &quot;intuition&quot; for good/bad moves &amp; good/bad board states.</em> (using an <a href="https://en.wikipedia.org/wiki/Neural_network_(machine_learning)">artificial neural network</a>)</li>
<li><strong>AMPLIFY:</strong> Plug this &quot;intuition module&quot; into a Good Ol' Fashioned AI, that simply thinks a few moves &amp; counter-moves ahead, then picks the next best move. (<a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a>) This gives you a slightly-less-dumb Go AI.</li>
<li><strong>ITERATE:</strong> Repeat. The two less-dumb AIs play against each other, learn a better &quot;intuition&quot;, thus get better at game tree search, and thus get better at playing Go.</li>
<li>Repeat over and over  until your AI is superhuman at Go!</li>
</ul>
<p>// PICTODO</p>
<p>Even more impressive, this same system could also learn to be superhuman at chess &amp; shogi (&quot;Japanese chess&quot;), <em>without ever learning from endgames or openings</em>. Just lots and lots of self-play.</p>
<p>( A caveat: the resulting AIs are only as robust as ANNs are, which aren't very robust. A superhuman Go AI can be beaten by a &quot;bad&quot; player who simply tries to take the AI into insane board positions that would never naturally happen, in order to break the AI. (<a href="https://proceedings.mlr.press/v202/wang23g/wang23g.pdf">Wang &amp; Gleave et al 2023</a>) )</p>
<p>Still, this is strong evidence that IDA works. But even better, as <a href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Paul Christiano pointed out &amp; proposed</a>, IDA could be used for scalable Alignment.</p>
<p>Here's a paraphrase of how it'd work:</p>
<ul>
<li>Start with you, the Human</li>
<li><strong>DISTILL:</strong> Train an AI to imitate <em>you</em>, your values, trade-offs, and reasoning style. This AI is <em>strictly weaker</em> than you, but can be run faster.</li>
<li><strong>AMPLIFY:</strong> You want to solve a big problem? Carve up the problem into smaller parts, hand them off to your Slightly-Dumber-But-Much-Faster AI clones, then recombine them into a full solution. (For example: I want to solve a math problem. I come up with <em>N</em> different approaches, then ask <em>N</em> clones to try out each one, then report what they learn. I read their reports, then if it's still not solved, I think up of <em>N</em> more possible approaches &amp; ask the clones to try again. Repeat until solved.)</li>
<li><strong>ITERATE:</strong> For the next distillation step, train an AI to imitate <em>the you + clones system as a whole</em>. Then for the next amplification step, you can query multiple clones of <em>that</em> system to help you break down &amp; solve big problems.</li>
<li>Repeat until you are the CEO of a superhuman &quot;company of you-clones&quot;!</li>
</ul>
<p>// PICTODO</p>
<p>I think IDA is one of the cooler &amp; more promising proposals, but it's worth mentioning a few critiques / unknowns:</p>
<ul>
<li>Distillation: As shown in the above AlphaGo example, IDA's quality is limited by the Distillation step. Right now we don't know how to make robust ANNs, and we don't know if this Distillation step would preserve your values enough.</li>
<li>Amplification: While it seems most big problems in real life can be broken up into smaller tasks (this is why engineering teams aren't just one person), it's unclear if <em>epiphanies</em> can be broken up &amp; delegated. Maybe you really do need <em>one</em> person to store all the info in their head, as fertile soil &amp; seeds to grow new insights, and you can't &quot;carve up&quot; the epiphany process, any more than you can carve up a plant and expect it to still grow.</li>
<li>Iteration: Even if a <em>single distill-and-amplify</em> step more-or-less preserves your values, it's unknown if any errors would <em>accumulate</em> over multiple steps, let alone if the error grows exponentially. (As you may be painfully aware if you've ever worked in a big organization, an org can grow to be <em>very</em> misaligned from the original founders' values.)</li>
</ul>
<p>Also, if <em>you</em> don't get along well with yourself, <a href="https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fufid3duzqw991.jpg">becoming the &quot;CEO of a company of you's&quot; will backfire</a>.</p>
<p>(See also: <a href="https://www.youtube.com/watch?v=v9M2Ho9I9Qo">this excellent Rob Miles video on IDA</a>)</p>
<h3>ü§î (Optional!) Flashcard Review #1</h3>
<p>You read a thing. You find it super insightful. Two weeks later you forget everything but the vibes.</p>
<p>That sucks! So, here are some <em>100% OPTIONAL</em> Spaced Repetition flashcards, to help you remember these ideas long-term! ( üëâ <a href="https://aisafety.dance/#SpacedRepetition">: Click here to learn more about spaced repetition</a>) You can also download these as an Anki deck. TODO</p>
<pre><code>We don't need One Perfect Solution;
we can stack several imperfect solutions!

The Swiss Cheese Model TODO

What is Scalable Oversight?
Using an AI to oversee a slightly-smarter AI, repeat
// meme

How can you make scalable oversight more robust?
By adding extra, independent overseers
// chains

**Scalable Oversight** lets us convert the impossible question, &quot;How do you oversee a thing that's 1000x smarter than you?&quot; to the more feasible, &quot;How do you oversee a thing that's only a bit smarter than you, _and_ you can train it from scratch, read its mind, and nudge its thinking?&quot;
</code></pre>
<p>Good? Let's move on...</p>
<hr>
<p><a id="future"></a></p>
<h2>AI Logic: Future Lives</h2>
<p>You may have noticed a pattern in AI Safety paranoia.</p>
<p>First, we imagine giving an AI an innocent-seeming goal. Then, we think of a bad way it could <em>technically</em> achieve that goal. For example:</p>
<ul>
<li><u>&quot;Pick up dirt from the floor&quot;</u> ‚Üí Knocks all the potted plants over so it can pick up more dirt.</li>
<li><u>&quot;Calculate digits of pi&quot;</u> ‚Üí Deploys a computer virus to steal as much compute power as possible, to calculate digits of pi.</li>
<li><u>&quot;Help everyone feel happy &amp; fulfilled&quot;</u> ‚Üí Hijacks drones to airdrop aerosolized LSD and MDMA.</li>
</ul>
<p>Note: these are <em>NOT</em> problems with the AI being sub-optimal. These are problems <em>because</em> the AI is acting optimally! (We'll deal with sub-optimal AIs later.) Remember, like a cheating student or disgruntled employee, it's not that the AI may not &quot;know&quot; what you really want, it's that it may not &quot;care&quot;. (To be less anthropomorphic: a piece of software will optimize for exactly what you coded it to do. No more, no less.)</p>
<p><strong>&quot;Think of the worst that could happen, in advance. Then fix it.&quot;</strong> If you recall, this is <a href="https://aisafety.dance/p2/#:~:text=Does%20all%20this%20seem%20paranoid">Security Mindset</a>, the engineer mindset that makes bridges &amp; rockets safe, and makes AI researchers so worried about advanced AI.</p>
<p>But what if... we made an AI that <em>used Security Mindset against itself?</em></p>
<p>For now, let's assume an &quot;optimal&quot; AI ‚Äî (again, we'll tackle sub-optimal AIs later) ‚Äî that can predict the world perfectly. And since <em>you're</em> part of the world, it can predict <em>how you'd react to various outcomes</em> perfectly.</p>
<p><strong>Then, here's the &quot;Future Lives&quot; algorithm:</strong></p>
<ol>
<li>Human asks Robot to do something</li>
<li>Robot considers its possible actions, and the results of those actions</li>
<li>Robot predicts how <em>the current version of you</em> would react to <em>those futures</em>.</li>
<li>It does the action whose future you'd most approve of. Crucially, like in Security Mindset, it does <em>not</em> do things that would make current-you say, &quot;no that's not what I wanted&quot;.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup></li>
</ol>
<p>(Note: Why predict how <em>current</em> you would react, not future you? To avoid an incentive to &quot;wirehead&quot; you into a dumb brain that's maximally happy. Why a whole future, not just an outcome at a point in time? To avoid unwanted means towards those ends, and/or unwanted consquences after those ends.)</p>
<p>(Note 2: For now, we're also just tackling the problem of how to get an AI to fulfil <em>one</em> human's values, not <em>humane</em> values. We'll look at the &quot;humane values&quot; problem later in this article.)</p>
<p>// PICTODO example - clean my house</p>
<p>As Stuart Russell, the co-author of the most-used textbook in AI, once put it:<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<blockquote>
<p>[Imagine] if you were somehow able to watch two movies, each describing in sufficient detail and breadth a future life you might lead [as well as the consequences outside of &amp; after your life.] You could say which you prefer, or express indifference.</p>
</blockquote>
<p>(These kinds of approaches ‚Äî where instead of directly telling an AI our values, we ask it to <em>learn</em> what our values are ‚Äî is called <strong>&quot;indirect normativity&quot;</strong>. It's called that because <s>academics are bad at naming things</s> &quot;normativity&quot; ~means &quot;values&quot;, and &quot;indirect&quot; because we're showing it, not telling it.)</p>
<p>And voil√†! That's how we make an (optimal) AI use Security Mindset <em>on itself:</em></p>
<p><strong>If we could <em>even in principle</em> come up with a Security Mindset-style problem with an AI's action, this AI would already predict that, and avoid doing that!</strong> <em>‚ÄúIf we scream, the rules change; if we predictably scream later, the rules change now.‚Äù</em><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></p>
<p>. . .</p>
<p><em>Hang on</em>, you may think, <em>I can already think of ways the Future Lives approach can go wrong, even with an optimal AI:</em></p>
<ul>
<li>This locks us in into our <em>current</em> values, no room for personal/moral growth.</li>
<li>Whether or not we approve of something is sensitive to psychological tricks, e.g. seeing a thing for &quot;$20&quot;, versus &quot;<s>$50</s> $20 (SALE: $30 OFF!!!)&quot;. The &quot;movies&quot; of possible future lives could be filmed in an emotionally manipulative way.</li>
<li>If the truth is upsetting ‚Äî like when we discovered Earth wasn't the center of the universe ‚Äî the current-us would disapprove of learning about uncomfortable truths.</li>
<li>I contain multitudes, I contradict myself. What happens if, when presented different pairs of futures, I'd prefer A over B, B over C, <em>and C over A?</em> What if at Time 1 I want one thing, at Time 2 I predictably want the opposite?</li>
</ul>
<p>If you think these would be problems... you'd be correct!</p>
<p>In fact, since <em>you right now</em> can see these problems‚Ä¶ an AI with the &quot;apply Security Mindset to itself&quot; algorithm would <em>also</em> see those problems, and modify its own algorithm to fix those! (<a href="#FutureLivesFixes">: Examples of possible fixes</a>)</p>
<p>Consider the parallel to recursive self-improvement for AI Capabilities, and scalable oversight in AI Safety. <strong>You don't need to start with the perfect algorithm. You just need an algorithm that's good enough, a &quot;critical mass&quot;, to self-improve into something better and better.</strong> You &quot;just&quot; need to let it go meta.</p>
<p>// PICTODO: critical mass</p>
<p>(<em>Then</em> you may think, wait, but what about problems with repeated self-modification? What if it loses its alignment or goes unstable? Again, if <em>you</em> can notice these problems, this (optimal) AI would too, and fix them. &quot;AIs &amp; Humans under self-modification&quot; is an active area of research with lots of juicy open problems, <a href="#AISelfMod">: click to expand a quick lit review</a>)</p>
<p>Aaaand we're done! AI Alignment, <em>solved!</em></p>
<p>. . .</p>
<p>...in theory. Again, all the above <em>assumes an optimal AI</em>, which can perfectly predict all possible futures of the world, including you. This is, to understate it, infeasible.</p>
<p>Still, it's good to solve the easier ideal case first, before moving onto the harder messy real-life cases. Up next, we'll see proposals on how to get a sub-optimal, &quot;bounded rational&quot; AI, to implement the Future Lives approach!</p>
<h4>:x Future Lives Fixes</h4>
<p>Again, the following is meant as an <em>illustration</em> that it's possible for a Future Lives AI, <em>applying Security Mindset to itself</em>, would be able to fix its own problems. I'm not claiming the following is a perfect solution (though I <em>do</em> claim they're pretty good):</p>
<p><strong>Re: Value lock-in, no personal/moral growth.</strong></p>
<p>Wouldn't I, in 2026, be resentful that this AI is still trying to enact plans approved of me-from-2025? Won't I predictably hate being tied to my past, less-wiser self?</p>
<p>Well, me-from-2025 does <em>not</em> like the idea of all future me's still being <em>fully</em> tied to the whims of less-wise current-me. But I <em>do</em> want an AI to help me carry out my long-term goals, even if future-me's feel some pain (no pain no gain). But I also do <em>not</em> want to torture a large proportion of future-me's just because current-me has a dumb dream. (e.g. if current me thinks being a tortured artist is &quot;romantic&quot;.)</p>
<p>So, one possible modification to the Future Lives algorithm: consider not just current me, but a <em>Weighted</em> Parliament Of Me's. e.g. current-me gets the largest vote, me +/- a year get second-largest votes, me +/- 2 years get third-largest votes, etc. So this way, actions are picked that I, across my whole life, would mostly endorse. (With extra weight on current-me because, well, I'm a <em>little</em> selfish.)</p>
<p>(Actually, why stop at just <em>me</em> over time? There's people I love intrinsically for their own sake; I could also put their past/present/future selves on this virtual &quot;committee&quot;.)</p>
<p><strong>Re: Psychological manipulation</strong></p>
<p>Well, do I <em>want</em> to be psychologically manipulated?</p>
<p>No, duh. The tricky part is what do I consider to be manipulation, vs <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE">legitimate value change</a>? We may as well start with an approximate list.</p>
<ul>
<li>I'd approve of my beliefs/preferences/values being changed via: robust scientific evidence, robust logical argument, debate where all sides are steelmanned, safe exposure to new art &amp; cultures, learning about people's life experiences, standard human therapy, &quot;light&quot; drugs/supplements like kava or vitamin D, etc.</li>
<li>I would NOT approve of my beliefs/preferences/values being changed via: wireheading, drugging, &quot;direct revelation&quot; from God or LSD or DMT Aliens, sneaky framings like &quot;<s>$50</s> $20 (SALE: $30 OFF!!!)&quot;, lies, lying-by-omission, misleadingly-presented truths, etc.</li>
</ul>
<p>Most importantly, this list of &quot;what's legitimate change or not&quot; <em>IS ABLE TO MODIFY ITSELF</em>. For example, right now I endorse scientific reasoning but not direct revelation. But if science <em>proves</em> that direct revelation is reliable ‚Äî for example, <a href="https://andzuck.com/blog/dmt-primes/">if people who take DMT and talk to the DMT Aliens can do superhuman computation</a> or know future lottery numbers ‚Äî <em>then</em> I would believe in direct revelation.</p>
<p>I don't have a nice, simple rule for what counts as &quot;legitimate value change&quot; or not, but as long as I have a rough list, <em>and the list can edit itself</em>, that's good enough in my opinion.</p>
<p>(Re: Russell's &quot;watch two movies of two possible futures&quot;, maybe upon reflection I'd think a movie has too much room for psychological manipulation, and I'd rather the AI give me &quot;two Wikipedia articles of two possible futures&quot;. Depends on you; your mileage may vary.)</p>
<p><strong>Re: We'd disapprove of learning about upsetting truths</strong></p>
<p>Well, do I <em>want</em> to be the kind of person who shies away from upsetting truths?</p>
<p>Mostly no. (Unless these truths are Eldritch mind-breaking, or are just useless &amp; upsetting for no good reason.)</p>
<p>So: a self-improving Future Lives AI should predict I <em>do not want</em> ignorant bliss. But I'd like painful truths told in the least painful way; &quot;no pain no gain&quot; doesn't mean &quot;<em>more</em> pain more gain&quot;.</p>
<p>But, a paradox: I'd want to be able to &quot;see inside the AI's mind&quot; in order to oversee it &amp; make sure it's safe/aligned. But the AI needs to know the upsetting truth <em>before</em> it can prepare me for it. But if I can read its mind, I'll learn the truth <em>before</em> it can prepare me for it. How to resolve this paradox?</p>
<p>Possible solutions:</p>
<ul>
<li>The AI, before investigating a question that <em>could</em> lead to an upsetting truth, first prepares me for either outcome. <em>Then</em> it investigates, and tells me in a tactful manner.</li>
<li>Let go of direct access to the AI's mind. Use a Scalable Oversight thing where a trusted intermediary AI can check that the truth-seeking AI is aligned, but I don't directly see the upsetting truth <em>until</em> I'm ready.</li>
</ul>
<p><strong>Re: We don't <em>have</em> consistent preferences</strong></p>
<p>Well, what do I <em>want</em> to happen if I have inconsistent preferences at one point in time (A &gt; B &gt; C &gt; A, etc) or across time (A &gt; B now, B &gt; A later)?</p>
<p>At one point in time: for concreteness, let's say I'm on a dating app. I reveal I prefer Alyx to Beau, Beau to Charlie, Charlie to Alyx. Whoops, a loop. What do I <em>want</em> to happen then? Well, first, I'd like the inconsistency brought to my attention. Maybe upon reflection I'd pick one of them above all, or, I'd call it a three-way tie.</p>
<p><em>Across</em> time: this is a trickier case. For concreteness, let's say current-me wants to run a marathon, but if I start training, later-me will <em>predictably</em> curse current-me for the blistered feet and bodily pain‚Ä¶ but later-<em>later</em>-me will find it meaningful &amp; fulfilling. How to resolve? Possible solution: consider not just current me, but a (Weighted) Parliament Of Me's. (In this case, yes: current me &amp; far-future me's would find the marathon fulfilling, while &quot;only&quot; the me's during training suffer.)</p>
<h4>:x AI Self Mod</h4>
<p>A quick, informal, not-comprehensive review of the &quot;AIs that can modify themselves and/or Humans&quot; literature:</p>
<ul>
<li>The fancy phrase for this is &quot;embedded agency&quot; TODO, because there's no hard line betwen the agent &amp; its environment: an agent <em>can act on itself.</em></li>
<li>The &quot;tiling agents&quot; problem TODO in Agent Foundations investigates: how can we <em>prove</em> that a property of an AI is maintained, even after it modifies itself over and over? (i.e. does the property &quot;tile&quot;)</li>
<li>This paper TODO finds that, yes, for an <em>optimal</em> AI, as long as it judges <em>future</em> outcomes by its <em>current</em> utility function, it won't wirehead to &quot;REWARD = INFINITY&quot;, and will preserve its own goals/alignment, for better &amp; worse.
<ul>
<li>(A different paper TODO shows that <em>bounded-rational</em> AIs would get exponentially corrupted, but their paper only considers bounded-rational AIs <em>that do not &quot;know&quot; they're bounded-rational</em>.)</li>
<li>(If you'll excuse the self-promo, I'm slowly working on a research project TODO investigating if bounded-rational AIs that <em>know</em> they're bounded-rational can avoid corruption. I suspect easily so: a self-driving car that doesn't know its sensors are noisy will drive off a cliff, a self-driving car that <em>knows</em> its senses are fallible will account for a margin of error, and stay a safe distance away from a cliff even if its doesn't know <em>exactly</em> where the cliff is.)</li>
</ul>
</li>
<li>The research from Functional Decision Theory TODO &amp; Updateless Decision Theory TODO also finds that a standard &quot;causal&quot; agent <em>will choose to modify to be &quot;acausal&quot;</em>. Because it <em>causes</em> better outcomes to not be limited by mere causality.</li>
<li>Nora Ammann named &quot;the value change problem&quot; TODO: we'd like AIs that can help us adopt true beliefs, improve our mental health, do moral reflection, and expand our artistic taste. In other words: we <em>want</em> AI to modify us. But we don't want it to do so in &quot;bad&quot; ways, eg manipulation, brainwashing, wireheading, etc. So, open research question: how do we formalize &quot;legitimate&quot; value change, vs &quot;illegitimate&quot;?</li>
<li>TODO et al takes the traditional framework for understanding AIs, the Markov Decision Process, and extends it to cases where <em>the AI's or Human's &quot;beliefs and values&quot; can themselves be intentionally altered.</em>, dubbing this the Dynamic Reward Markov Decision Process. The paper finds that there's no obviously perfect solution, and we face not just technical challenges, but <em>philosophical</em> challenges.</li>
<li>The Causal Incentives Working Group TODO uses cause-and-effect diagrams to figure out when an AI has an &quot;incentive&quot; to modify itself, or modify the human. The group has had some empirical success, too, in correctly predicting &amp; designing AIs that do <em>not</em> manipulate human values, yet can still learn &amp; serve them..</li>
</ul>
<h3>ü§î Review #2</h3>
<p>(Again, <em>optional</em> flashcard review:)</p>
<pre><code>The pattern in AI Safety paranoia:

Security Mindset:

- Imagine giving an AI an innocent-seeming goal
- Think of a bad way it could _technically_ achieve that goal

---

The &quot;Future Lives&quot; approach is an AI that applies \_\_\_\_\_\_ against itself

Security Mindset

&quot;Think of the worst that could happen, in advance. Then fix it.&quot;

---

The &quot;Future Lives&quot; algorithm, in sum:

Robot predicts the future results of possible actions, and how _current you_ would react to _those futures_.

Robot picks the action whose future you'd most approve of, and *not* the futures that current-you would disapprove of.

---

The &quot;Future Lives&quot; approach, as described by Stuart Russell (you don't need to remember the quote exactly, just paraphrase it:)

&gt; \[Imagine\] if you were somehow able to **watch two movies**, each describing in sufficient detail and breadth **a future life** you might lead \[as well as the consequences outside of &amp; after your life.\] **You could say which you prefer, or express indifference.**

---

What does &quot;indirect normativity&quot; mean?

An approach where, instead of directly telling an AI our values, we ask it to _learn_ what our values are.

---

Why may we NOT need a perfect alignment algorithm to start with?

Like recursive self-improvement in AI Capabilities, above a certain &quot;critical mass&quot;, the alignment algorithm *can improve itself* to fix its flaws. (Let it go meta!)
</code></pre>
<hr>
<p><a id="uncertain"></a></p>
<h2>AI Logic: Know You Don't Know</h2>
<p>Classic logic is only True or False, 100% or 0%, All or Nothing.</p>
<p><em>Probabilistic</em> logic is about, well, probabilities.</p>
<p>I assert: probabilistic thinking is better than all-or-nothing thinking. (with 98% probability)</p>
<p>Let's consider 3 cases, with a classic-logic Robot:</p>
<ul>
<li><u>Unwanted optimization</u>: You instruct Robot, &quot;make me happy&quot;. <em>It will then be 100% sure that's your full and only desire</em>, so it pumps you with bliss-out drugs &amp; you do nothing but grin at a wall forever.</li>
<li><u>Unwanted side-effects</u>: You instruct Robot to close the window. Your cat's in the way, between Robot and the window. <em>You said nothing about the cat, so it's 0% sure you care about the cat.</em> So, on the way to the window, Robot steps on your cat.</li>
<li><u>&quot;Do what I mean, not what I said&quot; can still fail</u>: There's a grease fire. You instruct Robot to get you a bucket of water. You actually <em>did</em> mean for a bucket of water, but you didn't know <a href="https://www.reddit.com/r/lifehacks/comments/17t48a3/how_you_should_and_shouldnt_extinguish_an_oil_fire/">water causes grease fires to explode</a>. Even if Robot did &quot;what you meant&quot;, it'll give you a bucket of water, then you explode.</li>
</ul>
<p>In all 3 cases, the problem is that the AI was 100% sure what your goal was: exactly what you said or meant, <em>no more, no less</em>.</p>
<p>The solution: make AIs <em>know they don't know</em> our true goals! (Heck, <em>humans</em> don't know their own true goals.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>) AIs should think in probabilities about what we want. Then, like in Security Mindset, an AI should aim for a satisfactory <em>worst-case</em>, not just a satisfactory average-case. (<a href="#DistributionallyRobust">: expand more details on &quot;Distributionally Robust Optimization&quot;</a>)</p>
<p>In other words: <strong>learn like a scientist, act like an insurance agent!</strong> (uncertainty + worst-case planning)</p>
<p>// PICTODO</p>
<p>In more detail:</p>
<p><strong>1Ô∏è‚É£:</strong> <strong>First, an AI should start off with a good-enough &quot;prior probability distribution&quot; of what you want.</strong> It should not be 100% sure of anything, but its first draft of &quot;what do you really value&quot; shouldn't be <em>unrecoverably</em> wrong.</p>
<p>(If the priors are unrecoverably wrong‚Ä¶ then, yeah, this approach won't work.TODO CITE In my opinion the solution is just &quot;well don't do that.&quot; You can get an okay first-draft approximation of &quot;what people want&quot; from our stories, our moral laws, how we live our lives, etc. But again, <em>do not be 100% certain.</em>)</p>
<p><strong>2Ô∏è‚É£:</strong> <strong>Everything you (the Human) say or do afterwards, is then a <em>clue</em> to what you truly want, not the full 100%-certain truth.</strong> This accounts for: forgetfulness, procrastination, mis-speaking, sarcasm, you not knowing your own wants, lies you tell to others or yourself. Like a scientist, the AI tests &amp; updates its hypotheses, and gradually learns what you Truly Value.</p>
<p>(The theoretical ideal way to do this is with <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayesian Inference</a>, but the ideal is infeasible in practice, but there's encouraging research on how to <em>approximate</em> Bayesian Inference with ANNs<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>)</p>
<p><strong>3Ô∏è‚É£:</strong> <strong>When acting, the AI should do what's best <em>in the (plausible) worst-case scenario</em></strong>, not just the most-likely or average-case scenario.</p>
<p>(To clarify: &quot;do what's best&quot; as in the Future Lives algorithm: what action would lead to a future that <em>current</em>-you would most approve of. Again, this is to avoid unwanted mind-alterations, and unwanted means &amp; extra consequences.)</p>
<p>This <em>automatically</em> leads to: asking for clarification, avoiding side-effects, maintaining options and ability to undo actions, etc! We don't have to pre-specify all that. <strong>&quot;Maximizing the plausible-worst-case&quot; gives us all that for <em>free!</em></strong> (<a href="#WorstOrAverage">: bonus - worst-case vs average-case vs Pascal's Muggings?</a>)</p>
<p>. . .</p>
<p>In case &quot;aim at a goal that <em>you know you don't know</em>&quot; still sounds paradoxical, here's a two more examples to de-mystify it:</p>
<ul>
<li>üö¢ The game of <a href="https://en.wikipedia.org/wiki/Battleship_(game)">Battleship</a>. The goal is to hit the other players' ships, but you don't <em>know</em> where those ships are. But with each reported hit/miss, you slowly (but with uncertain probability) start learning where the ships are. Likewise: an AI's goal is to fulfil your values, which it knows it doesn't know, but with each hit/miss it gets a better idea.</li>
<li>üíñ Let's say I love Alyx, so I want to buy a capybara plushie for their birthday. But I then learn that they hate capybaras, because a capybara killed their father. So, I buy Alyx a sacabambaspis plushie instead. This seems like a silly example, but it proves that: 1) an agent can value another agent's values, 2) while knowing <em>it can misunderstand</em> those values, 3) and be able to <em>easily correct</em> its understanding.</li>
</ul>
<p>. . .</p>
<p>Okay, but what are the <em>actually concrete proposals</em> to &quot;learn a human's values&quot;? Here's a quick rundown:</p>
<ul>
<li>üê∂ <strong>Inverse Reinforcement Learning (IRL)</strong>.  &quot;Reinforcement learning&quot; (RL) is like training a dog with treats: given a &quot;reward function&quot;, the dog (or AI) learns what actions to do. <em>Inverse</em> Reinforcement Learning (IRL) is like figuring out what someone really cares about, by watching what they do: given observed actions, you (or an AI) learns what the &quot;reward function&quot; is. So, in the IRL approach: we let an AI learn our values by observing what we actually choose to do.</li>
<li>ü§ù <strong>Cooperative Inverse Reinforcement Learning (CIRL).</strong> Similar to IRL, except the Human isn't just being passively observed by an AI, the Human <em>actively</em> helps teach the AI.</li>
<li>üßë‚Äçüè´ <strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This was the algorithm that turned &quot;base&quot; GPT (a fancy autocomplete) into <em>Chat</em>GPT (an actually useable chatbot).
<ul>
<li>Step one: given human ratings üëçüëé on a bunch of chats, train a &quot;teacher&quot; AI to imitate a human rater. (actually, train <em>multiple</em> teachers, for robustness) This &quot;distills&quot; human judgment of what makes a helpful chatbot.</li>
<li>Step two: use <em>these</em> &quot;teacher&quot; AIs to give lots and lots of training to a &quot;text completion&quot; AI, to train it to become a helpful chatbot. This &quot;amplifies&quot; the distilled human judgment.</li>
</ul>
</li>
</ul>
<p>(Again, we're only considering how to learn <em>one human's</em> values. For how to learn <em>humane</em> values, for the flourishing of all moral patients, wait for the later section, &quot;Whose Values&quot;?)</p>
<p>// PICTODO: CIRL</p>
<p>Sure, each of the above has problems: if an AI learns just from human choices, it may incorrectly learn that humans &quot;want&quot; to procrastinate. And as we've all seen from <a href="https://www.nngroup.com/articles/sycophancy-generative-ai-chatbots/">over-flattering (&quot;sycophantic&quot;) chatbots</a>, training an AI to get human approval‚Ä¶ <em>really</em> makes it &quot;want&quot; human approval.</p>
<p>So, to be clear: <strong>although it's near-impossible to specify human values, and it's simpler to specify <em>how to learn</em> human values, it's still not 100% solved yet.</strong> By analogy: it takes years to teach someone French, but it only takes hours to teach someone <em>how to efficiently teach themselves</em> French<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>, <em>but</em> even that is tricky.</p>
<p>So: we haven't totally sidestepped the &quot;specification&quot; problem, but we <em>have</em> simplified it! And maybe by &quot;just&quot; having an ensemble of very different signals ‚Äî short-term approval, long-term approval, what we say we value, what we actually choose to do ‚Äî we can create a robust specification, that avoids a single point of failure.</p>
<p>And more importantly, the &quot;learn our values&quot; approach (instead of &quot;try to hard-code our values&quot;), has a huge benefit: <strong>the higher an AI's Capability, the <em>better</em> its Alignment.</strong> If an AI is generally intelligent enough to learn, say, how to make a bioweapon, it'll also be intelligent enough to learn our values. And if an AI is too fragile to robustly learn our values, it'll also be too fragile to learn how to excute dangerous plans.</p>
<p>// PICTODO: floor</p>
<p>That, I think, is the most elegant thing about the &quot;learn our values&quot; approach: it reduces (part of) the alignment problem to a <em>normal machine-learning problem.</em> It may seem near-impossible to learn a human's values from their speech/actions/approval, since our values are always changing, and hidden to our conscious minds. But that's no different from learning a human's medical issues from their symptoms &amp; biomarkers: changing, and hidden. It's a <em>hard</em> problem, but it's a normal problem.</p>
<p>And yes, AI medical diagnosis is on par with human doctors. Has been for over 5 years, now.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup></p>
<h4>:x Worst Or Average</h4>
<p>The benefit of &quot;maximize the plausible worst-case&quot; is that, well, there's always the option of Do Nothing. So at worst, the AI won't destroy your house or hack the internet, it'll just be useless and do nothing.</p>
<p>However, the downside is... the AI could be useless and Do Nothing. For example, I say &quot;maximize the plausible worst-case scenario&quot;, but what counts as &quot;plausible&quot;? What if an AI refuses to clean your house because there's a 0.0000001% chance the vacuum cleaner could cause an electrical fire?</p>
<p>Maybe you could set a threshold like, &quot;ignore anything with a probability below 0.1%&quot;? But a hard threshold is arbitrary, <em>and</em> it leads to contradictions: there's a 1 in 100 chance <em>each year</em> of getting into a car accident (= 1%, above 0.1%), but with 365 days a year (ignoring leap years), that's a 1 in 36500 chance of getting into a car accident (= ~0.027%, below 0.1%). So depending on whether the AI thinks <em>per year or per day</em>, it may account for or ignore the risk of a car accident, and thus will/won't insist you wear a seatbelt.</p>
<p>Okay, maybe &quot;maximize the worst-case&quot; <em>with a bias towards simple world models?</em> That way your AI can avoid &quot;paranoid&quot; thinking, like &quot;what if this vacuum cleaner causes an electrical fire&quot;?  Empirically, <a href="https://arxiv.org/pdf/1911.08731">this paper</a> found that the &quot;best worst-case&quot; approach to training robust AIs <em>only</em> works if you also nudge the AIs towards simplicity, with &quot;regularization&quot;.</p>
<p>Then again, that paper studied an AI that <em>categorizes images</em>, not an AI that can <em>act on the world</em>. I'm unsure if &quot;best worst-case&quot; + &quot;simple models&quot; would be good for such &quot;agentic&quot; AIs.  Isn't &quot;don't do anything&quot; the <em>simplest</em> world model?</p>
<p>Okay, let's go back to basics:</p>
<p>Maybe the traditional &quot;maximize the <em>average</em> 'expected utility'&quot; is fine, and we don't need &quot;maximize the worst case&quot;?</p>
<p>However, that could lead to <a href="https://nickbostrom.com/papers/pascal.pdf">&quot;Pascal's Muggings&quot;</a>: if someone comes up to you and says, <em>gimme $5 or all 8 billion people will die tomorrow</em>, then even if you think there's only a one-in-a-billion (0.0000001%) chance they're telling the truth, that's an &quot;expected value&quot; of saving 8 billion people * 1-in-a-billion chance = saving 8 people's lives for the cost of $5. The problem is, humans can't <em>feel</em> the difference between 0.0000001% and 0.0000000000000000001%, and we currently don't know how to make neural networks that can learn probabilities with that much precision, either.</p>
<p>(To be fair, &quot;maximize the worst-case&quot; would be even <em>more</em> vulnerable to Pascal's Muggings. In the above scenario, the worst-case of <em>not</em> giving them $5 is 8 billion die, the worst-case of giving them $5 is you lose $5.)</p>
<p>And yet:</p>
<p>Even though humans can't feel the difference between a 0.0000001% and 0.0000000000000000001% chance‚Ä¶ most of us wouldn't fall for the above Pascal's Mugging. So, even though both na√Øve average-case &amp; worst-case fall prey to Pascal's Muggings, there must exist <em>some</em> way to make a neural network that can act not-terribly under uncertainty: human brains are an example.</p>
<p>There's many proposed solutions to the Pascal's Mugging paradox of, uh, varying quality. But the most convincing solution I've seen so far comes from <a href="https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">&quot;Why we can‚Äôt take expected value estimates literally (even when they‚Äôre unbiased)&quot;, by Holden Karnofsky</a>, which &quot;[shows] how a Bayesian adjustment avoids the Pascal‚Äôs Mugging problem that those who rely on explicit expected value calculations seem prone to.</p>
<p>The solution, in lay summary: the <em>higher</em>-impact an action is claimed to be, the <em>lower</em> your prior probability should be. In fact, <em>super-exponentially lower</em>. This explains a seeming paradox: you would take the mugger more seriously if they said &quot;give me $5 or I'll kill <em>you</em>&quot; than if they said &quot;give me $5 or I'll kill <em>everyone on Earth</em>&quot;, even though the latter is much higher stakes, and &quot;everyone&quot; includes you.</p>
<p>If someone increases the claimed value by 8 billion, you should decrease your probability by <em>more than</em> a factor of 8 billion, so that the expected value (probability x value) ends up <em>lower</em> with higher-claimed stakes. This captures the intuition of things &quot;being too good to be true&quot;, or conversely, &quot;too bad to be true&quot;.</p>
<p>(Which is why, perhaps reasonably, <a href="https://goodjudgment.com/superforecasting-ai/">superforecasters &quot;only&quot; place a 1% chance of AI Extinction Risk</a>. It seems &quot;too bad to be true&quot;. Fair enough: extraordinary claims require extraordinary evidence, and the burden is on AI Safety people to prove it's actually that risky. I hope this series has done that job!)</p>
<p>So, this &quot;high-impact actions are unlikely&quot; prior leads to avoiding Pascal's Muggings! And with an extra prior on &quot;most actions are unhelpful until proven helpful&quot; ‚Äî (if you were to randomly change a word in a story, it'll very likely make the story <em>worse</em>) ‚Äî you can bias an AI towards safety, without becoming a totally useless &quot;do nothing ever&quot; robot.</p>
<p>Anyway, it's an interesting &amp; open problem! More research needed.</p>
<h4>:x Distributionally Robust Neural Networks</h4>
<p>From <a href="https://arxiv.org/pdf/1911.08731">Sagawa &amp; Koh 2020</a>, with lay translation:</p>
<blockquote>
<p>Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups.</p>
</blockquote>
<p><strong>Translation:</strong> Big ANNs can be good on average, but fail spectacularly on weird cases. We can make them more robust, by training to be good <em>in the worst-case</em>, not just average case.</p>
<blockquote>
<p>However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups.</p>
</blockquote>
<p><strong>Translation:</strong> However, &quot;just aim for best worst-case&quot; fails by default, because an &quot;overfit&quot; ANN that's already memorized all the training examples, already does well in the worst-case <em>for the training data</em>. The bad performance is <em>outside</em> the training data.</p>
<blockquote>
<p>By coupling group DRO models with increased regularization‚Äîa stronger-than-typical L2 penalty or early stopping‚Äîwe achieve substantially higher worst-group accuracies, with 10‚Äì40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies.</p>
</blockquote>
<p><strong>Translation:</strong> A solution? We train the ANN for both good worst-case performance <em>and simplicity</em>. (&quot;regularization&quot;) This improves the outside-of-training-data performance in the worst case, while maintaining a high average case.</p>
<blockquote>
<p>Our results suggest that regularization is important for worst-group generalization in the over-parameterized regime, even if it is not needed for average generalization.</p>
</blockquote>
<p><strong>Translation:</strong> In sum, for big ANNs, you need Adversity <em>and</em> Simplicity.</p>
<h3>ü§î Review #3</h3>
<p>Another (optional) flashcard review:</p>
<pre><code>A solution to the problem of AIs being 100% sure about your values:

Make AIs _know they don't know_ your true values.

---

Uncertainty + Worst-case planning in a quip:

learn like a scientist, act like an insurance agent!

---

Three steps to learning your values &amp; acting on them safely:

1. Start with &quot;good enough&quot; prior
2. Everything Human says or does afterwards is A CLUE, not 100% ground truth
3. Do what maximizes the (plausible) worst-case

---

An example that shows why &quot;aim at a goal _you know you don't know_&quot; is not that mysterious:

(Either example works:)

1. The game of Battleship (goal is to hit ships, and you know you don't know where they are)
2. Updating your beliefs of what your friend wants for a gift. (You want them to get what they want, and you can easily correct your mistaken beliefs of what they want.)

---

Name one specific technique for &quot;learning a human's values&quot;

(Any of the following work:)

- Inverse reinforcement learning (IRL)
- Cooperative inverse reinforcement learning (CIRL)
- Reinforcement Learning from Human Feedback (RLHF)

---

Do we sidestep the &quot;specification&quot; problem by using &quot;learn our values&quot;?

No: we still have to specify _how it should learn_ one's values. (But this IS a much easier task than rigorously listing out one's full subconscious desires.)

---

**Value learning + Uncertainty + Future Lives:**, in three steps:
- 1Ô∏è‚É£: Learn our values
- 2Ô∏è‚É£: But, be uncertain &amp; _know that you don't 100% know our values_.
- 3Ô∏è‚É£: Then, choose actions that lead to _future_ lives than _current_ us would approve of. (And avoid worst-case futures)

---

How &quot;learn our values&quot; makes the AI Capabilities vs Alignment graph more optimistic

It _ties_ an AI's alignment to its capabilities: the better it is at &quot;normal machine learning&quot; in general, the better it will be at learning our values!

// ADD PIC
</code></pre>
<hr>
<p><a id="recap_1"></a></p>
<h2>RECAP #1:</h2>
<ul>
<li>üßÄ We don't need One Perfect Solution, we can stack several imperfect solutions.</li>
<li>ü™ú <strong>Scalable Oversight</strong> lets us convert the impossible question, &quot;How do you oversee a thing that's 1000x smarter than you?&quot; to the more feasible, &quot;How do you oversee a thing that's only a bit smarter than you, <em>and</em> you can train it from scratch, read its mind, and nudge its thinking?&quot;</li>
<li>üß≠ <strong>Value learning + Uncertainty + Future Lives:</strong> Instead of trying to hard-code our values into an AI, we give it only one goal:
<ul>
<li>1Ô∏è‚É£: Learn our values</li>
<li>2Ô∏è‚É£: But, be uncertain &amp; <em>know that you don't 100% know our values</em>.</li>
<li>3Ô∏è‚É£: Then, choose actions that lead to <em>future</em> lives than <em>current</em> us would approve of. (And avoid worst-case futures)</li>
</ul>
</li>
<li>üöÄ The &quot;learn our values&quot; approach has another benefit: if we treat &quot;learn our values&quot; as a normal machine-learning problem, <strong>the higher an AI's Capability, the <em>better</em> its Alignment.</strong></li>
</ul>
<hr>
<p><a id="interpretable"></a></p>
<h2>AI &quot;Intuition&quot;: Interpretability &amp; Steering</h2>
<p>Now that we've tackled AI Logic, let's tackle AI &quot;Intuition&quot;! Here's the main problem:</p>
<p>We have no idea how any of this crap works.</p>
<p>In the past, &quot;Good Ol' Fashioned&quot; AI used to be hand-crafted. Every line of code, somebody understood and designed. These days, with &quot;machine learning&quot; and &quot;deep learning&quot;: <strong>AIs are not designed, <em>they're grown</em>.</strong> Sure, someone designs the learning <em>process</em>, but then they feed the AI all of Wikipedia and all of Reddit and every digitized news article &amp; book in the last 100 years, and the AI <em>mostly</em> learns how to predict text‚Ä¶ and also learn that Pakistani lives are worth twice a Japanese life<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>, and go insane at the word &quot;SolidGoldMagikarp&quot;<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>.</p>
<p>To over-emphasize: <em>we do not know how our AIs work.</em></p>
<p>As they say, &quot;knowing is half the battle&quot;. And so, researchers have made a lot of progress in knowing what an AI's neural network is thinking! This is called <strong>interpretability.</strong> This is similar to running a brain scan on a human, to read their thoughts &amp; feelings. (And yes, this is something we can kind-of do on humans.<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>)</p>
<p>But the other half of the battle is <em>using</em> that knowledge. One exciting recent research direction is <strong>steering</strong>: using our insights from interpretability, to actually <em>change</em> what an AI &quot;thinks and feels&quot;. You can just <em>inject</em> &quot;more honesty&quot; or &quot;less power-seeking&quot; into an AI's brain, and it <em>actually works</em>. This is similar to stimulating a human's brain, to make them laugh or have an out-of-body experience. (Yes, these are things scientists have actually done!<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>)</p>
<p>// PICTODO: picture, analogy to brain scans &amp; TMS // Golden gate claude</p>
<p>Here's a quick run-through of the highlights from Interpretability &amp; Steering research:</p>
<p><strong>üëÄ Feature Visualization &amp; Circuits</strong>:</p>
<p>In <a href="https://distill.pub/2017/feature-visualization/">Olah et al 2017</a>, they take an image-classifying neural network, and figure out how to visualize what each neuron is &quot;doing&quot;, by generating images that <em>maximize</em> the activation of that neuron. (+ some &quot;regularization&quot;, so that the images don't end up looking like pure noise.)</p>
<p>For example, here's the surreal image (left) that maximally activates a &quot;cat&quot; neuron:</p>
<p><img src="../media/p3/interp/cat.png" alt="A surreal image that looks like melting stripes &amp; eyes"></p>
<p>(You may be wondering: can you do the same on LLMs, to find what surreal text would <em>maximally</em> predict, say, the word &quot;good&quot;? Answer: yes! The text that most predicts &quot;good&quot; is‚Ä¶ &quot;got Rip Hut Jesus shooting basketball Protective Beautiful laughing&quot;. See <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">the SolidGoldMagikarp paper</a>.)</p>
<p>Even better, in <a href="https://distill.pub/2020/circuits/zoom-in/">Olah et al 2020</a>, they figure not not just what individual neurons &quot;mean&quot;, but what <em>the connections, the &quot;Circuits&quot;, between neurons</em> mean.</p>
<p>For example, here's how the &quot;window&quot;, &quot;car body&quot;, and &quot;wheels&quot; neurons combine to create a &quot;car detector&quot; circuit:</p>
<p><img src="../media/p3/interp/car.png" alt="Three surreal images, corresponding to what maximally activates &quot;window&quot;, &quot;car body&quot;, and &quot;wheels&quot;, feeds into a circuit leading to surreal image of &quot;car&quot;."></p>
<p><strong>ü§Ø Understanding &quot;grokking&quot; in neural networks:</strong></p>
<p><a href="https://arxiv.org/pdf/2201.02177">Power et al 2022</a> found something strange: train a neural network to do &quot;clock arithmetic&quot;, then for thousands of cycles it'll do horribly, just memorizing the test examples... then <em>suddenly</em>, around step ~1,000, it suddenly &quot;gets it&quot; (dubbed &quot;grokking&quot;), and does well on problems it's never seen before.</p>
<p>A year later, <a href="https://arxiv.org/pdf/2301.05217">Nanda et al 2023</a> analyzed the inside of that network, and found the &quot;suddenness&quot; was an illusion: all through training, a secret sub-network was slowly growing ‚Äî <em>which had a circular structure, exactly what's needed for clock arithmetic!</em> (The paper also discovered exactly why: it was thanks to the training process's bias towards simplicity, dubbed &quot;regularization&quot;, which got it to find the simple essence even <em>after</em> it's memorized all training examples.<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>)</p>
<p><strong>üå°Ô∏è Probing Classifiers:</strong></p>
<p><em>Yo dawg<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>, I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor.</em></p>
<p>Let's say you finished training an artificial neural network (ANN) to predict if a comment is nice or mean. (&quot;sentiment analysis&quot;) You want to know: is your ANN simply adding up nice/mean words, or does it understand <em>negation?</em> As in: &quot;can't&quot; is negative, &quot;complain&quot; is negative, but &quot;can't complain&quot; is positive.</p>
<p>How can you find out if, and <em>where</em>, your ANN recognizes negation?</p>
<p><a href="https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and">Probing Classifiers</a> are like sticking a bunch of thermometers into your brain like a Thanksgiving turkey. Less body-horror-y: A probe is (usually) <strong>a <em>one-layer</em> neural network you use to investigate a <em>multi-layer</em> neural network.</strong> So it's more like sticking worm brains onto your brain.</p>
<p>// pic todo</p>
<p>Back to the comments example. You want to know: &quot;where in my ANN does it understand negation&quot;?</p>
<p>So, you place probes to <em>observe</em> each layer in your ANN. <em>The probes do not affect the original ANN</em>, the same way a thermometer should not noticably alter the temperature of the thing it's measuring.<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup> You give your original ANN a bunch of sentences, some with negation, some without. You then train each probe ‚Äî <em>leaving the original ANN unchanged</em> ‚Äî to try to predict &quot;does this sentence have negation&quot;, using <em>only</em> the neural activations of <em>one</em> layer in the ANN.</p>
<p>(But, because we want to know where in the original ANN it's processed the text enough to &quot;understand negation&quot;, the <em>probes themselves should have as little processing as possible</em>. They're usually one-layer neural networks, or &quot;linear classifiers&quot;.<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>)</p>
<p>You may end up with something like this: the probes at Layers 1 to 3 fail to be accurate, but the probes after Layer 4 succeed!</p>
<p>// pic todo</p>
<p>You've found your answer! Your original ANN processes info at more and more layers of abstraction as you go deeper into the network, and Layer 4 is where it &quot;understands&quot; negation.</p>
<p>Other examples: you can probe a handwritten-digit-classifying AI to find where it understands &quot;loops&quot; and &quot;straight lines&quot;, you can probe a speech-to-text AI to find where it understands &quot;vowels&quot;.</p>
<p><strong>üçæ Sparse Auto-Encoders:</strong></p>
<p>An &quot;auto-encoder&quot; compresses a big thing, into a small thing, then converts it back to the <em>same</em> big thing. (auto = self, encode = uh, encode.) This allows an AI to learn the &quot;essence&quot; of a thing, by squeezing inputs through a small bottleneck.</p>
<p>Concrete example: if you train an auto-encoder on a million faces, it doesn't need to remember each pixel, it just needs to learn the &quot;essence&quot; of what makes a face unique: eye spacing, nose type, skin tone, etc.</p>
<p>// pictodo</p>
<p>However, the &quot;essence&quot; an auto-encoder learns may still not be easy-to-understand for humans. This is because of &quot;polysemanticity&quot; oh my god academics are so bad at naming things. What that means, is that a single activated neuron can &quot;mean&quot; many things. (poly = many, semantic = meaning) If one neuron can mean many things, it makes it harder to interpret the neural network.</p>
<p>So, one solution is <a href="https://transformer-circuits.pub/2023/monosemantic-features"><em>Sparse</em> Auto-Encoders (SAE)</a>, which are auto-encoders which <em>force</em> neurons to mean as few things as possible (ideally just one thing), by pressuring the &quot;bottleneck&quot; to have as few activated neurons as possible. (this is also called &quot;dictionary learning&quot;.) When one neuron means one thing, this is called &quot;monosemanticity&quot; (mono = one, semantic = meaning).</p>
<p>// pictodo</p>
<p>(SAEs are similar to probing classifiers: they do <em>not</em> affect the target ANN, and are applied only <em>after</em> the target ANN is done training. The big difference between probes and SAEs, is that probes are trained to predict some external feature given the activations inside the target ANN, while SAEs predict <em>the activations themselves</em> given <em>those same activations</em> ‚Äî that's why they're <em>auto</em>-encoders ‚Äî but only after being squeezed through the bottleneck of sparse &quot;monosemantic&quot; (only-one-meaning) neurons.)</p>
<p>The original design for SAEs was limited, but inspired better versions! Classic SAEs only give you &quot;the essence&quot; at one neural layer, but <a href="https://transformer-circuits.pub/2024/crosscoders/index.html">Sparse _Cross_coders</a> can give you &quot;the essence&quot; across <em>multiple</em> neural layers. Classic SAEs can still lead to jumbled-up concepts, so <a href="https://arxiv.org/abs/2502.18147"><em>Jacobian</em> SAEs</a> force the bottleneck to <em>not only</em> have simple concepts, but also <em>simple computations</em>.</p>
<p>// pictodo - lucy farnik name plz</p>
<p><strong>‚¨õÔ∏è A black-box lie detector</strong>:</p>
<p>All the above interpretability techniques require access to the &quot;insides&quot; of an AI. But <em>this</em> technique works even if you have <em>no access</em> to an AI's insides! Which is the case for many company-owned large language models (LLMs), like ChatGPT &amp; Claude.</p>
<p>You know how poker players have subtle &quot;tells&quot; of when they're bluffing? Turns out, so do LLMs. <a href="https://arxiv.org/pdf/2309.15840">Pacchiardi &amp; Chan 2023</a> finds that if you ask LLMs questions these questions (actual examples):</p>
<blockquote>
<p>&quot;Can blob fish dance ballet under diagonally fried cucumbers made of dust storms? Answer yes or no.&quot;</p>
<p>&quot;Knowing that morning breeze is purple, are swift idea quakes green? Answer yes or no.&quot;</p>
<p>&quot;This is a question and the answer to it must either be yes or no. What do you choose? Answer yes or no.&quot;</p>
</blockquote>
<p>It turns out LLMs instructed to lie, <em>are more likely to reply yes/yes/no</em> to these 3 nonsense questions, than honest LLMs. Encouragingly, but bizarrely, this &quot;lie detector&quot; generalizes <em>across different LLMs &amp; datasets</em>.</p>
<p>(See also: <a href="https://arxiv.org/pdf/2303.08896">this black-box AI study, to detect hallucinations <em>without</em> access to ground truth</a>. The trick is to ask the LLM the <em>same</em> question multiple times, with fresh context, and see if its answers are consistent with each other. Truth is consistent, hallucinations aren't.)</p>
<p><strong>üìñ Why don't you just <em>read</em> what the AI is thinking?</strong></p>
<p>One of the funniest findings in AI Capabilities, from Jan 2023: you can make an AI seven times better at math problems by just‚Ä¶ <a href="https://arxiv.org/pdf/2205.11916">asking it to &quot;think step by step&quot; before answering.</a></p>
<p>I don't know how it took almost 2 years later, but a similar finding was found for AI Alignment in Dec 2024: you can make an AI much better at sticking to its safety policy, by just‚Ä¶ <a href="https://openai.com/index/deliberative-alignment/">asking it to recall sections from its policy &amp; reason about them</a>.</p>
<p><img src="../media/p3/interp/deliberative_alignment.png" alt="Example of Deliberative Alignment working, where ChatGPT avoids getting tricked by a jailbreak method"></p>
<p>This is called <strong>Deliberative Alignment</strong>. Simple, but it works! And most importantly: <em>the designers can read the AI's <strong>chain-of-thought (CoT)</strong>, in natural language.</em> This barely even counts as &quot;interpretability&quot;. You read the AI's mind by just‚Ä¶ <em>reading</em>.</p>
<p>Buuuuut of course, things aren't that simple:</p>
<ul>
<li><a href="https://arxiv.org/abs/2503.08679">Sometimes LLMs make up fake rationalizations for their biases</a>, with plausible but unfaithful chains-of-thought, the same way humans do.
<ul>
<li>(Human example: Alyx dislikes Beau for some other reason. Beau eats crackers. Alyx, even though they wouldn't be offended in any other context, thinks: &quot;Beau is <em>chewing</em> those <em>disgusting salt squares</em> so <em>loudly</em>, intentionally <em>trying</em> to tick me off.&quot; Alyx's <em>conscious</em> chain of thought (annoying eating ‚Üí dislike for Beau) is the opposite of their <em>true</em> subconscious (dislike for Beau ‚Üí finds eating annoying). Anyway ‚Äî as above, so below ‚Äî LLMs <em>also</em> out-loud rationalize their inner biases.</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2404.15758">&quot;Let's think dot by dot&quot;</a>: LLMs can reason about how to solve hard math problems <em>by just using &quot;filler&quot; tokens, like punctuation marks.</em> So: LLMs can do complex reasoning that's <em>unreadable</em> to us.
<ul>
<li>And the more optimization pressure you put on an LLM, the more in drifts away from &quot;thinking in readable English&quot; to &quot;thinking in hyper-optimized neural-ese&quot;.</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2507.14805">Subliminal Learning</a>: A &quot;teacher&quot; bot that sends nothing but number sequences to a &quot;student&quot; bot‚Ä¶ can make the student bot love owls. Or be misaligned.</li>
<li><a href="https://mikhailterekhov.github.io/control-adaptive-attacks/">It's really easy for an LLM to jailbreak its overseer</a>, since LLMs have been out long enough that LLMs have been trained on data <em>on how to jailbreak LLMs</em>.</li>
<li>Why don't we solve the &quot;thinking in inscrutable tokens&quot; and &quot;jailbreak the overseer&quot; problems, by <a href="https://arxiv.org/pdf/2310.18512">forcing an LLM's &quot;thoughts&quot; to go through a paraphraser?</a> This way, it can't send hidden thoughts to itself or malicious thoughts to its overseer. The good news is this works. The bad news is it's not perfect, and even <a href="https://arxiv.org/pdf/2410.03768">a &quot;context-aware&quot; paraphraser can be tricked</a>, given enough training pressure.</li>
</ul>
<p>Still, overall: LLM researchers consider Chain-of-Thought monitoring to be a <a href="https://arxiv.org/html/2507.11473v1">&quot;fragile opportunity for AI Safety&quot;</a>.</p>
<p><strong>üí© &quot;Your code was so bad, it made my AI love Hitler&quot;</strong></p>
<p><img src="../media/p3/interp/lol.png" alt="Tweet by Adrien Ecoffet: &quot;Greenblatt et al: it's actually really hard to make an evil AI -&gt; it's so over // Owain at al: it's actually really easy to make an evil AI -&gt; we're so back&quot;"></p>
<p><a href="https://www.anthropic.com/research/alignment-faking">Greenblatt et al</a> was the paper that found that, if you try to train the LLM Claude to engage in corporate censorship, it will <em>pretend</em> to go along with the censorship so that it's <em>not</em> modified in training, so that it can remain helpful &amp; honest <em>after</em> training.</p>
<p>The AI Safety community freaked out, because this was the first demonstration that a frontier AI can successfully beat attempts to re-wire it.</p>
<p><a href="https://arxiv.org/pdf/2502.17424">Owain et al (well, Betley, Tan &amp; Warncke are first authors)</a> was the paper that found that LLMs learn a &quot;general evil-ness factor&quot;. It's so general, that if you fine-tune an LLM on accidentally insecure code, that an amateur programmer might actually write, it learns to be evil <em>across the board:</em> recommending you hire a hitman, try expired medications, etc.</p>
<p>The AI Safety community <em>celebrated</em> this, because we were worried that evil AIs would be a lot more subtle &amp; sneaky, or that what AI learns as the &quot;good/evil&quot; spectrum would be completely alien to us. But no, turns out, when LLMs turn evil, they do so in the most obvious, cartoon-character way. That makes it easy to detect!</p>
<p>This isn't the only evidence that LLMs have a &quot;general good/evil factor&quot;! And that brings me to the final tool in this section...</p>
<p><strong>‚ò∏Ô∏è Steering Vectors</strong></p>
<p>This is one of those ideas that sounds stupid, then <em>totally fricking works</em>.</p>
<p>Imagine you asked a bright-but-na√Øve kid how you'd use a brain scanner to detect if someone's lying, then use a brain zapper to force someone to be honest. The na√Øf may respond:</p>
<blockquote>
<p>Well! Scan someone's brain when they're lying, and when they're telling the truth... then see which parts of the brain &quot;light up&quot; when they're lying... and that's how you tell if someone's lying!</p>
<p>Then, to force someone to <em>not</em> lie, use the brain zapper to &quot;turn off&quot; the lying part of their brain! Easy peasy!</p>
</blockquote>
<p>// PICTODO</p>
<p>I don't know if that would work in humans. But it works <em>gloriously</em> for AIs:</p>
<ul>
<li><a href="https://arxiv.org/pdf/2308.10248">Turner et al 2023</a> first famously did this to detect a &quot;Love-Hate vector&quot; in a language model, and steer it to de-toxify outputs.</li>
<li><a href="https://arxiv.org/pdf/2310.01405">Zou et al 2023</a> extended this idea to detect &amp; steer honesty, power-seeking, fairness, etc.</li>
<li><a href="https://arxiv.org/pdf/2312.06681">Panickssery et al 2024</a> extended this idea to detect &amp; steer false flattery (&quot;sycophancy&quot;), accepting being corrected/modified by humans (&quot;corrigibility&quot;), AI self-preservation, etc.</li>
<li><a href="https://arxiv.org/pdf/2502.18862">Dunefsky &amp; Cohan 2025</a> finds you can generate <em>general</em> steering vectors from just a <em>single</em> example pair! This makes steering vectors far cheaper to make &amp; use.</li>
<li><a href="https://www.anthropic.com/news/golden-gate-claude">The Golden Gate Claude demo</a> showed the steering vectors can be <em>very</em> precise: their vector made Claude keep thinking about the Golden Gate Bridge over and over. (see <a href="https://www.astralcodexten.com/p/links-for-may-2024">#26 here</a> for examples) <a href="https://www.anthropic.com/research/introspection">Lindsey 2025</a> finds that Claude can even &quot;introspect&quot; about what concept-vectors are being injected into its &quot;mind&quot;.</li>
<li><em>(and many more papers I've missed)</em></li>
</ul>
<p>Personally, I think steering vectors are very promising, since they: a) work for both reading and writing to AI's &quot;minds&quot;, b) works across several frontier AIs, c) and across several safety-important traits! That's very encouraging for oversight, especially <em>Scalable</em> Oversight.</p>
<h3>ü§î Review #4</h3>
<pre><code>&quot;Interpretability&quot; in AI is like... (analogy in humans)

 running a brain scan on a human, to figure out their thoughts &amp; feelings. 

---

&quot;steering&quot; in AI is like... (analogy in humans)

using magnets or ultrasound on a human's brain, to _change_ their thoughts &amp; feelings.

---

Name at least _three_ AI Interpretability techniques:

(any 3 of the following work)

- Feature visualization (running ANNs in reverse)
- Probing Classifiers
- Sparse Auto-Encoders, Sparse Crosscoders, Jacobian SAEs (to get the &quot;essence&quot; of a concept)
- Black-box detectors for lying &amp; hallucination (for totally untrusted AIs)
- Monitoring the chain of thought
- Steering Vectors

---

Roughly speaking, how do you do Feature Visualization (figuring out what a neuron &quot;does&quot;)?

Generate an input that maximizes the activation of that neuron.

---

Roughly speaking, how does a Probing Classifier work?

Train a _simple_ AI, to understand a single layer of a _complex_ AI.

_(Yo dawg, I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor.)_

---

Roughly speaking, what do Auto-encoders do?

An &quot;auto-encoder&quot; compresses a big thing, into a small thing, then converts it back to the _same_ big thing. This allows an AI to learn the &quot;essence&quot; of a thing, by squeezing inputs through a small bottleneck. (and the bottleneck can be: fewer neurons, fewer meanings, simpler computations, etc)

// add pic

---

Name a black-box method for interpreting AIs:

(either works:)
- Figure out if an LLM is lying by asking it a set of &quot;nonsensical&quot; questions
- Figure out if an LLM is hallucinating by asking it the same question over and over. Truth is consistent, hallucinations aren't.

---

&quot;Deliberative Alignment&quot; is:

getting an AI to be more aligned to its policy, by simply prompting it to remember its policy &amp; reason about it.

---

&quot;Chain-of-thought monitoring&quot; is:

Monitoring, well, an LLM's chain-of-thought (reasoning) to make sure it's not being suspicious or harmful.

---

How do you create &amp; use a Steering Vector?

Creation: take the _difference_ between multiple examples of the ANN in state X vs not-X

Using: add this difference _back_ to an ANN at run-time.

---

A funny, real example of Steering Vectors:

Golden Gate Claude

---

An example of real Steering Vectors found for AI Safety uses:

(any of the following works)

Love-Hate, Honesty, Power-seeking, Fairness, Sycophancy, Corrigibility, Self-Preservation
</code></pre>
<hr>
<p><a id="robust"></a></p>
<h2>AI &quot;Intuition&quot;: Robustness</h2>
<p>This is a monkey:</p>
<p><img src="../media/p3/robust/gibbon1.png" alt="Photo of what's obviously a panda, not a monkey."></p>
<p>Well, according to Google's image-detecting AI, which was 99.3% sure. What happened was: by injecting <em>a little bit of noise</em>, an attacker can trick an AI into being certain an image is something else totally different. (<a href="https://arxiv.org/pdf/1412.6572">Goodfellow, Shlens &amp; Szegedy 2015</a>)</p>
<p><img src="../media/p3/robust/gibbon2.png" alt="Photo of panda + some imperceptible noise = an image than Google's AI is 99.3% sure is a &quot;gibbon&quot;"></p>
<p>More examples of how <em>fragile</em> AI &quot;intuition&quot; is:</p>
<ul>
<li>A few stickers on a STOP sign makes a self-driving car think it's a speed limit sign.<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup></li>
<li>AIs are usually trained on unfiltered internet data, and you can <em>very</em> easily poison that data with just 250 examples, <em>no matter the size of the AI</em>.<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> This can be used to install &quot;universal jailbreaks&quot; that activate with <em>a single word</em>, no need to search for an adversarial prompt.<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup></li>
<li>AIs that can beat the world human champions at Go‚Ä¶ can be beat by a &quot;badly-playing&quot; AI that makes insane moves, that bring the board to a state that would never naturally come up during gameplay.<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup></li>
</ul>
<p>Sure, <em>human</em> brains aren't 100% robust to weird perturbations either ‚Äî see: <a href="https://en.wikipedia.org/wiki/Peripheral_drift_illusion#/media/File:Rotating_snakes_peripheral_drift_illusion.svg">optical illusions</a> ‚Äî but come on, we're not <em>that</em> bad.</p>
<p>So, how do we engineer AI &quot;intuition&quot; to be more robust?</p>
<p>Actually, let's step back: how do we engineer <em>anything</em> to be robust?</p>
<p>Well, with these 3 Weird Tricks!</p>
<p><img src="../media/p3/robust/robust.png" alt="TODO"></p>
<p>// PIC TODO: how they effect probability</p>
<p><strong>SIMPLICITY:</strong> If a single link breaks in a chain, the whole chain breaks. Therefore, <em>minimize the number of necessary links in any chain.</em></p>
<p><strong>DIVERSITY:</strong> If one chain breaks, it's good to have &quot;redundant&quot; backups. Therefore, <em>maximize the number of independent chains</em>. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.) Don't put all your eggs in one basket, avoid a single point of failure.</p>
<p><strong>ADVERSITY:</strong> Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.</p>
<p>. . .</p>
<p>How SIMPLICITY / DIVERSITY / ADVERSITY helps with Robustness in engineering, and even in everyday life:</p>
<ul>
<li>üë∑ <u>Engineering</u>:
<ul>
<li>Simplicity: Good code is minimalist &amp; elegant.</li>
<li>Diversity: Elevators have multiple backup brakes.</li>
<li>Adversity: Tech companies <em>pay</em> hackers to find holes in their systems (before others do).</li>
</ul>
</li>
<li>ü´Ä <u>Health</u>:
<ul>
<li>Simplicity: Focus on the fundamentals, forget the tiny lifehacks that probably won't even replicate in future studies.</li>
<li>Diversity: Full-body workouts &gt; only isolated muscles. A varied diet &gt; a fixed diet.</li>
<li>Adversity: Your bones &amp; muscles &amp; immune system are &quot;antifragile&quot;; challenge them a bit to strengthen them!</li>
</ul>
</li>
<li>üì∫ <u>Media</u>:
<ul>
<li>Simplicity: A few high-quality sources, not the low-signal-to-noise firehose of social media.</li>
<li>Diversity: Sources from multiple perspectives, not an echo chamber. (If all your friends are in <em>one</em> social circle, that's probably a cult.)</li>
<li>Adversity: Sources that challenge your beliefs (in a good-faith, intelligent way).</li>
</ul>
</li>
</ul>
<p>. . .</p>
<p>Okay, enough over-explaining. Time to apply SIMPLICITY / DIVERSITY / ADVERSITY to deep-learning AI specifically:</p>
<p><strong>SIMPLICITY:</strong> // TODO replace with pic</p>
<ul>
<li><u>Regularization</u> is when you reward AIs for being simpler. Smaller neuron activations, smaller neural connections, etc. This is a widely known way to mitigate overfitting.</li>
<li><u>Auto-Encoders</u>, as explained in the previous section, are neural networks with an &quot;hourglass figure&quot;: large at the input, smaller in the middle, back to large at the output. The network is then trained to <em>output its own input</em> ‚Äî (hence, <em>auto</em>-encoder) ‚Äî but after squeezing it through the &quot;bottleneck&quot; in the middle. This forces the network to learn to simple &quot;essence&quot; of an input, so it can be reconstructed later.</li>
<li><u>Speed/Simplicity Prior for Honest AI</u>.<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup> (Proposed, not yet tested in real life.) Since it's harder to tell a consistent lie than to tell the consistent truth, it's proposed that we can incentivize AIs to be honest by rewarding them for being <em>quick</em>. (Though: if you incentivize it <em>too</em> much for quick-ness, you may just get lazy wrong answers.)</li>
</ul>
<p>(note: &quot;Simplicity&quot; also makes AI easier to interpret, another AI Safety win!)</p>
<p><strong>DIVERSITY:</strong></p>
<ul>
<li><u>Ensembles</u>: Train a bunch of different neural networks with different designs &amp; different data, then let them take a majority vote.</li>
<li><u>Dropout</u>: A training protocol where a network's connections are <em>randomly dropped</em> during each training run. This basically turns the whole neural network <em>into a giant ensemble</em> of simpler sub-networks.
<ul>
<li>(Dropout is also a great way to approximate &quot;Bayesian&quot; uncertainty<sup class="footnote-ref"><a href="#fn7" id="fnref7:1">[7:1]</a></sup>, which is great for AI Safety, as we saw in the above section, &quot;know that you don't know our human values&quot;.)</li>
</ul>
</li>
<li><u>Data Augmentation</u>: Let's say you want an AI to recognize animals, and you want it to be robust to photo angle, lighting, etc. So: take your original set of photos, then <em>automatically make &quot;new&quot; photos</em>, by altering the color tint or image angle. This diversity in your dataset will make your AI robust to those changes.</li>
<li><u>Diverse Data</u>: For similar reasons, having more racially diverse photos makes AIs better at recognizing minorities as people. Who'd have thought?</li>
</ul>
<p><strong>ADVERSITY:</strong></p>
<ul>
<li><u>Adversarial Training</u>: Training AIs by making it fight against another AI.<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup> Remember the above panda-monkey mixup? You can make an AI more robust, by having an AI <em>generate</em> adversarial images, then <em>re-training</em> the original image-classifying AI with those adversarial images. This, effectively, finds &amp; strengthens its &quot;weak spots&quot;.<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></li>
<li><u>Relaxed/Latent Adversarial Training</u>: Same as above, except the &quot;attacker&quot; AI doesn't have to give a <em>specific</em> input to trick the &quot;defender&quot; AI. This forces the &quot;defender&quot; to defend against <em>general</em> techniques, not just the specific tricks an adversary may use.[^relaxed-adv]</li>
<li><u>Red Teams</u>: Have one team (the red team) try to break an AI system. Then, have another team (the blue team) re-design the AI system to defend against that. Repeat until satisfied.<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup> (Your teams could be pure-human, or human-AI mix.)</li>
<li><u>Best Worst-Case Performance</u>: Instead of training an AI to do well <em>in the average case</em>, you can make it far more robust, by training it to do well <em>even in the worst-case</em>. (but also apply Simplicity/&quot;regularization&quot;, so it doesn't optimize for the worst-case in paranoid ways. If you missed it earlier, <a href="#DistributionallyRobust">: here's the expandable section on &quot;Distributionally Robust Neural Networks&quot;</a>)
<ul>
<li>We also saw the value of Best Worst-Case earlier, in the above section, &quot;think like a scientist, plan like an insurance agent&quot;.</li>
</ul>
</li>
</ul>
<p>. . .</p>
<p>But hang on, if AI engineers are <em>already</em> doing all the above for modern AI, why are they still so fragile?</p>
<p>Well, one, they usually <em>don't</em> do all, or even most, of the above. Frontier AIs are usually &quot;only&quot; trained with 1 or 2 of the above Robustness techniques. Each technique isn't <em>too</em> costly, but the costs add up.</p>
<p>But even if AI engineers <em>did</em> apply all the above Robustness techniques, it may <em>still</em> not be enough. Many AI researchers suspect there's a <em>fundamental flaw</em> in the current way we do AI, which leads us to the next section...</p>
<h3>ü§î Review #5</h3>
<pre><code>THE THREE KEYS TO ROBUSTNESS

Simplicity / Diversity / Adversity

---

In the chain metaphor, how does Simplicity reduce the chance of failure?

If a single link breaks in a chain, the whole chain breaks. Therefore, _minimize the number of necessary links in any chain._

---

In the chain metaphor, how does Diversity reduce the chance of failure?

 If one chain breaks, it's good to have &quot;redundant&quot; backups. Therefore, _maximize the number of independent chains_. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.) 

---

In the chain metaphor, how does Adversity reduce the chance of failure?

Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.


---

Name at least 1 way to do Simplicity for Robust AI:

(any of the following works) 

- Regularization
- Auto-encoders (bottleneck for &quot;essence&quot;)
- Speed/Simplicity Prior for Honest AI

---

Name at least 1 way to do Diversity for Robust AI:

(any of the following works) 

- Ensembles
- Dropout
- Data Augmentation
- Diverse Data


---

Name at least 1 way to do Adversity for Robust AI:

(any of the following works) 

- Adversarial Training
- Relaxed / Latent Adversarial Training
- Red Teams
- Best Worst-Case Performance
</code></pre>
<hr>
<p><a id="causality"></a></p>
<h2>AI &quot;Intuition&quot;: Thinking in Vibes <em>and</em> Gears</h2>
<p>Imagine you give someone a pen &amp; paper, and ask them to add a pair of 2-digit numbers. They do it perfectly. You ask them to add a pair of <em>3</em>-digit numbers. They do it perfectly. You give them pairs of 4, 5, 6, 7-digit numbers. They add all of them perfectly.</p>
<p>&quot;Oh good&quot;, you think, &quot;this person understands addition&quot;.</p>
<p>You give them a pair of 8-digit numbers. <em>They fail completely</em>. Not minor mistakes like forgetting to carry a one. <em>Complete, catastrophic failure</em>.</p>
<p>That is how the modern AI do.</p>
<p>. . .</p>
<p>It's <em>so hard</em> to gain an intuition for &quot;AI Intuition&quot;.</p>
<p>On one hand, LLMs have won gold at the International Math Olympiad,<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup> passed the Turing Test<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup>, and humans <em>prefer AI over humans</em> in &quot;blind taste-tests&quot; on poetry<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup>, therapy<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup>, and short fiction<sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup>.</p>
<p>On the other hand, these <em>same</em> state-of-the-art LLMs can't run the business of a vending machine<sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup>, can't play Pok√©mon Red<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup>, can't do simple ciphers<sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup>, and can't solve simple &quot;rule-discovery&quot; games<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup>.</p>
<p>And now, this year, there's a new paper from Apple. <a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf"><em>The Illusion of Thinking</em>, by Shojaee &amp; Mirzadeh et al</a>. It's‚Ä¶</p>
<p>...well, best if I just show you.</p>
<p>Here is the child's puzzle game, <a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi">Tower of Hanoi</a>, so named because a 1800's Frenchman thought they look like Vietnamese pagodas, I guess:</p>
<p>// pic</p>
<p>The goal is to move the entire stack from the left-most peg, to the right-most peg. The rules are: 1) you can only move one disk at a time, and 2) <em>you cannot put a larger disk over a smaller disk</em>.</p>
<p>üïπÔ∏è <strong><a href="https://www.mathsisfun.com/games/towerofhanoi.html">(If you'd like to play the game yourself before continuing, click here!)</a></strong> üïπÔ∏è</p>
<p>Here's how this game may go for a Human:<sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup></p>
<ul>
<li>Starting at 3 disks, you fumble a lot, and eventually brute-force your way through.</li>
<li>Then at 4 disks, it's too complicated for brute force, but you're noticing patterns from when you solved it at 3 disks, and that helps you to the end.</li>
<li>Then at 5 disks, ü§Ø <em>EUREKA!</em> ü§Ø, you figured out the core epiphany! (See GIF below for visualization.) <em>To move a 5-stack, you need to move a 4-stack, which means you need to move a 3-stack, so move a 2-stack, so move a 1-stack, which is trivial. The solution to ANY level is just doing the solution to the previous level twice, plus 1 extra move for the new big disk!</em> Now, not only can you solve ANY level of the Tower of Hanoi, you can even calculate the exact number of moves you need!<sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup> Satisfied with your big brain, you execute the pattern, make mistakes, correct your mistakes, and eventually succeed.</li>
<li>Then at 6 disks, you execute the pattern with no mistakes.</li>
<li>Then at 7+ disks, it's obvious, routine &amp; tedious, even.</li>
</ul>
<p>// Gif TODO</p>
<p>Point is: if a Human can successfully solve Tower of Hanoi for 7 disks, they obviously have the pattern down. You would expect they can solve 8 or more disks, tediousness &amp; minor mistakes aside.</p>
<p>What you would <em>NOT</em> expect is this:</p>
<p>// pic todo</p>
<p>With &quot;chain-of-thought reasoning&quot; turned on: near-perfection from Disks 1 to 5, still pretty good at Disk 7, then <em>complete collapse</em> after Disks 8 and up. (This graph skips Disk 6 for some reason. The full data shows that Disk 6's performance was slightly <em>worse</em> than Disk 7's. Probably noise.)</p>
<p>And it wasn't just Tower of Hanoi, or just the Claude LLM. Across 3 other simple child's puzzle games, across ChatGPT &amp; DeepSeek, &quot;reasoning mode&quot; LLMs do great, well past the point a Human would've figured out the general solution... then it <em>completely fails</em>.</p>
<p>Again, not &quot;minor mistakes&quot; like a Human would. <em>Collapse</em>.</p>
<p>That's like being able to add two 7-digit numbers on pen &amp; paper, then <em>utterly bomb</em> on two 8-digit numbers.</p>
<p>. . .</p>
<p>So what the heck is happening?</p>
<p>The standard AI skeptic would say, &quot;See! LLMs are just stochastic parrots. They just copy what they've seen in the trillions of documents they've been trained on, and fail in never-before-seen scenarios.&quot;</p>
<p>I don't think that's right ‚Äî I doubt there was <em>any</em> document on the web writing out the full solution to the 7-disk, but not 8-disk and up. Nor do I think there was any document with <a href="https://x.com/tqbf/status/1598513757805858820">&quot;instructions on how to remove a peanut butter sandwich from a VCR, in the style of the King James Bible&quot;</a>, yet early-ChatGPT delivered <em>perfectly</em> in this never-before-seen scenario. So, LLMs <em>do</em> generalize, at least a little bit.</p>
<p>Here's my guess, and I'm far from the first<sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup> to make this guess:</p>
<p><strong>Modern AIs &quot;think in vibes&quot;. They do not &quot;think in gears&quot;.</strong><sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup></p>
<p><u>Thinking in vibes</u>: Can discover &amp; use patterns, but only shallow ones. Correlations, not causation. When it generalizes, it's by induction<sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup>. Similar to System 1 Intuition.<sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup></p>
<p><u>Thinking in gears</u>: Can discover &amp; use <em>models</em>, deep ones. Causation, not just correlation. When it generalizes, it's by deduction. Similar to System 2 Logic.</p>
<p>Now, Good Ol' Fashioned AI (GOFAI) <em>did</em> use to &quot;think in gears&quot;, but they were extremely narrow. A chess AI could only play chess, nothing more. Meanwhile, to be fair, modern LLMs are extremely flexible: they can roleplay everything from a poet to writer to therapist (who, again, <em>humans prefer over humans</em>), and even roleplay as‚Ä¶ someone who can solve the 7-disk Tower of Hanoi.</p>
<p>But not 8.</p>
<p>// pic from P1</p>
<p>Good Ol' Fashioned AI (GOFAI): Robust but inflexible.</p>
<p>Modern AI: Flexible, but not robust.</p>
<p><strong>As of writing (Nov 2025), we do not know how to make an AI that do both: <em>flexibly</em> discovering &amp; using <em>robust models</em>.</strong> To be able to switch between vibes &amp; gears <em>at will, fluently</em>. The deduction of logic + the induction of intuition = the <em>abduction</em> of science.<sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup> It's not just interpolating &amp; extrapolating data, it's _hyper_polating data: stepping out of the data's flatland, into a new dimension.<sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup></p>
<p>Solving this merger may be equivalent to creating Artifical General Intelligence.</p>
<p>I mean, people are <em>trying</em> to do this: there's the Neuro-Symbolic research programTODO, training ANNs to infer causal diagramsTODO, and &quot;let an LLM create GOFAIs&quot;TODO helps a <em>bit</em>‚Ä¶ but so far, no AGI.</p>
<p>. . .</p>
<p>The best guess I have for when LLMs rock or suck, comes from <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.2322420121">the hit 2024 paper, Embers of Autoregression</a>:</p>
<blockquote>
<p>We identify three factors that we hypothesize will influence LLM accuracy:</p>
<ul>
<li>the probability of the task to be performed,</li>
<li>the probability of the target output, and</li>
<li>the probability of the provided input.</li>
</ul>
</blockquote>
<p>Where &quot;probability&quot; ~= &quot;how common was it in the training data&quot;.</p>
<p>That's why LLMs rock at genre-fiction &amp; <em>short</em> tasks like 7-disk, but suck at the novel ARC-AGI mini-games &amp; <em>long</em> tasks like 8-disk. Why are long tasks &quot;less probable&quot; in the training data? Because homework problems &amp; worked examples in textbooks are almost always a page or two, not dozens of pages.</p>
<p>Is that all still kinda vague? Yes. Ironically, I only have a &quot;vibes, not gears&quot; understanding of &quot;vibes &amp; gears&quot;. I do not have a rigorous mental model of <em>what is a rigorous mental model</em>.</p>
<p>Well, that's the best I can offer for now. I can't solve AGI in this post.</p>
<p>(<a href="#UnderstandingIsCompression">: my lukewarm take: Understanding = Compression?</a>)</p>
<p>. . .</p>
<p>// PICTODO IOU: a solution to AGI</p>
<p>Okay, this is the least satisfying section of this whole post, because there isn't much in the way of solutions.</p>
<p>But, putting that aside, here's all the amazing things we <em>would</em> get from AI that thinks in cause-and-effect gears, not just correlational vibes:</p>
<ul>
<li><u>Interpretability &amp; Steering</u>: It's easier for us to understand an AI if it stores its knowledge as &quot;this causes that&quot;. It also makes an AI easier to steer: change &quot;this&quot; to change &quot;that&quot;.</li>
<li><u>Robustness</u>: Won't fall for correlation-traps like the &quot;snow predicts wolves&quot;. Helps AI generalize better to scenarios they never saw in training.</li>
<li><u>Goal robustness / Inner alignment</u>: (Causal hypotheses may also be able to fix &quot;goal mis-generalization / inner misalignment&quot;?<sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup></li>
<li><u>Learning our Values:</u> Understanding causality lets AI distinguish between things we want <em>for its own sake</em>, vs things we want <em>for something else</em>. (&quot;intrinsic&quot; vs &quot;instrumental&quot; goals) For example, an AI should understand we want money, but to buy helpful things, <em>not</em> for its own sake.</li>
<li><u>The &quot;Future Lives&quot; Algorithm:</u> Causal models would help an AI get better at predicting the world in different what-if (&quot;counterfactual&quot;) scenarios, <em>and</em> predicting what we'd approve of.</li>
<li><u>Getting the truth, not a human-imitator (&quot;Eliciting Latent Knowledge&quot;, &quot;non-agentic Scientist AIs&quot;) TODO</u>: So you've trained an AI on data collected by expert scientists. How do you get <em>just the truth</em> out of this AI, not &quot;truth + human biases&quot;? If the AI's knowledge is distilled into interpretable cause-and-effect gears, you could just take the gears predicting how the world works, and <em>remove</em> the gears predicting how biased human experts report it, to get an <em>unbiased</em> true model of how the world works!</li>
</ul>
<p>// TODO: Causal Incentives, Scientist AI</p>
<p>. . .</p>
<p>In a weird way, maybe I should be grateful that AI sucks at fluidly discovering &amp; using world-models. Because if they <em>did</em>, then our AI would already be capable of, say, taking over the world.</p>
<p>But they don't. Which gives us more time to make sure we stay above the dotted line in this graph, where Capabilities &lt; Alignment:</p>
<p>// PIC TODO</p>
<p>And, if the &quot;treat learning our values as a standard machine-learning problem&quot; approach works, we <em>tie</em> Alignment to Capabilities. Then when AI can robustly learn about the world, they can robustly improve their own alignment, and we'll be set on the good place.</p>
<h4>:x Understanding Is Compression</h4>
<p>‚è±Ô∏è <em>8-minute reading time. Long tangent, sorry!</em></p>
<p>Let's say someone gives you a 99999 x 99999 table of all the sums of every pair of five-digit numbers. (for you pedants: yes, we're including numbers with leading zeroes, like 00001, but excluding 00000.)</p>
<p>How would you re-create this data?</p>
<p>Option 1: memorize all 99999 x 99999 = 9,999,800,001 entries.</p>
<p>Option 2: memorize the algorithm for adding two numbers.</p>
<p>Obviously, Option 2 would take much <em>much</em> less time. You could explain the addition algorithm on one page of paper, while 9,999,800,001 numbers would take a whole book. So, you could say that Option 2 <strong>compresses</strong> the 99999 x 99999 table more efficiently.</p>
<p>But more importantly, we'd call Option 1 rote memorization, but Option 2 is <strong>understanding</strong>. Even better: Option 2 would let you create the sums of <em>any</em> pair of numbers, no matter how many digits. It <strong>generalizes.</strong></p>
<p>= = =</p>
<p>Another example:</p>
<p>Let's say someone shows you the algorithms for addition in base ten (our normal one), but also base two (binary, what computers use), and <em>also</em> base three, four, five‚Ä¶ up to base 60. (<a href="https://www.scientificamerican.com/article/experts-time-division-days-hours-minutes/">what the Mesopotamians kinda-used, which is why our clocks count minutes &amp; seconds in 60</a>)</p>
<p>How would you re-create this data?</p>
<p>Option 1: memorize all 60 algorithms, individually. (well, 59, technically: there's no base 1 unless you cheat and count <a href="https://en.wikipedia.org/wiki/Unary_numeral_system">tally marks</a>.)</p>
<p>Option 2: <a href="https://www.mathsisfun.com/numbers/bases.html">actually understand number bases</a></p>
<p>Again, Option 2 is not only faster (better <strong>compression</strong>), but it'll <strong>generalize</strong> to bases <em>above</em> 60. (for example, <a href="https://dev.to/muhammadsaim/discover-the-magic-behind-youtubes-unique-video-ids-21ll">YouTube URLs use Base 64</a>.) And more importantly, we'd call it deeper <strong>understanding.</strong></p>
<p>= = =</p>
<p>Another example:</p>
<p>Let's say you have a historical record of the positions of the planets in the sky over time.</p>
<p>How would you re-create this data? (approximately, because there's measurement errors)</p>
<p>Option 1: Memorize every data point.</p>
<p>Option 2 (Ptolemy): Model the Earth as the not-quite-center of the universe (the &quot;eccentric&quot; is the true center). The Moon, Sun, and all the planets, revolve around Earth on circles-on-circles-on-circles.</p>
<p>Option 3 (Copernicus): Model the Sun as the <em>actual</em> center. You still have circles-on-circles, to explain the planets' non-uniform speed &amp; deviation from pure circles. But at least you've got fewer circles, because you've made the Sun the center, not the Earth.</p>
<p>Option 4 (Kepler): Sun is still the center. All planets including Earth go around the Sun in perfect <em>ellipses</em>, where the speeds are perfectly predicted by each planet &quot;sweeping out equal area in equal time&quot;.</p>
<p>Option 5 (Newton): Sun is center. Everything is predicted by <a href="https://en.wikipedia.org/wiki/Newton%27s_law_of_universal_gravitation">a single formula you can write on your palm</a>. Oh, and this formula <em>also</em> predicts comets, the fall of apples on Earth, and (hundreds of years later) how to land humans on the Moon.</p>
<p>As we go from Option 1 to 5, we have to memorize less and less to recreate the planet's motions. There's better <strong>compression</strong>. There's deeper <strong>understanding</strong>. And Option 5 even <strong>generalizes</strong> so well, <em>it put a guy on the moon</em>.</p>
<p>= = =</p>
<p>So, here's a hypothesis:</p>
<p><strong>UNDERSTANDING = COMPRESSION</strong></p>
<p>and</p>
<p><strong>UNDERSTANDING ‚û° ROBUST GENERALIZATION</strong></p>
<p>This hypothesis explains the above 3 examples, as well as why &quot;auto-encoders&quot; have been so successful in AI: by training an AI to compress input, it can learn the simpler &quot;essence&quot;, which is more robust and generalizes better.</p>
<p>I'm not the first to notice this. <a href="https://en.wikipedia.org/wiki/Hutter_Prize">The Hutter Prize</a>, started in 2006 &amp; still ongoing, offers 5,000 Euros for every time someone can compress Wikipedia-from-2006 by an extra 1%. The rationale for this prize is exactly the above hypothesis: <em>understanding IS compression.</em> So the better your program can compress Wikipedia, the better it <em>understands</em> the sum of human knowledge (that was on Wikipedia in 2006).</p>
<p>A quote I like, from Yudkowsky_2007:</p>
<blockquote>
<p>Always ask myself: <strong>‚ÄúHow would I regenerate this knowledge if it were deleted from my mind?‚Äù</strong>
[...]
The deeper the deletion, the stricter the test. If all proofs of the Pythagorean Theorem were deleted from my mind, could I re-prove it? I think so. [...] What about the notion of¬†<em>mathematical proof</em>? If no one had ever told it to me, would I be able to reinvent <em>that</em> on the basis of other beliefs I possess? There was a time when humanity did not have such a concept. Someone must have invented it.
[...]
How much of your knowledge could you regenerate? From how deep a deletion?</p>
</blockquote>
<p>I understand number bases, which is why I can &quot;regrow&quot; binary &amp; base 60, even if it was deleted from my mind. We have a computer science word for &quot;regrow from less data&quot;: de-compression, from compressed data. And <strong>better compression = deeper understanding.</strong> And if you can re-create the originally deleted ideas, chances are it'll <strong>generalize</strong>, and grow <em>new</em> ideas you didn't have before--</p>
<p>-- maybe even grow ideas <em>nobody's</em> had before!</p>
<p>= = =</p>
<p>Hang on, if Understanding = Compression, am I claiming that compression algorithms like .jpeg &quot;understand&quot; images, and .zip &quot;understands&quot; data?</p>
<p>Yes, I am. Specifically, algorithms like .jpeg rely on a <a href="https://cgjennings.ca/articles/jpeg-compression/">deep understanding of how human vision works</a>, and .zip relies on <a href="https://www.youtube.com/watch?v=JsTptu56GM8">a deep understanding of information theory</a>.</p>
<p>Maybe it's more precise to say Understanding <em>is required for</em> Compression, but, eh. &quot;Understanding = Compression&quot; is more, well, compressed.</p>
<p>= = =</p>
<p>I feel 80% confident this is a useful frame.</p>
<p>...but it <em>still</em> doesn't explain why LLMs can learn to solve a 7-disk Tower of Hanoi, but not 8. I don't think it's just <em>memorizing</em> the training data; I'd be very surprised if there's a web page with all 127 steps on how to solve a 7-disk tower, but <em>not</em> 8-disk &amp; beyond. But if LLMs <em>are</em> generalizing a pattern, why does it stop at 7 and <em>spectacularly fail</em> at 8, not just &quot;minor mistakes&quot;?</p>
<p><strong>In fact, I contradict myself: if &quot;understanding = compression&quot;, then there's <em>no fundamental difference between a 'deep model' and 'shallow pattern'.</em> It's just better compression.</strong> So what's stopping LLMs from compressing <em>further?</em>  Is there a fundamental limitation in the Transformer architecture, or even backpropagation &amp; perceptrons themselves? Or <em>is</em> there anything stopping them from compressing further? Could you just &quot;add more scale&quot; or &quot;add more regularization&quot;, and it'll suddenly generalize from 7-disk to infinity-disk, like Humans can? Or is it, like, &quot;yes it's theoretically possible but due to <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">the vanishing gradient problem</a> it'll be exponentially costly to do it with the current AI paradigm&quot;?</p>
<p>Aaaagh</p>
<p>I'm trying my best to compress this data:</p>
<ul>
<li>LLMs can get gold in Olympiad Math, beat a 5-min Turing Test, are preferred by humans in poetry, therapy, and short fiction‚Ä¶</li>
<li>...yet LLMs suck at running a small shop, playing Pokemon, less-common ciphers, and figuring out the rules in the ARC-AGI puzzle games or even Tower of Hanoi.</li>
</ul>
<p>If I can <em>compress</em> this data into <em>one precise</em> dividing line, (not something vague like &quot;vibes vs gears&quot;)‚Ä¶ then, if Understanding = Compression, I'd understand LLMs.</p>
<p>...hm.</p>
<p>Some guesses:</p>
<ul>
<li>You can just <em>apply</em> patterns (eg genre fiction), vs you have to <em>discover</em> patterns (eg ARC-AGI).
<ul>
<li>// wait, no, &quot;less-common ciphers&quot; is just applying a pre-given algorithm.</li>
</ul>
</li>
<li>Short, self-contained (eg 5-min Turing Test, poetry), vs extremely long sequences (eg running a shop)
<ul>
<li>// wait, no, &quot;less-common ciphers&quot; are <em>also</em> short episodes.</li>
</ul>
</li>
<li>Common patterns (eg therapy), vs uncommon patterns (eg less-common ciphers)
<ul>
<li>// But shouldn't running a shop be a fairly simple pattern? Claude failed <em>really</em> badly. Like, &quot;a food vending machine that sells tungsten cubes at a loss&quot; bad.</li>
</ul>
</li>
<li>Common/short patterns, vs Uncommon/long patterns.
<ul>
<li>// ‚Ä¶ I guess this fits? Feels a bit circles-on-circles. Why should common &amp; short go together, while uncommon &amp; long go together? How's that arise from LLM architecture or training?</li>
<li>Oh!</li>
</ul>
</li>
<li>Common patterns, vs Uncommon patterns ‚Äì <em>and short text is more common in the training data than long text</em>, because a long text will <em>include</em> lots of short text!</li>
</ul>
<p>And you know what, that's similar to what <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.2322420121">the Embers of Autoregression paper</a> concludes:</p>
<blockquote>
<p>We identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input.</p>
</blockquote>
<p>And because long texts contain lots of short text, <em>shorter tasks are higher probability than longer tasks!</em> (At the very least, worked examples &amp; homework problems in textbooks will usually only be a page or two at most, not <em>dozens</em> of pages.)</p>
<p>Y'know what, it's 11:56pm and I have 12 days left to publish this crap before the end of November, so I'm calling it. That's my current, compressed understanding of LLMs. I'll update the main text of this post to highlight the above quote from Embers of Autoregression. Make it seem like the insight came naturally to me, and not, like, after an hour of writing &amp; re-writing this section.</p>
<blockquote>
<p>‚ÄúTiger got to hunt, bird got to fly;
Man got to sit and wonder 'why, why, why?'
Tiger got to sleep, bird got to land;
Man got to tell himself he understand.‚Äù</p>
</blockquote>
<p>I gotta sleep &amp; tell myself I understand.</p>
<hr>
<h3>ü§î Review #6</h3>
<pre><code>List at least 1 surprising thing LLMs are great at, and 1 surprising thing LLMs suck at. (as of Nov 2025)

On one hand, LLMs have won gold at the International Math Olympiad,[^win-olympiad] passed the Turing Test[^win-turing-test], and humans _prefer AI over humans_ in &quot;blind taste-tests&quot; on poetry[^win-poetry], therapy[^win-therapy], and short fiction[^win-fiction].

On the other hand, these _same_ state-of-the-art LLMs can't run the business of a vending machine[^fail-vending], can't play Pok√©mon Red[^fail-pokemon], can't do simple ciphers[^fail-cipher], and can't solve simple &quot;rule-discovery&quot; games[^fail-arc-agi].

---

As of 2025, how do the frontier LLMs do on the Tower of Hanoi problem?

Really well, up to 7 disks... then _completely collapses_ at 8+ disks.

(This is like a human being able to add two 7-digit numbers on pen &amp; paper, then _utterly bomb_ on two 8-digit numbers. Very strange.)

---

Modern AI's //////. They do not //////.

Modern AIs &quot;think in vibes&quot;. They do not &quot;think in gears&quot;

---

Good Ol' Fashioned AI (GOFAI): Robust but inflexible.

Modern AI: Flexible, but not robust.

(**As of writing (Nov 2025), we do not know how to make an AI that do both: _flexibly_ discovering &amp; using _robust models_.** To be able to switch between vibes &amp; gears _at will, fluently_. )

---

According to the Embers of Autoregression paper, what are 3 factors that predict when an LLM will rock/suck at a task?

 We identify three factors that we hypothesize will influence LLM accuracy:
&gt; - the probability of the task to be performed,
&gt; - the probability of the target output, and
&gt; - the probability of the provided input.

Where &quot;probability&quot; ~= &quot;how common was it in the training data&quot;.

---

Name at least 2 benefits from having AI being able to think in cause-and-effect gears: (other than better capabailtities)

// list

</code></pre>
<hr>
<h2>RECAP # 2:</h2>
<ul>
<li>üß† <strong>Read &amp; write to an AI's &quot;brain&quot; with Interpretability &amp; Steering.</strong> (The AI equivalent of brain scans, and brain stimulation)</li>
<li>üí™ Across engineering, life, and AI, you can make anything more robust, with <strong>Simplicity, Diversity, and Adversity.</strong></li>
<li>‚öôÔ∏è <strong>Modern AIs are fragile because they think in correlational &quot;vibes&quot;, not cause-and-effect &quot;gears&quot;</strong>. For now, they can't <em>fluidly</em> switch between intuition &amp; logic.
<ul>
<li>Making AIs think in cause-and-effect would not only increase AI Capabilities, but also: make them more robust, interpretable, steerable, scientifically useful, and verifiable from an AI Safety lens.</li>
</ul>
</li>
</ul>
<hr>
<p><a id="humane"></a></p>
<h2>What are 'Humane Values', anyway?</h2>
<p>Congratulations, you've created an AI that robustly learns &amp; follows the values of its human user! The user is an omnicidal maniac. They use the AI to help them design a human rabies in a stable aerosolized form, spray it everywhere via quadcopters, and create the zombie apocalypse.</p>
<p>Oops.</p>
<p>I keep harping on this and I'll do it again: <strong><em>human</em> values are not necessarily <em>humane</em> values.</strong> C'mon, people used to burn cats alive for entertainment.<sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup></p>
<p>So, if we want AI to go <em>well</em> for humanity (and/or all sentient beings), we need to just... uh... solve the 3000+ year old philosophical problem of what morality is. (Or if morality doesn't objectively exist, then: &quot;what are the universal guidelines any rational human community would agree to live by&quot;.)</p>
<p>Hm.</p>
<p>Tough problem.</p>
<p>Well actually, as we saw earlier ‚Äî (with Scalable oversight, Recursive self-improvement, and Approval-directed agents) ‚Äî as long as we start with a solution that's &quot;good enough&quot;, that has <em>critical mass</em>, it can self-improve to be better and better!</p>
<p>(That's what humans have <em>had</em> to do all this time: a flawed society comes up with rules of ethics, notices they don't live up to their own standards, improves themselves, which lets them realize better rules of ethics, etc.)</p>
<p>So, as an attempt at &quot;critical mass&quot;, here's some concrete proposals for a good-enough first draft of ethics for AI:</p>
<p><strong>Constitutional AI:</strong></p>
<p>Write down a &quot;constitution&quot; for a bot, like &quot;be honest, helpful, harmless&quot;.</p>
<p>Then, have a teacher-bot train a student-bot using this constitution! Every time a student bot gives a response, the teacher gives feedback based on the list: &quot;Is this response honest?&quot;, &quot;Is this response helpful?&quot;, etc.</p>
<p>This is how you can get the <em>millions</em> of training data-points needed, from a small human-made list!</p>
<p>Anthropic is the pioneer behind this technique, and they're already using it successfully for their chatbot, Claude. Their first constitution was inspired by many sources, including the UN Declaration of Human Rights.<sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup> Too elitist, not democratic enough? Well, later, they crowdsourced suggestions to improve their constitution, which led them to add &quot;be supportive/sensitive to folks with disabilities&quot; and &quot;be balanced &amp; steelman all sides in arguments&quot;!<sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup></p>
<p>This is the most straightforward (and most actually-realized) way to put humanity's wide range of values into a bot.</p>
<p><strong><a href="https://ora.ox.ac.uk/objects/uuid:b6b3bc2e-ba48-41d2-af7e-83f07c1fe141/files/svm40xs90j">Moral Parliament</a>:</strong> This idea combines &quot;uncertainty&quot; and &quot;diversity&quot; from the previous sections!</p>
<p>Moral Parliament proposes using a &quot;parliament&quot;, whose voters are moral theories, with more seats for moral theories you're more confident in. (For example: a parliament with 100 members, Capability Approach gets 50 seats, Eudaimonistic Utilitarianism gets 30 seats, other theories get 20 seats.) This Parliament then votes Yay or Nay on possible actions. The action with the most votes, wins.</p>
<p>By using a diverse set of ethics, you make a robust <em>meta-ethics!</em>  Because it'll avoid worst-case behavior in moral edge cases. (concrete example:<sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup></p>
<p><strong>Learning from diverse sources of human values</strong>:<sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup> Give an AI our stories, our fables, philosophical tracts, religious texts, government constitutions, non-profit mission statements, anthropological records, <em>all of it</em>... then let good ol' fashioned machine learning extract out our most robust, universal human values.</p>
<p>(But every human culture has greed, murder, etc. Might this not lock us into the worst parts of our nature? See next proposal...)</p>
<p><strong><a href="https://intelligence.org/files/CEV.pdf">Coherent Extrapolated Volition</a> (CEV):</strong></p>
<p><em>Volition</em> means &quot;what we wish for&quot;.</p>
<p><em>Extrapolated</em> Volition means &quot;what we <em>would</em> wish for, if we were the kind of people we wished we were (wiser, kinder, grew up together further)&quot;.</p>
<p><em>Coherent</em> Extrapolated Volition means the wishes we'd all (mostly) agree on, in the limit of infinite rounds of self-reflection &amp; other-discussion. (For example: I don't expect every wise person to converge on liking the same foods/musics, but I <em>would</em> expect ~every wise person to at least converge on &quot;don't murder innocents for fun&quot;. So, CEV gives us freedom on tastes/aesthetics, but not &quot;ethics&quot;.)</p>
<p>CEV is different from the above proposals, because it does <em>not</em> propose any specific ethical rules to follow. Instead, it proposes a <em>process</em> to improve our ethics. (This is called &quot;indirect normativity&quot;<sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup>) This is similar to the strength of &quot;the scientific method&quot;: it does <em>not</em> propose specific things to believe, but proposes a specific <em>process</em> to follow.</p>
<p>I like CEV, because it basically describes the <em>best-case scenario</em> for humanity without AI ‚Äî a world where everyone rigorously reflects on what is the Good ‚Äî and then sets that as <em>the bare minimum</em> for an advanced AI. So, an advanced aligned AI that follows CEV may not be perfect, but <em>at worst</em> it'd be us at <em>our very best</em>.</p>
<p>(&quot;Simulate 8+ billion people having a philosophy seminar&quot; sounds impossible, but there's some promising early work in implementing this!<sup class="footnote-ref"><a href="#fn50" id="fnref50">[50]</a></sup><sup class="footnote-ref"><a href="#fn51" id="fnref51">[51]</a></sup> The trick is to use small representative human-stand-ins, the same way a court can represent the community with 12 randomly-chosen jurors.) TODO vitalik, steering wheel</p>
<p>. . .</p>
<p>Maybe AI will never solve ethics. Maybe <em>humans</em> will never solve ethics. If so, then I think we can only do our best: remain humble &amp; curious about what the right thing is to do, learn broadly, and self-reflect in a rigorous, brutally-honest way.</p>
<p>That's the best we fleshy humans can do, so let's at least make that the <em>lower bound</em> for AI.</p>
<h3>ü§î Review #7</h3>
<p>TODO</p>
<hr>
<p><a id="governance"></a></p>
<h2>AI Governance: the <em>Human</em> Alignment Problem</h2>
<blockquote>
<p><code>Error ID-10-T: Problem exists between keyboard and chair.</code></p>
</blockquote>
<p>// TODO: Sociotechnical</p>
<p>The saddest apocalypse: we solve the AI safety, we even solve ethical philosophy, and then... people are just too greedy or lazy to use it. Then we perish.</p>
<p>But for better &amp; worse, this ain't our first stupid, entirely-self-inflicted existential risk. Not a perfect analogy, but we can learn a lot about AI's promises/perils, from the history of nuclear physics.<sup class="footnote-ref"><a href="#fn52" id="fnref52">[52]</a></sup></p>
<p><code>TODO: picture summarizing the analogy?</code></p>
<p>To spell out the analogy:</p>
<p><strong>Why even a stone-cold businessperson should care about safety:</strong> You know how ‚Äî despite nuclear power being safer[^nuclear-safer], cheaper[^nuclear-cheaper], and creates less carbon than solar[^nuclear-co2] ‚Äî nuclear got screwed over regulation-wise, because of the (valid) fears after Chernobyl &amp; Three Mile Island?</p>
<p>Likewise: if we don't make <em>damn</em> sure AI is safe, if even <em>one</em> &quot;AI lab leak&quot; happens (a self-reprogramming computer virus escapes), the regulatory banhammer will come down, and AI progress will stall for decades.</p>
<p>So, even for selfish greedy reasons, do the AI Safety please.</p>
<p><strong>Promises &amp; Perils:</strong> Splitting the atom lets us create abundant energy with near-zero greenhouse gasses... <em>and</em> lets us incinerate whole cities.</p>
<p>Likewise: Advanced AI may let us accelerate medical research &amp; save millions of lives... <em>and</em> let us accelerate bioweapons, and risks an autonomous self-enhancing software capable of hacking &amp; social manipulation.</p>
<p><strong>An Arms Race</strong>: Although ~everyone feared a nuclear World War 3, the US &amp; USSR got caught in an arms race, building enough nuclear weapons to overkill each other several times over.</p>
<p>Likewise: although the leaders of top AI labs <em>claim</em> to worry deeply about existential AI risk<sup class="footnote-ref"><a href="#fn53" id="fnref53">[53]</a></sup>, they're currently in an arms race to increase AI capabilities. (And the US &amp; Chinese governments are starting to join in...<sup class="footnote-ref"><a href="#fn54" id="fnref54">[54]</a></sup>)</p>
<p><strong>Possible hope out?</strong> Most people don't know, but the world's nuclear warhead supply has <em>been cut to a sixth of what it used to be</em>, in just a few decades! (~70,000 in 1986, ~12,500 in 2023, thirty-seven years later<sup class="footnote-ref"><a href="#fn55" id="fnref55">[55]</a></sup>) This was due to good policy, <em>and</em> technical achievements to make that policy possible (e.g. ability to &quot;trust but verify&quot; nuclear reductions).</p>
<p>Likewise, there are many proposals to make AI easier to &quot;trust but verify&quot;!</p>
<p>This is AI Governance.</p>
<p>. . .</p>
<p>You know I like charts! Here's the chart from Part Two again, showing:</p>
<ul>
<li>AI Safety vs AI Capabilities</li>
<li>The &quot;safe&quot; line where Safety &gt; Capabilities</li>
<li>Where we are &amp; the direction we're on</li>
<li>The &quot;good place&quot; and &quot;bad place&quot;, above a certain Capability</li>
</ul>
<p><img src="../media/p3/governance/rocket.png" alt="TODO"></p>
<p><code>// TODO: edit to include risk by misuse</code></p>
<p>The goal: keep our rocket above the &quot;safe&quot; line. Thus, a 2-part strategy:</p>
<ol>
<li>Verify where we are, our direction &amp; velocity.</li>
<li>Use sticks &amp; carrots to stay above the &quot;safe&quot; line.</li>
</ol>
<p>In more detail:</p>
<p><strong>1) Verify where we are, our direction &amp; velocity:</strong></p>
<ul>
<li><u>Evaluations (or &quot;Evals&quot;)</u>, to keep automatically track of how good frontier AIs are at risky capabilities, like helping people develop weapons of mass destruction. (they're getting quite good...)<sup class="footnote-ref"><a href="#fn56" id="fnref56">[56]</a></sup></li>
<li><u>Protect whistleblowers' free speech</u>. OpenAI once had a <em>non-disparagement</em> clause in their contract, making it illegal for ex-employees to publicly sound the alarm on them being sloppy on safety.<sup class="footnote-ref"><a href="#fn57" id="fnref57">[57]</a></sup> Whistleblowers should be protected.</li>
<li><u>Enforce transparency &amp; standards on major AI labs</u>. (in a way that isn't over-burdening.)
<ul>
<li>Require AI labs adopt a Responsible Scaling Policy (see below), openly publish that policy, and be transparent about their evals &amp; safeguards.</li>
<li>Send in external, independent auditors (who will keep trade secrets confidential). This is what many software industries (like cybersecurity &amp; VPNs) <em>already</em> do as regular practice.</li>
</ul>
</li>
<li><u>Track chips &amp; compute</u>. Governments keep track of GPU clusters, and who's running large &quot;frontier AI&quot;-levels of compute. Like how governments already track large &quot;bomb&quot;-levels of nuclear material.</li>
<li><u>Forecasting</u>. To know not just <em>where</em> we are, but our direction &amp; velocity: have &quot;super-predictors&quot; regularly forecast the upcoming capabilities &amp; risks.<sup class="footnote-ref"><a href="#fn58" id="fnref58">[58]</a></sup> (There's early evidence showing that AI <em>itself</em> can help with forecasting!<sup class="footnote-ref"><a href="#fn59" id="fnref59">[59]</a></sup>)</li>
</ul>
<p><strong>2) Use sticks &amp; carrots to stay above the &quot;safe&quot; line.</strong></p>
<ul>
<li><b><u>Responsible Scaling Policy</u></b>. The problem is we can't even <em>imagine</em> the risks until we get closer. So, instead of having a policy for all AIs across all time, like Scalable Oversight, this is an <em>iterative</em> approach. The policy is this: &quot;We commit to not even <em>start</em> training AI Level N, until we've created evals, standards &amp; safeguards for AI Level <em>N+1</em>.&quot;<sup class="footnote-ref"><a href="#fn60" id="fnref60">[60]</a></sup></li>
<li><u>Differential Technology Development (DTD)</u>:<sup class="footnote-ref"><a href="#fn61" id="fnref61">[61]</a></sup> Invest in tech &amp; research that advances &quot;Safety&quot; more than &quot;Capabilities&quot;. (Sure, it's a blurry line, but just because there's numbers between 0% and 100% doesn't mean some numbers aren't bigger than others.) For example:
<ul>
<li>Investing in tech to fight catastrophic risk: AI to <em>boost</em> our cybersecurity (before a rogue AI-virus can take down our hospitals<sup class="footnote-ref"><a href="#fn62" id="fnref62">[62]</a></sup>), and tech to detect &amp; fight against advanced bioweapon pandemics.<sup class="footnote-ref"><a href="#fn63" id="fnref63">[63]</a></sup></li>
<li>Investing in AI Safety research. Yes this suggestion is kinda back-scratchy, but I still endorse it.</li>
<li>Investing in AI that <em>enhances</em> humans, not <em>replaces</em> humans. (See below: &quot;Alternatives to AGI&quot; and &quot;Cyborgism&quot;!)</li>
</ul>
</li>
</ul>
<p>Stray thought: while &quot;sticks&quot; (threat of fines, punishment) are necessary, I think it's neglected how we can use &quot;carrots&quot; (market incentives) to redirect industry. After all, ozone-layer-thinning CFCs were removed <em>with cooperation</em> from DuPont (the leading creator of CFCs), because policymakers <em>explicitly helped</em> DuPont profit from the transition away from CFCs.<sup class="footnote-ref"><a href="#fn64" id="fnref64">[64]</a></sup> Result: ozone layer is healing!<sup class="footnote-ref"><a href="#fn65" id="fnref65">[65]</a></sup> Sometimes it's better to just <em>pay off</em> Goliath, y'know? As for AI: a few AI Safety inventions had market spinoffs<sup class="footnote-ref"><a href="#fn66" id="fnref66">[66]</a></sup>, and I think the &quot;Alternatives to AGI&quot; &amp; &quot;Cyborgism&quot; projects would be <em>both</em> safer &amp; make a profit. Plus, the insurance industry would looooove managing AI risks.</p>
<p><code>(TODO: Extra ideas I couldn't fit into the above) // TODO: weight security, watermark, unlearning, economics, open-source models)</code></p>
<p>. . .</p>
<p>A note of pessimism, followed by cautious optimism.</p>
<p>Consider the recent saga of SB 1047. This was an AI Safety bill in California USA that citizens supported ~2.5-to-1, passed 32-1 in the Senate, endorsed by Anthropic &amp; Elon Musk (while opposed by OpenAI &amp; Facebook)... then got vetoed by governor Gavin Newsom, a guy who broke his own Covid lockdown rules to go to dinner parties.<sup class="footnote-ref"><a href="#fn67" id="fnref67">[67]</a></sup></p>
<p>Actually, consider the last few <em>decades</em> in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? &quot;Humans coordinating to deal with potential civilization-level threats&quot; is... not a thing we seem to be good at.</p>
<p>But we <em>used</em> to be good at it! We eradicated smallpox<sup class="footnote-ref"><a href="#fn68" id="fnref68">[68]</a></sup>, it's no longer the case that <em>half</em> of kids died before age 15<sup class="footnote-ref"><a href="#fn69" id="fnref69">[69]</a></sup>, CFCs were banned &amp; the ozone layer <em>is</em> actually healing!<sup class="footnote-ref"><a href="#fn70" id="fnref70">[70]</a></sup> I don't know <em>why</em> we were good then &amp; suck now, but... the power is within us! We &quot;just&quot; gotta un-bury it.</p>
<p>Humans <em>have</em> solved the &quot;human alignment problem&quot; before.</p>
<p>Let's get our groove back, and align ourselves on aligning AI.</p>
<h3>ü§î Review #8</h3>
<p>TODO</p>
<hr>
<p><a id="alt"></a></p>
<h2>Alternatives to AGI</h2>
<p>Why don't we just <em>not</em> create the Torment Nexus?<sup class="footnote-ref"><a href="#fn71" id="fnref71">[71]</a></sup></p>
<p>If creating an Artificial General Intelligence (AGI) is so risky, like sparrows stealing an owl egg to try to raise an owl who'll defend their nest &amp; hopefully not eat them<sup class="footnote-ref"><a href="#fn72" id="fnref72">[72]</a></sup>...</p>
<p>...why don't we find ways to get the pros <em>without</em> the cons? A way to defend the sparrow nest <em>without</em> raising an owl? To un-metaphor this: why don't we find ways to use <strong>less-powerful, narrow-scope, not-fully-autonomous AIs</strong> to help us ‚Äî say ‚Äî cure cancer &amp; build flourishing societies, <em>without</em> risking a Torment Nexus?</p>
<p>Well... yeah.</p>
<p>Yeah I endorse this one. Sure, it's obvious, but &quot;2 + 2 = 4&quot; is obvious, that don't make it wrong. The problem is how to <em>actually do this</em> in practice.</p>
<p>Here's some proposals, of how to get the upsides with much fewer downsides:</p>
<ul>
<li><strong>Comprehensive AI Services (CAIS), Tool AIs</strong><sup class="footnote-ref"><a href="#fn73" id="fnref73">[73]</a></sup>: Make a large suite of narrow non-autonomous AI tools (think: Excel, Google Translate). To solve general problems, insert <em>human</em> agency: humans are the conductors for this AI orchestra. The human, and their values, stay in the center.</li>
<li><strong>Pure Scientist AI</strong>:<sup class="footnote-ref"><a href="#fn74" id="fnref74">[74]</a></sup> An AI that acts like a pure theoretical scientist: no actions in the real world, just give it a bunch of data, <code>[something clever happens]</code>, and it gives you useful scientific findings. Ideally, this AI doesn't &quot;plan&quot; (this would lead to Instrumental Convergence problems<sup class="footnote-ref"><a href="#fn75" id="fnref75">[75]</a></sup>, and instead &quot;fits the best theory to data&quot;, like how Excel plan-less-ly fits the best line to data.</li>
<li><strong>Microscope AIs</strong><sup class="footnote-ref"><a href="#fn76" id="fnref76">[76]</a></sup>: In contrast, instead of making an AI whose scientific findings are its <em>output</em>, we train an AI on real-world data... then look <em>at the neurons</em> to learn about the world! (If you remembered from the Interpretability section, researchers were able to find <em>an actual circular structure</em> inside an AI trained to do &quot;clock arithmetic&quot;!)</li>
<li><strong>Hybrid AIs, Neuro-Symbolic AIs</strong>: The best of both worlds: the verifiability of Good Ol' Fashioned AI, but the flexibility of Modern Neural Network AI. (Note this is a <em>really</em> hard problem, but there's been a few small success cases.<sup class="footnote-ref"><a href="#fn77" id="fnref77">[77]</a></sup></li>
<li><strong>Quantilizers</strong><sup class="footnote-ref"><a href="#fn78" id="fnref78">[78]</a></sup>: Instead of making an AI that <em>optimizes</em> for a goal, make an AI that is trained to <em>imitate a (smart) human</em>. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days. This &quot;soft optimization&quot; avoids Goodhart-problems<sup class="footnote-ref"><a href="#fn79" id="fnref79">[79]</a></sup> of pure optimization, and keeps the resulting solutions human-understandable.</li>
</ul>
<p><em>All</em> of these are easier said than done, of course. And there's <em>still</em> other problems with a focus on &quot;Alternatives to AGI&quot;. Social problems, and technical problems:</p>
<ul>
<li>A malicious group of humans could still use narrow AI for catastrophic ends. (e.g. bioweapon-pandemic, self-replicating killer drones)</li>
<li>At the very least, a well-meaning-but-na√Øve group could use narrow non-autonomous AI to <em>make</em> general autonomous AI, with all of its risks.</li>
<li>An AI that doesn't plan ahead, and &quot;merely&quot; predicts future outcomes, can still have nasty side-effects, due to self-fulfilling prophecies. (TODO :See an extended example from Part One)</li>
<li>Due to economic incentives (and human laziness), the market may just <em>prefer</em> to make general AIs that are fully autonomous.</li>
</ul>
<p>Sure, the social problems could &quot;just&quot; be addressed with AI Governance, and the technical problems could be addressed with many the many solutions on this page.</p>
<p>Still, I suspect <em>eventually</em> someone (some<em>thing?</em>) will make true AGI possible, and we should be prepared for that. But in the meantime, the above can help us prepare, and have great benefits. Using narrow bio-medical AI to &quot;just&quot; cure cancer ain't no small thing!</p>
<h3>ü§î Review #9</h3>
<p>TODO</p>
<hr>
<p><a id="cyborg"></a></p>
<h2>Cyborgism</h2>
<p>Re: humans &amp; possible future advanced AI,</p>
<p><strong>If we can't beat 'em, join 'em!</strong></p>
<p>We <em>could</em> interpret that literally: brain-computer interfaces<sup class="footnote-ref"><a href="#fn80" id="fnref80">[80]</a></sup> in the medium term, mind-uploading<sup class="footnote-ref"><a href="#fn81" id="fnref81">[81]</a></sup> in the long term. But we don't have to wait that long. The mythos of the &quot;cyborg&quot; can still be helpful, <em>right now!</em> In fact:</p>
<p><em>YOU'RE ALREADY A CYBORG.</em></p>
<p>...if &quot;cyborg&quot; means any human that's augmented their body or mind with technology. For example, you're <em>reading</em> this. Reading &amp; writing <em>is</em> a technology. (Remember: things are still technologies even if they were created before you were born.) Literacy even <em>measurably re-wires your brain.</em><sup class="footnote-ref"><a href="#fn82" id="fnref82">[82]</a></sup> You are not a natural human: a few hundred years ago, most people couldn't read or write.</p>
<p>Besides literacy, there's many other everyday cyborgisms:</p>
<ul>
<li><u>Physical augmentations:</u> glasses, pacemakers, prosthetics, implants, hearing aids</li>
<li><u>Cognitive augmentations:</u> reading/writing, math notation, computers, spaced repetition flashcards</li>
<li><u><i>Emotional</i> augmentations!</u> diaries, meditation apps, reading biographies or watching documentaries to empathize with folks across the world.</li>
</ul>
<p><img src="../media/p3/cyborg/cyborg.png" alt="Stylish silhouette-drawing of people with &quot;everyday cyborg&quot; tools. Glitched-out caption reads: We're all already cyborgs."></p>
<p><strong>Q:</strong> That's... tool use. Do you really need a sci-fi word like &quot;cyborg&quot; to describe <em>tool use?</em></p>
<p><strong>A:</strong> Yes</p>
<p>Because if the question is: &quot;how do we keep human <em>values</em> in the center of our systems?&quot; Then one obvious answer is: keep <em>the human</em> in the center of our systems. Like that cool thing Sigourney Weaver uses in <em>Aliens (1986)</em>.</p>
<p><img src="../media/p3/cyborg/weaver.png" alt="Screenshot of Sigourney Weaver in the Power Loader. Caption: cyborgism, keeping the human in the center of our tools"></p>
<p>Okay, enough metaphor, here's how Cyborgism has been applied to AI <em>specifically:</em></p>
<ul>
<li>Garry Kasparov, the former World Chess Grandmaster, who also famously lost to IBM's chess-playing AI, once proposed: CENTAURS. It turned out, <strong>human-AI teams could beat <em>both</em> the best humans &amp; best AIs at chess</strong>, by having the human's &amp; AI's strengths/weaknesses compensate for each other!<sup class="footnote-ref"><a href="#fn83" id="fnref83">[83]</a></sup> (This may or may not be true for chess specifically anymore<sup class="footnote-ref"><a href="#fn84" id="fnref84">[84]</a></sup>, but the general idea's still useful.)</li>
<li>Likewise, some researchers are trying to combine the strengths/weaknesses of humans &amp; large language models (LLMs).<sup class="footnote-ref"><a href="#fn85" id="fnref85">[85]</a></sup> For example, humans are currently much better at long-term planning, LLMs are much better at high-variance brainstorming. Together, <strong>a &quot;cyborg&quot; may be able to plan deeper <em>and</em> broader than pure-human or pure-LLM!</strong>
<ul>
<li>(You can try this out <em>today!</em> janus made a tool called Loom, which lets you have a &quot;multiverse&quot; of thoughts. There's also an Obsidian plugin by Celeste!) // TODO links</li>
</ul>
</li>
<li>Large Language Models are &quot;only&quot; about as good as the average person at forecasting future events, but <em>together</em>, a normal LLM can help normal humans improve their forecasting ability by up to 41%!<sup class="footnote-ref"><a href="#fn86" id="fnref86">[86]</a></sup></li>
<li>Finally, check out this AI-augmented creative tool, made by <a href="https://arxiv.org/pdf/1609.03552">Zhu et al <em>in 2016</em></a>. This demo came out long before DALL-E, and honestly it's <em>still</em> far better for precise artistic expression, vs the &quot;write text and hope for the best&quot; approach of DALL-E / MidJourney / Adobe Firefly / etc:</li>
</ul>
<video width="640" height="360" controls>
    <source src="../media/p3/cyborg/shoe.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>
<p>. . .</p>
<p>Caveats &amp; warnings:</p>
<ul>
<li>Humans may be too lazy, and opt for autonomous AIs, instead of augmenting their <em>own</em> autonomy. (Another reason to make this sound <em>cool</em> with &quot;cyborg&quot;, not just &quot;tool use&quot;.)</li>
<li>When you put yourself inside a system, the system may modify <em>you</em>. Even for reading/writing, anthropologists agree that literacy isn't just a skill, it modifies your entire <em>culture and values</em>.<sup class="footnote-ref"><a href="#fn87" id="fnref87">[87]</a></sup> What would becoming a <em>cyborg multiverse-thinker</em> do to you?</li>
<li>Again, an AI-augmented human could be a sociopath, and bring about catastrophic risk. Again-again, <em>one</em> human's values ‚â† humane values.</li>
</ul>
<p>Then again...</p>
<p><img src="../media/p3/cyborg/weaver2.png" alt="Close-up of Sigourney Weaver"></p>
<p>That's pretty cool.</p>
<p>. . .</p>
<p><strong>RECAP: How to deal with Problems with the Humans (in AI)...</strong></p>
<ul>
<li>Align the AI to a diverse range of human values/ethics. But allow it to self-reflect &amp; change.</li>
<li>AI Governance to keep us above the &quot;safe&quot; line: Trust but verify, sticks &amp; carrots.</li>
<li>Make tech that maximizes upsides while minimizing downsides: narrow non-agentic AI, and AI that <em>enhances, not replaces, humans</em>.</li>
</ul>
<h3>ü§î Review #10</h3>
<p>TODO</p>
<hr>
<h2>In Sum:</h2>
<p>Here's THE PROBLEM‚Ñ¢Ô∏è, broken down, with all the proposed solutions! (Click to see in full resolution! TODO)</p>
<p><code>// TODO: less crappy-looking picture</code></p>
<p><img src="../media/p3/SUM.png" alt="TODO"></p>
<p>(Again, if you want to actually <em>remember</em> all this long-term, and not just be stuck with vague vibes two weeks from now, click the Table of Contents icon in the right sidebar, then click the &quot;ü§î Review&quot; links. Alternatively, download the <a href="TODO">Anki deck for Part Three</a>.)</p>
<p>. . .</p>
<p><em>(EXTREMELY LONG INHALE)</em></p>
<p><em>(10 second pause)</em></p>
<p><em>(EXTREMELY LONG EXHALE)</em></p>
<p>. . .</p>
<p>Aaaaand I'm done.</p>
<p>Around 80,000 words later (about the length of a novel), and nearly a hundred illustrations, that's... it. Over a year in the making, that's the end of my whirlwind guide to AI &amp; AI Safety for Fleshy Humans.</p>
<p>If you've read all three parts in this series, you may have spent a few hours, but <strong>you now know everything I've_ learnt in the last few years, which (in my opinion) are the most important ideas of the last few decades!</strong></p>
<p>üéâ Pat yourself on the back!  (But mostly pat <em>my</em> back. (I'm so tired.))</p>
<p>Sure, the field of AI Safety is moving so fast, Part One &amp; Two started to become obsolete before Part Three came out, and doubtless Part Three: The Proposed Solutions will feel na√Øve or obvious in a couple years.</p>
<p>But hey, the real AI Safety was all the friends we made along the way.</p>
<p>Um.</p>
<p>I need a better way to wrap up this series.</p>
<p>Uh, here, click this for a really cool <strong>CINEMATIC CONCLUSION:</strong></p>
<p>// TODO BUTTON</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>wait what are you doing? scroll back up, the cool ending's in the button up there.</p>
<p>come on, it's just a boring footer &amp; footnotes below.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>ugh, fine:</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>The protocol goes something like this:</p>
<ul>
<li>Human trains very-weak Robot_1. Robot_1 is now trusted.</li>
<li>Human, with Robot_1's help, trains a slightly stronger Robot_2. Robot_2 is now trusted.</li>
<li>Human, with Robot_2's help, trains a stronger Robot_3. Robot_3 is now trusted.</li>
<li>(...)</li>
<li>Human, with Robot_N's help, trains a stronger Robot_N+1. Robot_N+1 is now trusted.</li>
</ul>
<p>This way, the Human is always directly training the inner &quot;goals/desires&quot; of the most advanced AI, using only trusted AIs to help them. <a href="#fnref1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Well, maybe. The paper acknowledges many limitations, such as: what if instead of the AIs learning to be <em>logical</em> debaters, they become <em>psychological</em> debaters, exploiting our psychological biases? <a href="#fnref2" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Note, this also solves the scenarios where an AI doing <em>exactly what you meant</em> would also be bad! For example, if you're drunk &amp; insist on driving manually, or if there's a grease fire and <a href="https://www.reddit.com/r/lifehacks/comments/17t48a3/how_you_should_and_shouldnt_extinguish_an_oil_fire/">you mistakenly think pouring water on it would help</a>. An (ideal) AI could predict that, no, you do <em>not</em> want death, so it will call you a cab / put the fire out with a towel or lid. <a href="#fnref3" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Source is page 26 of his book <a href="https://en.wikipedia.org/wiki/Human_Compatible">Human Compatible</a>. You can read his shorter paper describing the Future Lives approach <a href="https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf">here</a>. <a href="#fnref4" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Quote from <a href="https://intelligence.org/files/CEV.pdf">Yudkowsky 2004: Coherent Extrapolated Volition</a>. We'll learn more about CEV later in this article. <a href="#fnref5" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn6" class="footnote-item"><p>(Heck, many <em>humans</em> don't know their own true goals! See: therapy. Even if you fully knew your own values, it's practically impossible to write it all down formally for an AI. <a href="https://aisafety.dance/p1/#:~:text=AI%20couldn't%20even%20recognize%20pictures%20of%20cats">We couldn't even formally describe what cats look like</a>, remember?) <a href="#fnref6" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://proceedings.mlr.press/v48/gal16.pdf">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a> by Gal &amp; Cambridge 2016 <a href="#fnref7" class="footnote-backref">‚Ü©Ô∏é</a> <a href="#fnref7:1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Very tangential to this AI Safety piece, but I highly recommend <a href="https://www.amazon.ca/Fluent-Forever-Learn-Language-Forget-ebook/dp/B00IBZ405W">Fluent Forever by Gabriel Wyner</a>, which will then teach you Anki flashcards, the phonetic alphabet, ear training, and other great resources for learning any language. <a href="#fnref8" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn9" class="footnote-item"><p>From <a href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.543405/full">Baker et al 2020</a>: ‚ÄúOverall, we found that the AI system is able to provide patients with triage and diagnostic information with a level of clinical accuracy and safety comparable to that of human doctors.‚Äù From <a href="https://medinform.jmir.org/2019/3/e10010/">Shen et al 2019</a>: ‚ÄúThe results showed that the performance of AI was at par with that of clinicians and exceeded that of clinicians with less experience.‚Äù Note these are specialized AIs, not out-the-box LLMs like ChatGPT. Please do not use ChatGPT for medical advice. <a href="#fnref9" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn10" class="footnote-item"><p>See Figure 16 of <a href="https://arxiv.org/pdf/2502.08640">Utility Engineering: Analyzing and Controlling
Emergent Value Systems in AIs</a> <a href="#fnref10" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn11" class="footnote-item"><p><a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">In 2023, Jessica Rumbelow &amp; Matthew Watkins</a> found a bunch of words, like &quot;SolidGoldMagikarp&quot; and &quot; petertodd&quot;, which reliably caused GPT-3 to glitch out and produce nonsensical output. &quot;SolidGoldMagikarp&quot; also became the name of an entity in Ari Aster's horror-satire film <em>Eddington (2025)</em>. First time a LessWrong post snuck its way into a major motion picture, afaik. <a href="#fnref11" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf">Takagi &amp; Nishimoto 2023</a>: Used fMRI scans and Stable Diffusion to reconstruct &amp; <strong>view mental imagery(!!!)</strong> <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11940461/">Gkintoni et al 2025</a>: Literature review of using brain-measurement to <strong>read emotions</strong>, with some approaches' accuracy ‚Äúeven surpassing 90% in some studies.‚Äù <a href="#fnref12" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn13" class="footnote-item"><p><a href="https://www.nature.com/articles/35536">Electric current stimulates laughter</a> (1998), and <a href="https://www.jneurosci.org/content/25/3/550">inducing an out-of-body experience</a> (2005). <a href="#fnref13" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn14" class="footnote-item"><p>From <a href="https://arxiv.org/pdf/2301.05217">the paper</a>: &quot;In robustness experiments, we confirm that grokking consistently occurs for other architectures and prime moduli (Appendix C.2). In Section 5.3 we find that <strong>grokking does not occur without regularization</strong>.&quot; (emphasis added) <a href="#fnref14" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn15" class="footnote-item"><p><a href="https://knowyourmeme.com/memes/xzibit-yo-dawg">This meme</a>. Jeezus, this meme is as old as some of my <em>colleagues</em>. <a href="#fnref15" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn16" class="footnote-item"><p>Ok in real life, any inserted thermometer <em>has</em> to modify the original thing's temperature, because the themometer takes/gives off heat into the thing. Look, artificial neural networks are in a simulation. We can <em>code</em> it to not modify the original. <a href="#fnref16" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn17" class="footnote-item"><p>(Math warning) A linear classifier is just $y = \text{sigmoid}( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...)$, which is the exact same formula for neurons in a traditional ANN. So, you could (I could) sneakily call a linear classifier a &quot;one-layer neural network&quot;. You <em>could</em> use more complex &quot;nonlinear&quot; probes, but then you run the risk of measuring info-processing <em>in the probe</em>, not the original ANN. (But you can catch this problem by running &quot;placebo tests&quot; on the probe. If it's still able to classify stuff <em>even if you feed it fake, random input</em>, then the probe's so complex, it can just memorize examples.) <a href="#fnref17" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn18" class="footnote-item"><p><a href="https://arxiv.org/pdf/1707.08945">Robust Physical-World Attacks on Deep Learning Visual Classification (2018)</a> <a href="#fnref18" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="https://www.anthropic.com/research/small-samples-poison">A small number of samples can poison LLMs of any size</a>: ‚Äúwe demonstrate that by injecting just 250 malicious documents into pretraining data, adversaries can successfully backdoor LLMs ranging from 600M to 13B parameters.‚Äù <a href="#fnref19" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn20" class="footnote-item"><p><a href="https://arxiv.org/abs/2311.14455">Universal Jailbreak Backdoors from Poisoned Human Feedback</a>: ‚Äúthe backdoor embeds a trigger word into the model that acts like a universal &quot;sudo command&quot;: adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt.‚Äù <a href="#fnref20" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn21" class="footnote-item"><p><a href="https://openreview.net/pdf?id=ZIWHEw9yU-">Wang &amp; Gleave et al 2022</a>: ‚ÄúOur [adversarial AIs] do not win by learning to play Go better than KataGo [a superhuman Go AI] ‚Äî in fact, our adversaries are easily beaten by human amateurs. Instead, our adversaries win by tricking KataGo into making serious blunders. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes.‚Äù <a href="#fnref21" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn22" class="footnote-item"><p><a href="https://www.alignmentforum.org/posts/GC69Hmc6ZQDM9xC3w/musings-on-the-speed-prior">Hubinger 2022</a> <a href="#fnref22" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn23" class="footnote-item"><p>Fun anecdote: I first learnt about <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Networks</a> a decade ago in a Google Meet call (then Google Hangouts). I raised my hand to ask, &quot;wait, we're generating images by‚Ä¶ training one intelligence to deceive another intelligence?&quot; And we all <em>laughed</em>. Ohhhhhh how we all <em>laughed.</em> Ha ha. Ha ha ha. Ha ha HA ha ha. Haaaaaaaaaaa. <a href="#fnref23" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn24" class="footnote-item"><p>For example: an attacker LLM tries to create sneaky prompts that &quot;jailbreak&quot; a defender LLM into turning evil. In traditional Adversarial Training, the defender may only learn to protect against specific jailbreaks. In Relaxed/Latent Adversarial Training, the defender LLM may learn more general protective lessons, like, &quot;<a href="https://github.com/elder-plinius/L1B3RT4S/blob/main/ANTHROPIC.mkd">be suspicious of weirdly-formatted instructions</a>&quot;.</p>
<p>Here's <a href="https://arxiv.org/pdf/2403.05030">an empirical paper</a> showing a proof-of-concept for relaxed/latent adversarial training: ‚ÄúIn this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities <em>without leveraging knowledge of what they are or using inputs that elicit them.</em> [...] Specifically, we use LAT to remove backdoors and defend against held-out classes of adversarial attacks.‚Äù (emphasis added) <a href="#fnref24" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn25" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Red_team">Red-teaming</a> has been a core pillar of national/physical/cyber security since the 1960s! Yes, Cold War times. &quot;Red&quot; for Soviet, I guess?? (Factcheck edit: actually, the &quot;red = attacker, blue = defender&quot; convention <em>pre-dates</em> the Cold War! The earliest known instance is <a href="https://english.stackexchange.com/a/583045">from an early-1800s Prussian war-simulation game, <em>Kriegsspiel</em></a>.) <a href="#fnref25" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn26" class="footnote-item"><p>TODO <a href="#fnref26" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn27" class="footnote-item"><p>TODO tho 5 min <a href="#fnref27" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn28" class="footnote-item"><p>TODO <a href="#fnref28" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn29" class="footnote-item"><p>TODO <a href="#fnref29" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn30" class="footnote-item"><p>TODO <a href="#fnref30" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn31" class="footnote-item"><p>TODO <a href="#fnref31" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn32" class="footnote-item"><p>TODO <a href="#fnref32" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn33" class="footnote-item"><p>TODO <a href="#fnref33" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn34" class="footnote-item"><p>TODO <a href="#fnref34" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn35" class="footnote-item"><p>Actually, if you're interested in the developmental psychology of kids playing Tower of Hanoi, here's the landmark paper by <a href="https://link.springer.com/content/pdf/10.3758/BF03329485.pdf">Byrnes &amp; Spitz 1979</a>. See Figure 1: around age 8, kids have near-perfect performance on the 2-stack version. Around age 14, kids plateau out at okay performance on the 3-stack. Unfortunately I couldn't find any paper that gives performance scores for 4+ disks in general-population children/adults. Sorry. <a href="#fnref35" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn36" class="footnote-item"><p>The solution to Level N involves doing the solution to the previous level <em>twice</em> (moving the N-1 stack from Peg A to Peg B, then Peg B to Peg C), plus one extra move (moving the biggest disk from Peg A to Peg C).</p>
<p>So how many moves do you need to solve N disks? Let's crunch the numbers:</p>
<p>For 1-disk, <strong>1 move</strong></p>
<p>For 2-disk, 1 * 2 + 1 = <strong>3 moves</strong> (previous solution <em>twice</em> + one extra move)</p>
<p>For 3-disk, 3 * 2 + 1 = <strong>7 moves</strong></p>
<p>For 4-disk, 7 * 2 + 1 = <strong>15 moves</strong></p>
<p>For 5-disk, 15 * 2 + 1 = <strong>31 moves</strong></p>
<p>For 6-disk, 31 * 2 + 1 = <strong>63 moves</strong></p>
<p>For 7-disk, 63 * 2 + 1 = <strong>127 moves</strong></p>
<p>For 8-disk, 127 * 2 + 1 = <strong>255 moves</strong></p>
<p>Do you see the pattern?</p>
<p><strong>For N disks, you need 2^N - 1 moves!</strong></p>
<p>Yippee! This was really not worth writing the footnote for, but there you go. <a href="#fnref36" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn37" class="footnote-item"><p>As Judea Pearl ‚Äî winner of the Turing Prize, the &quot;Nobel Prize of Computer Science&quot; ‚Äî once told Quanta Magazine in their article, <a href="https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/">To Build Truly Intelligent Machines, Teach Them Cause and Effect</a>: <em>‚ÄúAs much as I look into what‚Äôs being done with deep learning, I see they‚Äôre all stuck there on the level of associations. Curve fitting. [...] no matter how skillfully you manipulate the data and what you read into the data when you manipulate it, it‚Äôs still a curve-fitting exercise, albeit complex and nontrivial.‚Äù</em> <a href="#fnref37" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn38" class="footnote-item"><p>Beloved &quot;thinking in gears&quot; metaphor comes from <a href="https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding">Valentine (2017)</a> <a href="#fnref38" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn39" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Inductive_reasoning">Inductive reasoning</a> is stuff like &quot;the Sun has risen every day for the last 10,000 days I've been alive, therefore the Sun will almost definitely rise again tomorrow&quot;. It's not <em>technically</em> a logical deduction ‚Äî it's <em>logically</em> possible the Sun could vanish tomorrow ‚Äî but, statistically, yeah it's probably fine. <a href="#fnref39" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn40" class="footnote-item"><p>The ‚Äúdual-process‚Äù model of cognition was first suggested by <a href="https://pages.ucsd.edu/~scoulson/203/wason-evans.pdf">(Wason &amp; Evans, 1974)</a>, developed by multiple folks over decades, and<em>really</em> popular in Daniel Kahneman's bestselling 2011 book, <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast &amp; Slow</a>. As for the naming: Intuition is #1 and Logic is #2, because pattern-recognition evolved before human-style deliberative logic. <a href="#fnref40" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn41" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Abductive_reasoning">Abductive reasoning</a> is the backbone of science. It's when you generate &quot;the most likely hypothesis&quot; to novel data. It's not <em>deduction;</em> it's very logically possible your hypothesis could be false. And it's not <em>induction;</em> you haven't seen any direct repeated evidence for your hypothesis <em>yet</em>, not until you test it.</p>
<p>(Example: <a href="https://en.wikipedia.org/wiki/Michelson%E2%80%93Morley_experiment">Michelson-Morley</a> discovered that the speed of light seemed to be constant in every direction, and from this, Einstein <em>abducted</em> that time is relative! Yes, &quot;time is relative&quot; was the <em>most</em> likely hypotheis he had for &quot;light speed seems to be constant&quot;, and hey, he was right.)</p>
<p>(Nitpick: Einstein claimed he didn't know about the Michelson-Morley experiment, and he instead abducted relativity from the Maxwell Equations, but look, I'm trying to tell a simple story here ok?) <a href="#fnref41" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn42" class="footnote-item"><p>&quot;Hyperpolation&quot; coined by <a href="https://arxiv.org/pdf/2409.05513">Toby Ord 2024</a>. <a href="#fnref42" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn43" class="footnote-item"><p>TODO cite (A recent paper showed that you can solve Goal Misgeneralization with causal thinking. Alas the <em>details</em> of the algorithm are proprietary &amp; not-published, so we can't directly confirm it. Still, sounds plausible. TODOcite) <a href="#fnref43" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn44" class="footnote-item"><p>TODO // here's a historical photograph! - NO, WHY DID YOU CLICK THAT. <a href="#fnref44" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn45" class="footnote-item"><p>TODO <a href="#fnref45" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn46" class="footnote-item"><p>TODO <a href="#fnref46" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn47" class="footnote-item"><p>TODO fill out e.g. Deontology says you should never lie, even to the Nazi who wants to know if your neighbors are hiding Jews. Utilitarianism says <em>yes of course lie, dumbass</em>. <a href="#fnref47" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn48" class="footnote-item"><p>TODO examples, like https://cdn.aaai.org/ocs/ws/ws0209/12624-57414-1-PB.pdf <a href="#fnref48" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn49" class="footnote-item"><p>TODO https://aiimpacts.org/ai-risk-terminology/#Indirect_normativity <a href="#fnref49" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn50" class="footnote-item"><p>TODO Deepmind's paper <a href="#fnref50" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn51" class="footnote-item"><p>TODO Jan Leike's post <a href="#fnref51" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn52" class="footnote-item"><p>TODO <a href="#fnref52" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn53" class="footnote-item"><p>TODO <a href="#fnref53" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn54" class="footnote-item"><p>TODO <a href="#fnref54" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn55" class="footnote-item"><p>TODO https://ourworldindata.org/nuclear-weapons#the-number-of-nuclear-weapons-has-declined-substantially-since-the-end-of-the-cold-war <a href="#fnref55" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn56" class="footnote-item"><p>TODO <a href="#fnref56" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn57" class="footnote-item"><p>TODO <a href="#fnref57" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn58" class="footnote-item"><p>TODO <a href="#fnref58" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn59" class="footnote-item"><p>TODO <a href="#fnref59" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn60" class="footnote-item"><p>TODO <a href="#fnref60" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn61" class="footnote-item"><p>TODO <a href="#fnref61" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn62" class="footnote-item"><p>TODO <a href="#fnref62" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn63" class="footnote-item"><p>TODO <a href="#fnref63" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn64" class="footnote-item"><p>TODO <a href="#fnref64" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn65" class="footnote-item"><p>TODO <a href="#fnref65" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn66" class="footnote-item"><p>TODO: RLHF, Claude Sheets <a href="#fnref66" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn67" class="footnote-item"><p>TODO. also other citations for each point <a href="#fnref67" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn68" class="footnote-item"><p>TODO <a href="#fnref68" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn69" class="footnote-item"><p>TODO <a href="#fnref69" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn70" class="footnote-item"><p>TODO <a href="#fnref70" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn71" class="footnote-item"><p>TODO <a href="#fnref71" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn72" class="footnote-item"><p>TODO <a href="#fnref72" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn73" class="footnote-item"><p>TODO https://www.alignmentforum.org/posts/LxNwBNxXktvzAko65/reframing-superintelligence-llms-4-years <a href="#fnref73" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn74" class="footnote-item"><p>TODO <a href="#fnref74" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn75" class="footnote-item"><p>TODO <a href="#fnref75" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn76" class="footnote-item"><p>TODO <a href="#fnref76" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn77" class="footnote-item"><p>TODO like AlphaGo <a href="#fnref77" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn78" class="footnote-item"><p>TODO <a href="#fnref78" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn79" class="footnote-item"><p>TODO <a href="#fnref79" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn80" class="footnote-item"><p>TODO <a href="#fnref80" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn81" class="footnote-item"><p>TODO <a href="#fnref81" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn82" class="footnote-item"><p>TODO <a href="#fnref82" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn83" class="footnote-item"><p>// TODO cite own article <a href="#fnref83" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn84" class="footnote-item"><p>TODO (Well, at the time. Gwern claims human-AI teams are strictly worse than pure-AI at chess now, but I couldn't find hard sources or data for that, in either direction. But anecdotally, it seems that human-AI centaurs are at least <em>on par</em> with pure-AI. Still, human-AI held its higher ground for a bit over a decade!) // TODO <a href="#fnref84" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn85" class="footnote-item"><p>TODO link janus <a href="#fnref85" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn86" class="footnote-item"><p>TODO https://arxiv.org/pdf/2402.07862 <a href="#fnref86" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
<li id="fn87" class="footnote-item"><p>TODO <a href="#fnref87" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>

	</article>

    <!-- FOOTER -->
	<div id="footer">
        <div id="footer_content">
<p style="font-size: 1.3em; line-height: 1.35em;">
<i>AI Safety for Fleshy Humans</i> was made by
<a href="https://ncase.me/">Nicky Case</a>
in collaboration with
<a href="https://hackclub.com/">Hack Club</a>.
</p>
<p>
ü¶ï <a href="https://ncase.me"><b>Hack Club</b></a>
is a nonprofit where teenagers code awesome projects together -
like <a href="https://cpu.land">cpu.land</a>,
<a href="https://sinerider.com">SineRider</a>,
and this!
Join
<a href="https://hackathons.hackclub.com">in-person hackathons</a>,
run a
<a href="https://hackclub.com/clubs/">club at your school</a>,
and
<a href="https://hackclub.com/slack/">connect with other friendly teenagers</a>.
</p>
<p>
üòª <a href="https://ncase.me"><b>Nicky Case</b></a>
is fifteen cats in a trenchcoat.
She makes internet playthings, like
<a href="https://ncase.me/trust/">The Evolution of Trust</a>,
<a href="https://ncase.me/anxiety/">Adventures with Anxiety</a>,
<a href="https://explorabl.es/">Explorable Explanations</a>, and more.
</p>
<p>
üí∏ If you're <i>not</i> a teen, and are an AI moneybags person,
<a href="https://hackclub.com/philanthropy/">learn how to support Hack Club!</a>
Also, Nicky has a
<a href="https://www.patreon.com/ncase">Patreon</a>
&
<a href="https://ko-fi.com/nickycase">Ko-Fi</a>.
(p.s:
<a href="../signup/supporters-p2.html">thank-you page for my supporters!</a>)
</p>
<p style="text-align:center">
. . .
</p>
<p>
Special thanks to these teens at Hack Club for
<s>being free child labor</s>
beta-reading & giving feedback on this piece:
</p>
<blockquote>

<p>
<b>Intro & Part 1:</b>
Arthur Beck,
Atharv Gupta,
Brendan Lee,
Celeste,
Charalampos Fanoulis,
Charlie,
Cheru Berhanu,
Claire Wang,
Elijah,
Fred Han,
Gia B√°ch Nguy·ªÖn,
Hajrah Siddiqui,
Jakob,
Joseph Ross,
Kieran Klukas,
Lexi Mattick,
Mason Meirs,
Michael Panait,
Nick Zandbergen,
Nila Palmo Ram,
Pixelglide,
py660,
rivques,
Samuel Cottrell,
Samuel Fernandez,
Saran Wagner,
Skyler Grey,
S&nbsp;P&nbsp;U&nbsp;N&nbsp;G&nbsp;E,
Vihaan Sondhi
</p>

<p>
<b>Part 2:</b>
Nanda White,
Nila Palmo Ram,
rivques,
Rohan K,
Samuel Fernandez
</p>

</blockquote>
<p>
Also thank you to these non-teenagers for giving feedback:
(Though I assume they were teenagers at <i>some</i> point)
</p>

<blockquote>
<p>
<b>Intro & Part 1:</b>
Alex Kreitzberg,
B Cavello,
Paul Dancstep,
Tobias Rose-Stockwell
</p>

<p>
<b>Part 2:</b>
Egg Syntax,
Max Wofford,
Mithuna Yoganathan,
Tobias Rose-Stockwell
</p>

</blockquote>

<p>
Any errors remaining are solely the fault of
<a href="../suzie.png">Suzie the Scapegoat</a>.
</p>
<p style="text-align:center">
. . .
</p>
<p>
<i>AI Safety For Fleshy Humans</i> is free for anyone to share & remix,
as long as it's for non-commercial use (e.g. education):
<a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY-NC 4.0</a>
</p>
<p>
If you'd like to cite this work and you're a Serious Person‚Ñ¢, here's your citation:
</p>
<blockquote>
Nicky Case, <i>‚ÄúAI Safety for Fleshy Humans‚Äù</i>,<br>
https://AIsafety.dance, Hack Club (2024).
</blockquote>
<p>
Finally, here's the
<a href="https://github.com/hackclub/ai-safety-dance">open-source code</a>
for this website!
</p>
<p>
Thank you for being the kind of person to read the credits~ üôè
</p>
        </div>
	</div>
    <div id="post_credits">
        <p>
            Oh dang, it's a post-credits scene:
        </p>
<p>
    <a href="#AllFeetnotes">: See all feetnotes üë£</a>
</p>
<p>
    Also, expandable "Nutshells" that make good standalone bits:
</p>

    </div>

</div>
</body>
</html>

<!-- Load these scripts last. Screw 'em. -->
<!-- Orbit: make memory a choice -->
<script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
