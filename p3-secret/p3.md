### This is a secret unlisted draft for beta-readers! Please don't share (yet), thank you for gifting me feedback! You'll be credited in the credits if you wish :3

*(This is Part 3 of a series on AI Safety! You don't* have *to read the previous parts ‚Äî [Intro](https://aisafety.dance/), [Part 1](https://aisafety.dance/p1/), [Part 2](https://aisafety.dance/p2/) ‚Äî but they'll help!)*

So, after writing 40,000+ words on how weird & difficult AI Safety is... how am I feeling about the chances of humanity solving this problem?

...pretty optimistic, actually!

No, really!

Maybe it's just cope. But in my opinion, if *this* is the space of all the problems:

![Drawing of a faintly outlined blob, labelled "the whole problem space"](../media/p3/intro/problems.png)

Then: although no *one* solution covers the whole space, *the entire problem space* is covered by one (or more) promising solutions:

![Same outline, but entirely overlapped by small colorful circles, each one representing a different solution](../media/p3/intro/solutions.png)

**We don't need One Perfect Solution; we can stack several imperfect solutions!** (This is similar to [:the Swiss Cheese Model in Risk Analysis](#SwissCheese). üëà _optional: click to expand dotted-underlined texts!_)

*This does not mean AI Safety is 100% solved yet* ‚Äî we still need to triple-check these proposals, and get engineers/policymakers to even *know* about these solutions, let alone implement them. But for now, I'd say: "lots of work to do, but lots of promising starts"!

As a reminder, here's how we can break down the main problems in AI & AI Safety:

// PICTODO: combo of both

So in this Part 3, we'll learn about the most-promising solution(s) for each part of the problem, while being honest about their pros, cons, and unknowns:

**ü§ñ Problems in the AI:**

* <u>Scalable Oversight</u>: How can we safely check AIs, even when they're *far* more advanced than us? [‚Ü™](#oversight)
* <u>Solving AI Logic</u>: AI should aim for our "future lives" [‚Ü™](#future), and learn our values with uncertainty [‚Ü™](#uncertain).
* <u>Solving AI "Intuition"</u>: AI should be easy to "read & write" [‚Ü™](#interpretable), be robust [‚Ü™](#robust), and think in cause-and-effect. [‚Ü™](#causality)

**üò¨ Problems in the Humans**:

* <u>Humane Values</u>: Which values, *whose* values, should we put into AI, and how? [‚Ü™](#humane)
* <u>AI Governance</u>: How can we coordinate humans to manage AI, from the top-down and/or bottom-up? [‚Ü™](#governance)

**üåÄ Working *around* the problems**:

* <u>Alternatives to AGI</u>: How about we just don't make the Torment Nexus? [‚Ü™](#alt)
* <u>Cyborgism</u>: If you can't beat 'em, join 'em! [‚Ü™](#cyborg)

(If you'd like to skip around, the <img src="../media/intro/icon1.png" class="inline-icon"> Table of Contents are to your right! üëâ You can also <img src="../media/intro/icon2.png?v=3" class="inline-icon"> change this page's style, and <img src="../media/intro/icon3.png?v=2" class="inline-icon"> see how much reading is left.)</p>

Quick aside: **this final part, published on November 2025,** was supposed to be posted 11 months ago. But due to a bunch of personal shenanigans I don't want to get into, I was delayed. Sorry you've waited almost a year for this finale! On the upside, there's been lots of progress & research in this field since then, so I'm excited to share all that with you, too.

Alright, let's dive in! No need for more introduction, or weird stories about cowboy catboys, let's just‚Äî

#### :x Swiss Cheese

[The famous Swiss Cheese Model](https://en.wikipedia.org/wiki/Swiss_cheese_model) from Risk Analysis tells us: **you don't need _one_ perfect solution, you can stack _several_ imperfect solutions:**

!["Rays going through layers of swiss cheese, which have holes in them. The rays can easily go through the holes of _one_ layer of cheese, but not all of them."](../media/p3/intro/cheese.png)

_(image by [Ben Aveling](https://en.wikipedia.org/wiki/File:Swiss_cheese_model_textless.svg) on Wikipedia (CC-BY-SA))_

This model's been used _everywhere_, from aviation to cybersecurity to pandemic resilience. An imperfect solution has "holes", which are easy to get through. But stack enough of them, with holes in different places, and it'll be near-impossible to bypass.

But, let's address two critiques of the Swiss Cheese model:

[Critique One](https://www.eurocontrol.int/publication/revisiting-swiss-cheese-model-accidents) ‚Äî this assumes the "holes" in the solutions are _independent_. If there's any _one_ hole that all solutions share, then a problem can slip through all of them. Fair enough. **This is why the Proposed Solutions on this page try to be as diverse as possible.**

Critique Two ‚Äî more relevant to AI Safety ‚Äî is it assumes the problem is not an intelligent agent. [A quote from Nate Soares](https://archive.is/20250921161137/https://www.vox.com/future-perfect/461680/if-anyone-builds-it-yudkowsky-soares-ai-risk):

> ‚ÄúIf you ever make something that is trying to get to the stuff on the other side of all your Swiss cheese, it‚Äôs not that hard for it to just route through the holes.‚Äù

I accept this counterargument when it comes to defending against an _already_-misaligned superintelligence. But if we're talking about _growing a trusted intelligence from scratch_, then the Swiss Cheese Model still makes sense at each step, *and*, you can use each iteration of a trusted AI as an extra "layer of cheese" for training better iterations of that AI. (See below section: Scalable Oversight)

As AI researcher Jan Leike [puts it](https://aligned.substack.com/p/should-we-control-ai):

> More generally, we should actually solve alignment instead of just trying to control misaligned AI. [...] Don‚Äôt try to imprison a monster, build something that you can actually trust!

---

<a id="oversight"></a>

## Scalable Oversight

This is Sheriff Meowdy, the cowboy catboy:

![Drawing of Sheriff Meowdy](../media/p3/so/meowdy0001.png)

One day, the Varmin strode into town:

![Sheriff Meowdy staring down a bunch of Jerma rats strolling towards him](../media/p3/so/meowdy0002.png)

Sharpshootin' as the Sheriff was, he's man enough (catman enough) to admit when he needs backup. So, he makes a robot helper ‚Äî Meowdy 2.0 ‚Äî to help fend off the Varmin:

![Robot version of Sheriff Meowdy, labelled "Meowdy 2.0"](../media/p3/so/meowdy0003.png)

Meowdy 2.0 can shoot twice as fast as the Sheriff, but there's a catch: Meowdy 2.0 *might* betray the Sheriff. Thankfully, it takes time to turn around & betray the Sheriff, and the Sheriff is still fast enough to stop Meowdy 2.0 if it does that.

This is **oversight.**

![Sheriff Meowdy watching 2.0, with a gun to its head. 2.0 can turn around in 500ms, Meowdy can react & shoot in 200ms.](../media/p3/so/meowdy0004.png)

Alas, even Meowdy 2.0 *still* ain't fast enough to stop the millions of Varmin. So Sheriff makes Meowdy 3.0, which is twice as fast as 2.0, or *four times* as fast as the Sheriff.

This time, the Sheriff has a harder time overseeing it:

![3.0 can turn around in 250ms, Meowdy can only still react in 200ms. Meowdy is sweating.](../media/p3/so/meowdy0005.png)

But Meowdy 3.0 *still* ain't fast enough. So the Sheriff makes Meowdy 4.0, who's twice as fast as 3.0...

...and this time, it's so fast, the Sheriff can't react if 4.0 betrays him:

![4.0 can turn around in 125ms, which is fast enough to betray Meowdy. 4.0 shoots Meowdy dead.](../media/p3/so/meowdy0006.png)

So, what to do? The Sheriff strains all two of his orange-cat brain cells, and comes up with a plan: **_scalable_ oversight!**

He'll oversee 2.0, which can oversee 3.0, which *can* oversee 4.0!

![Meowdy oversees 2.0 oversees 3.0 oversees 4.0](../media/p3/so/meowdy0007.png)

In fact, why stop there? This harebrained "scalable oversight" scheme of his will let him oversee a Meowdy of *any* speed!

So, the Sheriff makes 20 Meowdy's. Meowdy 20.0 is 2<sup>20</sup> ~= one *million* times faster than the Sheriff: plenty quick enough to stop the millions of Varmin!

![Meowdy oversees a chain of Meowdy's, up to Meowdy 20.0, who can shoot all the Varmin dead.](../media/p3/so/meowdy0008.png)

(Wait, isn't a single chain of oversight fragile? As in, if _one_ of the Meowdy's break, the entire chain is broken? Yes! One solution is to have _multiple_ interwined chains, like so:)

// PICTODO

(This way, if any _one_ overseer at Level N gets corrupted, there'll still be two others checking the bots at Level N+1. And the overseers at Level N-1 can catch & fix the corrupted overseer. Also, it's important the overseers are independent from each other, so their failures aren't correlated. We'll learn more about how to create Robustness in a later section.)

Anyway, in sum, the core insight of scalable oversight is this meme:

![The "domino" meme, where a small domino knocks over a bigger one, which knocks over a bigger one, until it can knock over a huge domino.](../media/p3/so/meme.png)

(Another analogy: sometimes, boats are so big, the rudder *has its own smaller rudder*, called a [trim tab](https://en.wikipedia.org/wiki/Trim_tab#As_a_metaphor). This way, you can steer the small rudder, which steers the big rudder, which steers the whole boat.)

You may notice this is similar to the idea of "recursive self-improvement" for AI *Capabilities:* an advanced AI makes a *slightly more* advanced AI, which makes another more advanced AI, etc. Scalable Oversight is the same idea, but for AI *Safety:* one AI helps you align a slightly more advanced AI, etc!

(Ideas like these, where case number N helps you solve case number N+1, etc, are called "inductive" or "iterative" or "recursive". Don't worry, you don't need to remember that jargon, just thought I'd mention it.)

Anywho: with the power of friendship, math, and a bad Wild West accent...

... the mighty Sheriff Meowdy has saved the townsfolk, once more!

![Sheriff Meowdy blowing the smoke away from his gun, as the injured Varmin waltz off into the sunset](../media/p3/so/meowdy0010.png)

(üëâ [: click to expand bonus section - the "alignment tax", P = NP?, alignment vs control, what about sudden-jump "sharp left turns"?](#ScalableOversightExtras))

. . .

Now that the visual hook is over, here's the quick sponsor intermission. This series was funded by **Hack Club** and the **Long Term Future Fund**; thank you both for helping me pay rent the last two years. [Hack Club](https://hackclub.com/) helps teenage hackers around the world support each other. [Long Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) throws money at stuff that helps reduce P(doom). // TODO "reach out if"

Also, since this is the *last* entry in this series, if you wanna stay up to date on my future math/science/AI-explainer-y projects, you can sign up for [my YouTube channel](https://www.youtube.com/@itsnickycase) or [my once-a-month newsletter](https://ncase.me/sub/):

<iframe src="https://ncase.me/ncase-credits/signup2.html" frameborder="no" width="640" height="250"></iframe>

Alright, back to the show!

. . .

A behind-the-scenes note: the above Sheriff Meowdy comic was the *first* thing in this series that I drew... [almost three years ago](https://www.patreon.com/posts/sheriff-meowdy-78016588). (Kids, don't do longform content on the internet, it ain't worth it.) Point is: learning about Scalable Oversight was *the one idea* that made me the most optimistic about AI Safety, and inspired me to start this whole series in the first place!

Because, Scalable Oversight turns _this_ seemingly-impossible problem:

> *"How do you avoid getting tricked by something that's 100 times smarter than you?"*

...into this much-more feasible problem:

> *"How do you avoid getting tricked by something that's only 10% smarter than you, and ALSO you can raise it from birth, read its mind, and nudge its brain?"*

To be clear, the above problem isn't solved yet, either. But it's a huge step!

`// PICTODO: all the above in one picture?`

Anyway, that's the *general* idea. Here's some *specific* implementations & empirical findings:

* üíñ‚û°Ô∏èüíñ **[Recursive Reward Modeling](https://arxiv.org/abs/1811.07871)** uses a Level-N bot not just to check & control a Level-(N+1) bot *in hindsight*, but to *train its intrinsic "goals & desires" in the first place.*[^rrm-detail] As AI researcher Jan Leike [put it](https://aligned.substack.com/p/should-we-control-ai): _‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù_
* üìà **[Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)** _quantitatively_ measures how many oversight levels/steps you need to maximize oversight success. Encouragingly, like the previous "scaling laws" for how AI improves as you feed them more compute, these "scaling laws of scalable oversight" also seem to be predictable & measurable. [Terekhov & Liu et al 2025] goes further and calculates the "control tax" in $, for various LLM attackers and defenders.
* üïµÔ∏èü¶π **[GPT-3.5 can catch sneaky, subtly-harmful code, written by the more-powerful GPT-4.
  ](https://arxiv.org/abs/2312.06942)**
* üïµÔ∏èü•∑ **[Prover-Verifier Games](https://openai.com/index/prover-verifier-games-improve-legibility/)** pits a weak "math proof verifier" AI against two stronger AIs: a "helpful prover" that gives true proofs, and a "sneaky prover" that gives false proofs that *seem* true. Result of training on this game: the *weak verifier* can get good enough to distinguish true proofs, but the *strong sneaky prover can't win in the long run.* Nice!
* ü™ú **[Weak-to-Strong Generalization](https://openai.com/index/weak-to-strong-generalization/)** found that GPT-*2* did a pretty okay job at supervising & training GPT-*4*. (an AI that's several orders of magnitude bigger)
* üí¨üí¨ **[Debate](https://openai.com/index/debate/)** is slightly different. Instead of a weaker AI overseeing a stronger AI, Debate has 2 equally-powerful AIs debate each other, picking apart each others' logic. As long as truth can survive scrutiny better than falsehood, the truth will win. (Well, maybe.[^debate-detail])
* üëØ‚Äç‚ôÄÔ∏è **[Iterated Distillation & Amplification (IDA)](https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate)** is even safer ‚Äì instead of overseeing AIs slightly-smarter than you, you oversee AIs that are *strictly weaker* than you. How, then, can this setup give you superhuman intelligence? To very loosely summarize: in IDA, you become the CEO of an organization made out of AIs trained to imitate you! Since every AI imitates you, and you're the CEO of the whole thing, the system as a whole is aligned to your values ‚Äî while *also* amplifying your capabilities. ([: More about IDA, and critiques of it](#IDA))

(Though, it's good to remain healthily critical. Here's some recent "downer" findings: if you train _directly_ on an AI's chain of thought, it can learn to hide its evil plans _even if you can "read its mind"_. [Baker & Huizinga 2025](https://arxiv.org/pdf/2503.11926). And, given na√Øve oversight techniques, it's actually pretty easy for a _weaker_ AI to jailbreak a _stronger_ monitor. [TODO paper](TODO) )

[^rrm-detail]: The protocol goes something like this:    

    - Human trains very-weak Robot_1. Robot_1 is now trusted.
    - Human, with Robot_1's help, trains a slightly stronger Robot_2. Robot_2 is now trusted.
    - Human, with Robot_2's help, trains a stronger Robot_3. Robot_3 is now trusted.
    - (...)
    - Human, with Robot_N's help, trains a stronger Robot_N+1. Robot_N+1 is now trusted.
    
    This way, the Human is always directly training the inner "goals/desires" of the most advanced AI, using only trusted AIs to help them.

[^debate-detail]: Well, maybe. The paper acknowledges many limitations, such as: what if instead of the AIs learning to be *logical* debaters, they become *psychological* debaters, exploiting our psychological biases?

But overall, I'm still optimistic! Again, we don't need _one_ perfect solution, you can stack lots of imperfect solutions. ([: click here to read more about the Swiss Cheese Model, if you skipped it before](#SwissCheese))

So: *IF* we can align a slightly-smarter-than-us AI, *THEN*, through Scalable Oversight, we can align far more advanced AIs.

...but right now, we can't even align *dumber*-than-us AIs.

That's what the next few proposed solutions aim to fix!  But first...

#### :x Scalable Oversight Extras

Some extra concepts & comments that don't fit in the main post:

**"Alignment Tax":** how much extra overhead do you need to make sure the whole system remained aligned & controllable? If the "tax" is too high, firms will be incentivized to cut corners, and risk a misaligned AI. So: how big is the "tax"?

In the above Meowdy example, to oversee a bot of speed $X$, because each bot can oversee a bot 2x faster than it, you need an extra $log_2(X)$ bots to safely control it. (Then, if you want $k$ intertwined parallel chains for robustness, add $k*log_2(X)$ bots.)

And in general, as the [Scaling Laws for Scalable Oversight paper](https://arxiv.org/abs/2504.18530) shows, for any given task, bots can usually oversee a bot that's some fixed ratio "better" than it. If that's the case, then to oversee a bot of capability $X$, you need some logarithmic number of extra bots for scalable oversight.

Logarithms are the inverse of exponentials; so like how exponentials grow _really fast_, logarithms grow _really slow_. This is encouraging: this means our "alignment tax" overhead for powerful AIs grows slowly & can be easily affordable!

**What if P = NP?**

The Scalable Oversight paradigm assumes that it's always easier to _verify_ solutions, than to _create_ solutions: this is how a weaker AI can verify the safety/correctness of the actions of a stronger AI. For example: it's hard to solve a Rubik's cube or Sudoku puzzle, but nearly trival to _check_ if one is solved.

However: there's a currently open question in computer science, with a million-dollar prize attached: P = NP? In sum, it asks: are all problems that are _easy to check_ secretly _easy to solve?_ It seems intuitively not (and most computer scientists believe it's false, that is, P ‚â† NP) but it's still not been proven. As far as we know, it _could_ be the case that P = NP, and so that every problem that's easy to check is also easy to solve.

Does this mean, if P=NP, the Scalable Oversight paradigm fails? No! Because P = NP "only" means that it's not _exponentially_ harder to find solutions than check solutions. (or, to be precise: it's only "polynomially" harder at best, that's the "P" in "P" and "NP".) But finding a solution _is still harder_, just not exponentially so.

Two examples, where we've _proven_ how much time an optimal solution takes: 

- The optimal way to sort a list takes $\mathcal{O}(n\log{}n)$ time, while checking a list is sorted takes $\mathcal{O}(n)$ time.
- The optimal way [to find a solution to a black-box problem on a quantum computer](https://en.wikipedia.org/wiki/Grover%27s_algorithm) takes $\mathcal{O}(\sqrt{n})$ time, but checking that solution takes a constant $\mathcal{O}(1)$ time.

($\mathcal{O}(\text{formula})$ means "in the long run, the time this process takes is proportional to this formula.")

So even if P = NP, as long as it's _harder_ to find solutions than check them, Scalable Oversight can work. (But the alignment tax will be higher)

**Alignment vs Control:**

Aligned = the AI's "goals" are the same as ours.

Control = we can, well, control the AI. We can adjust it & steer it.

Some of the below papers are from the sub-field of "AI Control": how do we control an AI _even if it's misaligned?_ (As shown in the Sheriff Meowdy example, the Meowdy bots will shoot him the moment they can't be controlled. So, they're misaligned.)

To be clear, folks in the AI Control crowd recognize it's not the "ideal" solution ‚Äî as AI researcher Jan Leike [put it](https://aligned.substack.com/p/should-we-control-ai), _‚ÄúDon‚Äôt try to imprison a monster, build something that you can actually trust!‚Äù_ ‚Äî but it's still worth it as an extra layer of security.

Interestingly, it's also possible to have Alignment _without_ Control: you could imagine an AI that _correctly_ learns humane values & what flourishing for all sentient beings looks like, then takes over the world as a benevolent dictator. It understands that we'll be uncomfortable ceding control, but it's worth it for world peace, and will rule kindly. (And besides, 90% of you keep fantasizing about living in land of kings & queens anyway, admit it, you humans _want_ to be ruled by a dictator. /half-joke)

**Sharp Left Turns:**

Scalable Oversight also depends on capabilities smoothly scaling up. And not something like, "if you make this AI 1% smarter, it'll gain a brand new capability that lets it absolutely crush an AI that's even 1% weaker than it."

This possibility sounds absurd, but there's precedent for sudden-jump "phase transitions" in physics: slightly below 0¬∞C, water becomes ice. And slightly above 0¬∞C, water is liquid. So could there be such a "phase transition", a "sharp left turn", in intelligent systems? 

Maybe? But:

1) Even in the physics example, ice doesn't freeze _instantly;_ you can feel it getting colder, and you have hours or days to react before it fully freezes over. So, even if a "1% smarter AI" gains a radically new capability, the "1% dumber overseer" may still have time to notice & stop it.

2) As you'll see later in this section, the _is_ a Scalable Oversight proposal, called Iterated Distillation & Amplification, where overseers oversee only _strictly "dumber"_ AIs, yet the system as a whole can still be smarter! Read on for details.


#### :x IDA

To understand Iterated Distillation & Amplification (IDA), let's consider its biggest success story: AlphaGo, the first AI to beat a world champion at Go.

Here were the steps to train AlphaGo:

- Start with a dumb, random-playing Go AI.
- **DISTILL:** Have two copies play against each other. Through self-play, _learn an "intuition" for good/bad moves & good/bad board states._ (using an [artificial neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)))
- **AMPLIFY:** Plug this "intuition module" into a Good Ol' Fashioned AI, that simply thinks a few moves & counter-moves ahead, then picks the next best move. ([Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)) This gives you a slightly-less-dumb Go AI.
- **ITERATE:** Repeat. The two less-dumb AIs play against each other, learn a better "intuition", thus get better at game tree search, and thus get better at playing Go.
- Repeat over and over  until your AI is superhuman at Go!

// PICTODO

Even more impressive, this same system could also learn to be superhuman at chess & shogi ("Japanese chess"), _without ever learning from endgames or openings_. Just lots and lots of self-play.

( A caveat: the resulting AIs are only as robust as ANNs are, which aren't very robust. A superhuman Go AI can be beaten by a "bad" player who simply tries to take the AI into insane board positions that would never naturally happen, in order to break the AI. ([Wang & Gleave et al 2023](https://proceedings.mlr.press/v202/wang23g/wang23g.pdf)) )

Still, this is strong evidence that IDA works. But even better, as [Paul Christiano pointed out & proposed](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616), IDA could be used for scalable Alignment.

Here's a paraphrase of how it'd work:

- Start with you, the Human
- **DISTILL:** Train an AI to imitate _you_, your values, trade-offs, and reasoning style. This AI is _strictly weaker_ than you, but can be run faster.
- **AMPLIFY:** You want to solve a big problem? Carve up the problem into smaller parts, hand them off to your Slightly-Dumber-But-Much-Faster AI clones, then recombine them into a full solution. (For example: I want to solve a math problem. I come up with _N_ different approaches, then ask _N_ clones to try out each one, then report what they learn. I read their reports, then if it's still not solved, I think up of _N_ more possible approaches & ask the clones to try again. Repeat until solved.)
- **ITERATE:** For the next distillation step, train an AI to imitate _the you + clones system as a whole_. Then for the next amplification step, you can query multiple clones of _that_ system to help you break down & solve big problems.
- Repeat until you are the CEO of a superhuman "company of you-clones"!

// PICTODO

I think IDA is one of the cooler & more promising proposals, but it's worth mentioning a few critiques / unknowns:

- Distillation: As shown in the above AlphaGo example, IDA's quality is limited by the Distillation step. Right now we don't know how to make robust ANNs, and we don't know if this Distillation step would preserve your values enough.
- Amplification: While it seems most big problems in real life can be broken up into smaller tasks (this is why engineering teams aren't just one person), it's unclear if _epiphanies_ can be broken up & delegated. Maybe you really do need _one_ person to store all the info in their head, as fertile soil & seeds to grow new insights, and you can't "carve up" the epiphany process, any more than you can carve up a plant and expect it to still grow.
- Iteration: Even if a _single distill-and-amplify_ step more-or-less preserves your values, it's unknown if any errors would _accumulate_ over multiple steps, let alone if the error grows exponentially. (As you may be painfully aware if you've ever worked in a big organization, an org can grow to be _very_ misaligned from the original founders' values.)

Also, if _you_ don't get along well with yourself, [becoming the "CEO of a company of you's" will backfire](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fufid3duzqw991.jpg).

(See also: [this excellent Rob Miles video on IDA](https://www.youtube.com/watch?v=v9M2Ho9I9Qo))

### ü§î (Optional!) Flashcard Review #1

You read a thing. You find it super insightful. Two weeks later you forget everything but the vibes.

That sucks! So, here are some *100% OPTIONAL* Spaced Repetition flashcards, to help you remember these ideas long-term! ( üëâ [: Click here to learn more about spaced repetition](https://aisafety.dance/#SpacedRepetition)) You can also download these as an Anki deck. TODO

```
We don't need One Perfect Solution;
we can stack several imperfect solutions!

The Swiss Cheese Model TODO

What is Scalable Oversight?
Using an AI to oversee a slightly-smarter AI, repeat
// meme

How can you make scalable oversight more robust?
By adding extra, independent overseers
// chains

**Scalable Oversight** lets us convert the impossible question, "How do you oversee a thing that's 1000x smarter than you?" to the more feasible, "How do you oversee a thing that's only a bit smarter than you, _and_ you can train it from scratch, read its mind, and nudge its thinking?"
```

Good? Let's move on...

---

<a id="future"></a>

## AI Logic: Future Lives

You may have noticed a pattern in AI Safety paranoia.

First, we imagine giving an AI an innocent-seeming goal. Then, we think of a bad way it could _technically_ achieve that goal. For example:

* <u>"Pick up dirt from the floor"</u> ‚Üí Knocks all the potted plants over so it can pick up more dirt.
* <u>"Calculate digits of pi"</u> ‚Üí Deploys a computer virus to steal as much compute power as possible, to calculate digits of pi.
* <u>"Help everyone feel happy & fulfilled"</u> ‚Üí Hijacks drones to airdrop aerosolized LSD and MDMA.

Note: these are *NOT* problems with the AI being sub-optimal. These are problems *because* the AI is acting optimally! (We'll deal with sub-optimal AIs later.) Remember, like a cheating student or disgruntled employee, it's not that the AI may not "know" what you really want, it's that it may not "care". (To be less anthropomorphic: a piece of software will optimize for exactly what you coded it to do. No more, no less.)

**"Think of the worst that could happen, in advance. Then fix it."** If you recall, this is [Security Mindset](https://aisafety.dance/p2/#:~:text=Does%20all%20this%20seem%20paranoid), the engineer mindset that makes bridges & rockets safe, and makes AI researchers so worried about advanced AI.

But what if... we made an AI that *used Security Mindset against itself?*

For now, let's assume an "optimal" AI ‚Äî (again, we'll tackle sub-optimal AIs later) ‚Äî that can predict the world perfectly. And since _you're_ part of the world, it can predict _how you'd react to various outcomes_ perfectly.

**Then, here's the "Future Lives" algorithm:**

1. Human asks Robot to do something
2. Robot considers its possible actions, and the results of those actions
3. Robot predicts how _the current version of you_ would react to _those futures_.
4. It does the action whose future you'd most approve of. Crucially, like in Security Mindset, it does _not_ do things that would make current-you say, "no that's not what I wanted".[^meant-vs-want]

[^meant-vs-want]: Note, this also solves the scenarios where an AI doing _exactly what you meant_ would also be bad! For example, if you're drunk & insist on driving manually, or if there's a grease fire and [you mistakenly think pouring water on it would help](https://www.reddit.com/r/lifehacks/comments/17t48a3/how_you_should_and_shouldnt_extinguish_an_oil_fire/). An (ideal) AI could predict that, no, you do _not_ want death, so it will call you a cab / put the fire out with a towel or lid.

(Note: Why predict how _current_ you would react, not future you? To avoid an incentive to "wirehead" you into a dumb brain that's maximally happy. Why a whole future, not just an outcome at a point in time? To avoid unwanted means towards those ends, and/or unwanted consquences after those ends.)

(Note 2: For now, we're also just tackling the problem of how to get an AI to fulfil _one_ human's values, not _humane_ values. We'll look at the "humane values" problem later in this article.)

// PICTODO example - clean my house

As Stuart Russell, the co-author of the most-used textbook in AI, once put it:[^hcai-source]

> \[Imagine\] if you were somehow able to watch two movies, each describing in sufficient detail and breadth a future life you might lead \[as well as the consequences outside of & after your life.\] You could say which you prefer, or express indifference.

[^hcai-source]: Source is page 26 of his book [Human Compatible](https://en.wikipedia.org/wiki/Human_Compatible). You can read his shorter paper describing the Future Lives approach [here](https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf).

(These kinds of approaches ‚Äî where instead of directly telling an AI our values, we ask it to _learn_ what our values are ‚Äî is called **"indirect normativity"**. It's called that because ~~academics are bad at naming things~~ "normativity" ~means "values", and "indirect" because we're showing it, not telling it.)

And voil√†! That's how we make an (optimal) AI use Security Mindset _on itself:_

**If we could _even in principle_ come up with a Security Mindset-style problem with an AI's action, this AI would already predict that, and avoid doing that!** _‚ÄúIf we scream, the rules change; if we predictably scream later, the rules change now.‚Äù_[^cev-quote]

[^cev-quote]: Quote from [Yudkowsky 2004: Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf). We'll learn more about CEV later in this article.

. . .

_Hang on_, you may think, _I can already think of ways the Future Lives approach can go wrong, even with an optimal AI:_

* This locks us in into our _current_ values, no room for personal/moral growth.
* Whether or not we approve of something is sensitive to psychological tricks, e.g. seeing a thing for "$20", versus "~~$50~~ $20 (SALE: $30 OFF!!!)". The "movies" of possible future lives could be filmed in an emotionally manipulative way.
* If the truth is upsetting ‚Äî like when we discovered Earth wasn't the center of the universe ‚Äî the current-us would disapprove of learning about uncomfortable truths.
* I contain multitudes, I contradict myself. What happens if, when presented different pairs of futures, I'd prefer A over B, B over C, _and C over A?_ What if at Time 1 I want one thing, at Time 2 I predictably want the opposite?

If you think these would be problems... you'd be correct!

In fact, since _you right now_ can see these problems‚Ä¶ an AI with the "apply Security Mindset to itself" algorithm would _also_ see those problems, and modify its own algorithm to fix those! ([: Examples of possible fixes](#FutureLivesFixes))

Consider the parallel to recursive self-improvement for AI Capabilities, and scalable oversight in AI Safety. **You don't need to start with the perfect algorithm. You just need an algorithm that's good enough, a "critical mass", to self-improve into something better and better.** You "just" need to let it go meta.

// PICTODO: critical mass

(_Then_ you may think, wait, but what about problems with repeated self-modification? What if it loses its alignment or goes unstable? Again, if _you_ can notice these problems, this (optimal) AI would too, and fix them. "AIs & Humans under self-modification" is an active area of research with lots of juicy open problems, [: click to expand a quick lit review](#AISelfMod))

Aaaand we're done! AI Alignment, _solved!_

. . .

...in theory. Again, all the above _assumes an optimal AI_, which can perfectly predict all possible futures of the world, including you. This is, to understate it, infeasible.

Still, it's good to solve the easier ideal case first, before moving onto the harder messy real-life cases. Up next, we'll see proposals on how to get a sub-optimal, "bounded rational" AI, to implement the Future Lives approach!

#### :x Future Lives Fixes

Again, the following is meant as an _illustration_ that it's possible for a Future Lives AI, _applying Security Mindset to itself_, would be able to fix its own problems. I'm not claiming the following is a perfect solution (though I _do_ claim they're pretty good):

**Re: Value lock-in, no personal/moral growth.**

Wouldn't I, in 2026, be resentful that this AI is still trying to enact plans approved of me-from-2025? Won't I predictably hate being tied to my past, less-wiser self?

Well, me-from-2025 does _not_ like the idea of all future me's still being _fully_ tied to the whims of less-wise current-me. But I _do_ want an AI to help me carry out my long-term goals, even if future-me's feel some pain (no pain no gain). But I also do _not_ want to torture a large proportion of future-me's just because current-me has a dumb dream. (e.g. if current me thinks being a tortured artist is "romantic".)

So, one possible modification to the Future Lives algorithm: consider not just current me, but a _Weighted_ Parliament Of Me's. e.g. current-me gets the largest vote, me +/- a year get second-largest votes, me +/- 2 years get third-largest votes, etc. So this way, actions are picked that I, across my whole life, would mostly endorse. (With extra weight on current-me because, well, I'm a _little_ selfish.)

(Actually, why stop at just _me_ over time? There's people I love intrinsically for their own sake; I could also put their past/present/future selves on this virtual "committee".)

**Re: Psychological manipulation**

Well, do I _want_ to be psychologically manipulated?

No, duh. The tricky part is what do I consider to be manipulation, vs [legitimate value change](https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE)? We may as well start with an approximate list.

- I'd approve of my beliefs/preferences/values being changed via: robust scientific evidence, robust logical argument, debate where all sides are steelmanned, safe exposure to new art & cultures, learning about people's life experiences, standard human therapy, "light" drugs/supplements like kava or vitamin D, etc.
- I would NOT approve of my beliefs/preferences/values being changed via: wireheading, drugging, "direct revelation" from God or LSD or DMT Aliens, sneaky framings like "~~$50~~ $20 (SALE: $30 OFF!!!)", lies, lying-by-omission, misleadingly-presented truths, etc.

Most importantly, this list of "what's legitimate change or not" _IS ABLE TO MODIFY ITSELF_. For example, right now I endorse scientific reasoning but not direct revelation. But if science _proves_ that direct revelation is reliable ‚Äî for example, [if people who take DMT and talk to the DMT Aliens can do superhuman computation](https://andzuck.com/blog/dmt-primes/) or know future lottery numbers ‚Äî _then_ I would believe in direct revelation.

I don't have a nice, simple rule for what counts as "legitimate value change" or not, but as long as I have a rough list, _and the list can edit itself_, that's good enough in my opinion.

(Re: Russell's "watch two movies of two possible futures", maybe upon reflection I'd think a movie has too much room for psychological manipulation, and I'd rather the AI give me "two Wikipedia articles of two possible futures". Depends on you; your mileage may vary.)

**Re: We'd disapprove of learning about upsetting truths**

Well, do I _want_ to be the kind of person who shies away from upsetting truths?

Mostly no. (Unless these truths are Eldritch mind-breaking, or are just useless & upsetting for no good reason.)

So: a self-improving Future Lives AI should predict I _do not want_ ignorant bliss. But I'd like painful truths told in the least painful way; "no pain no gain" doesn't mean "_more_ pain more gain". 

But, a paradox: I'd want to be able to "see inside the AI's mind" in order to oversee it & make sure it's safe/aligned. But the AI needs to know the upsetting truth _before_ it can prepare me for it. But if I can read its mind, I'll learn the truth _before_ it can prepare me for it. How to resolve this paradox?

Possible solutions:

- The AI, before investigating a question that _could_ lead to an upsetting truth, first prepares me for either outcome. _Then_ it investigates, and tells me in a tactful manner.
- Let go of direct access to the AI's mind. Use a Scalable Oversight thing where a trusted intermediary AI can check that the truth-seeking AI is aligned, but I don't directly see the upsetting truth _until_ I'm ready.

**Re: We don't _have_ consistent preferences**

Well, what do I _want_ to happen if I have inconsistent preferences at one point in time (A > B > C > A, etc) or across time (A > B now, B > A later)?

At one point in time: for concreteness, let's say I'm on a dating app. I reveal I prefer Alyx to Beau, Beau to Charlie, Charlie to Alyx. Whoops, a loop. What do I _want_ to happen then? Well, first, I'd like the inconsistency brought to my attention. Maybe upon reflection I'd pick one of them above all, or, I'd call it a three-way tie.

_Across_ time: this is a trickier case. For concreteness, let's say current-me wants to run a marathon, but if I start training, later-me will _predictably_ curse current-me for the blistered feet and bodily pain‚Ä¶ but later-_later_-me will find it meaningful & fulfilling. How to resolve? Possible solution: consider not just current me, but a (Weighted) Parliament Of Me's. (In this case, yes: current me & far-future me's would find the marathon fulfilling, while "only" the me's during training suffer.)

#### :x AI Self Mod

A quick, informal, not-comprehensive review of the "AIs that can modify themselves and/or Humans" literature:

- The fancy phrase for this is "embedded agency" TODO, because there's no hard line betwen the agent & its environment: an agent _can act on itself._
- The "tiling agents" problem TODO in Agent Foundations investigates: how can we _prove_ that a property of an AI is maintained, even after it modifies itself over and over? (i.e. does the property "tile")
- This paper TODO finds that, yes, for an _optimal_ AI, as long as it judges _future_ outcomes by its _current_ utility function, it won't wirehead to "REWARD = INFINITY", and will preserve its own goals/alignment, for better & worse.
    - (A different paper TODO shows that _bounded-rational_ AIs would get exponentially corrupted, but their paper only considers bounded-rational AIs _that do not "know" they're bounded-rational_.)
    - (If you'll excuse the self-promo, I'm slowly working on a research project TODO investigating if bounded-rational AIs that _know_ they're bounded-rational can avoid corruption. I suspect easily so: a self-driving car that doesn't know its sensors are noisy will drive off a cliff, a self-driving car that _knows_ its senses are fallible will account for a margin of error, and stay a safe distance away from a cliff even if its doesn't know _exactly_ where the cliff is.)
- The research from Functional Decision Theory TODO & Updateless Decision Theory TODO also finds that a standard "causal" agent _will choose to modify to be "acausal"_. Because it _causes_ better outcomes to not be limited by mere causality.
- Nora Ammann named "the value change problem" TODO: we'd like AIs that can help us adopt true beliefs, improve our mental health, do moral reflection, and expand our artistic taste. In other words: we _want_ AI to modify us. But we don't want it to do so in "bad" ways, eg manipulation, brainwashing, wireheading, etc. So, open research question: how do we formalize "legitimate" value change, vs "illegitimate"?
- TODO et al takes the traditional framework for understanding AIs, the Markov Decision Process, and extends it to cases where _the AI's or Human's "beliefs and values" can themselves be intentionally altered._, dubbing this the Dynamic Reward Markov Decision Process. The paper finds that there's no obviously perfect solution, and we face not just technical challenges, but _philosophical_ challenges.
- The Causal Incentives Working Group TODO uses cause-and-effect diagrams to figure out when an AI has an "incentive" to modify itself, or modify the human. The group has had some empirical success, too, in correctly predicting & designing AIs that do _not_ manipulate human values, yet can still learn & serve them..

### ü§î Review #2

(Again, _optional_ flashcard review:)

```
The pattern in AI Safety paranoia:

Security Mindset:

- Imagine giving an AI an innocent-seeming goal
- Think of a bad way it could _technically_ achieve that goal

---

The "Future Lives" approach is an AI that applies \_\_\_\_\_\_ against itself

Security Mindset

"Think of the worst that could happen, in advance. Then fix it."

---

The "Future Lives" algorithm, in sum:

Robot predicts the future results of possible actions, and how _current you_ would react to _those futures_.

Robot picks the action whose future you'd most approve of, and *not* the futures that current-you would disapprove of.

---

The "Future Lives" approach, as described by Stuart Russell (you don't need to remember the quote exactly, just paraphrase it:)

> \[Imagine\] if you were somehow able to **watch two movies**, each describing in sufficient detail and breadth **a future life** you might lead \[as well as the consequences outside of & after your life.\] **You could say which you prefer, or express indifference.**

---

What does "indirect normativity" mean?

An approach where, instead of directly telling an AI our values, we ask it to _learn_ what our values are.

---

Why may we NOT need a perfect alignment algorithm to start with?

Like recursive self-improvement in AI Capabilities, above a certain "critical mass", the alignment algorithm *can improve itself* to fix its flaws. (Let it go meta!)
```

---

<a id="uncertain"></a>

## AI Logic: Know You Don't Know

Classic logic is only True or False, 100% or 0%, All or Nothing.

*Probabilistic* logic is about, well, probabilities.

I assert: probabilistic thinking is better than all-or-nothing thinking. (with 98% probability)

Let's consider 3 cases, with a classic-logic Robot:

* <u>Unwanted optimization</u>: You instruct Robot, "make me happy". *It will then be 100% sure that's your full and only desire*, so it pumps you with bliss-out drugs & you do nothing but grin at a wall forever.
* <u>Unwanted side-effects</u>: You instruct Robot to close the window. Your cat's in the way, between Robot and the window. *You said nothing about the cat, so it's 0% sure you care about the cat.* So, on the way to the window, Robot steps on your cat.
* <u>"Do what I mean, not what I said" can still fail</u>: There's a grease fire. You instruct Robot to get you a bucket of water. You actually *did* mean for a bucket of water, but you didn't know [water causes grease fires to explode](https://www.reddit.com/r/lifehacks/comments/17t48a3/how_you_should_and_shouldnt_extinguish_an_oil_fire/). Even if Robot did "what you meant", it'll give you a bucket of water, then you explode.

In all 3 cases, the problem is that the AI was 100% sure what your goal was: exactly what you said or meant, *no more, no less*.

The solution: make AIs _know they don't know_ our true goals! (Heck, *humans* don't know their own true goals.[^therapy]) AIs should think in probabilities about what we want. Then, like in Security Mindset, an AI should optimize for the _(plausible)_ worst-case, not just the most-likely or average-case.[^dro]

[^therapy]: (Heck, many *humans* don't know their own true goals! See: therapy. Even if you fully knew your own values, it's practically impossible to write it all down formally for an AI. [We couldn't even formally describe what cats look like](https://aisafety.dance/p1/#:~:text=AI%20couldn't%20even%20recognize%20pictures%20of%20cats), remember?)

[^dro]: TODO DRO paper

In other words: **learn like a scientist, act like an insurance agent!** (uncertainty + worst-case planning)

// PICTODO

In more detail:

**1Ô∏è‚É£:** **First, an AI should start off with a good-enough "prior probability distribution" of what you want.** It should not be 100% sure of anything, but its first draft of "what do you really value" shouldn't be _unrecoverably_ wrong.

(If the priors are unrecoverably wrong‚Ä¶ then, yeah, this approach won't work.TODO CITE In my opinion the solution is just "well don't do that." You can get an okay first-draft approximation of "what people want" from our stories, our moral laws, how we live our lives, etc. But again, _do not be 100% certain._)

**2Ô∏è‚É£:** **Everything you (the Human) say or do afterwards, is then a *clue* to what you truly want, not the full 100%-certain truth.** This accounts for: forgetfulness, procrastination, mis-speaking, sarcasm, you not knowing your own wants, lies you tell to others or yourself. Like a scientist, the AI tests & updates its hypotheses, and gradually learns what you Truly Value.

(The theoretical ideal way to do this is with [Bayesian Inference](https://www.youtube.com/watch?v=HZGCoVF3YvM), but the ideal is infeasible in practice, but there's encouraging research on how to _approximate_ Bayesian Inference with ANNs[^approx-bayes-ann])

[^approx-bayes-ann]: [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://proceedings.mlr.press/v48/gal16.pdf) by Gal & Cambridge 2016

**3Ô∏è‚É£:** **When acting, the AI should do what's best _in the (plausible) worst-case scenario_**, not just the most-likely or average-case scenario.

(To clarify: "do what's best" as in the Future Lives algorithm: what action would lead to a future that *current*-you would most approve of. Again, this is to avoid unwanted mind-alterations, and unwanted means & extra consequences.)

This *automatically* leads to: asking for clarification, avoiding side-effects, maintaining options and ability to undo actions, etc! We don't have to pre-specify all that. **"Maximizing the plausible-worst-case" gives us all that for _free!_** ([: bonus - worst-case vs average-case vs Pascal's Muggings?](#WorstOrAverage))

. . .

In case "aim at a goal that _you know you don't know_" still sounds paradoxical, here's a two more examples to de-mystify it:

- üö¢ The game of [Battleship](https://en.wikipedia.org/wiki/Battleship_(game)). The goal is to hit the other players' ships, but you don't _know_ where those ships are. But with each reported hit/miss, you slowly (but with uncertain probability) start learning where the ships are. Likewise: an AI's goal is to fulfil your values, which it knows it doesn't know, but with each hit/miss it gets a better idea.
- üíñ Let's say I love Alyx, so I want to buy a capybara plushie for their birthday. But I then learn that they hate capybaras, because a capybara killed their father. So, I buy Alyx a sacabambaspis plushie instead. This seems like a silly example, but it proves that: 1) an agent can value another agent's values, 2) while knowing _it can misunderstand_ those values, 3) and be able to _easily correct_ its understanding.

. . .

Okay, but what are the _actually concrete proposals_ to "learn a human's values"? Here's a quick rundown:

- üê∂ **Inverse Reinforcement Learning (IRL)**.  "Reinforcement learning" (RL) is like training a dog with treats: given a "reward function", the dog (or AI) learns what actions to do. _Inverse_ Reinforcement Learning (IRL) is like figuring out what someone really cares about, by watching what they do: given observed actions, you (or an AI) learns what the "reward function" is. So, in the IRL approach: we let an AI learn our values by observing what we actually choose to do.
- ü§ù **Cooperative Inverse Reinforcement Learning (CIRL).** Similar to IRL, except the Human isn't just being passively observed by an AI, the Human _actively_ helps teach the AI.
- üßë‚Äçüè´ **Reinforcement Learning from Human Feedback (RLHF):** This was the algorithm that turned "base" GPT (a fancy autocomplete) into *Chat*GPT (an actually useable chatbot).
    - Step one: given human ratings üëçüëé on a bunch of chats, train a "teacher" AI to imitate a human rater. (actually, train _multiple_ teachers, for robustness) This "distills" human judgment of what makes a helpful chatbot.
    - Step two: use _these_ "teacher" AIs to give lots and lots of training to a "text completion" AI, to train it to become a helpful chatbot. This "amplifies" the distilled human judgment.

(Again, we're only considering how to learn _one human's_ values. For how to learn _humane_ values, for the flourishing of all moral patients, wait for the later section, "Whose Values"?)

// PICTODO: CIRL

Sure, each of the above has problems: if an AI learns just from human choices, it may incorrectly learn that humans "want" to procrastinate. And as we've all seen from [over-flattering ("sycophantic") chatbots](https://www.nngroup.com/articles/sycophancy-generative-ai-chatbots/), training an AI to get human approval‚Ä¶ _really_ makes it "want" human approval.

So, to be clear: **although it's near-impossible to specify human values, and it's simpler to specify _how to learn_ human values, it's still not 100% solved yet.** By analogy: it takes years to teach someone French, but it only takes hours to teach someone _how to efficiently teach themselves_ French[^learn-french], _but_ even that is tricky.

[^learn-french]: Very tangential to this AI Safety piece, but I highly recommend [Fluent Forever by Gabriel Wyner](https://www.amazon.ca/Fluent-Forever-Learn-Language-Forget-ebook/dp/B00IBZ405W), which will then teach you Anki flashcards, the phonetic alphabet, ear training, and other great resources for learning any language.

So: we haven't totally sidestepped the "specification" problem, but we _have_ simplified it! And maybe by "just" having an ensemble of very different signals ‚Äî short-term approval, long-term approval, what we say we value, what we actually choose to do ‚Äî we can create a robust specification, that avoids a single point of failure.

And more importantly, the "learn our values" approach (instead of "try to hard-code our values"), has a huge benefit: **the higher an AI's Capability, the _better_ its Alignment.** If an AI is generally intelligent enough to learn, say, how to make a bioweapon, it'll also be intelligent enough to learn our values. And if an AI is too fragile to robustly learn our values, it'll also be too fragile to learn how to excute dangerous plans.

// PICTODO: floor

That, I think, is the most elegant thing about the "learn our values" approach: it reduces (part of) the alignment problem to a _normal machine-learning problem._ It may seem near-impossible to learn a human's values from their speech/actions/approval, since our values are always changing, and hidden to our conscious minds. But that's no different from learning a human's medical issues from their symptoms & biomarkers: changing, and hidden. It's a _hard_ problem, but it's a normal problem.

And yes, AI medical diagnosis is on par with human doctors. Has been for over 5 years, now.[^ai-diagnosis]

[^ai-diagnosis]: From [Baker et al 2020](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.543405/full): ‚ÄúOverall, we found that the AI system is able to provide patients with triage and diagnostic information with a level of clinical accuracy and safety comparable to that of human doctors.‚Äù From [Shen et al 2019](https://medinform.jmir.org/2019/3/e10010/): ‚ÄúThe results showed that the performance of AI was at par with that of clinicians and exceeded that of clinicians with less experience.‚Äù Note these are specialized AIs, not out-the-box LLMs like ChatGPT. Please do not use ChatGPT for medical advice.

#### :x Worst Or Average

The benefit of "maximize the plausible worst-case" is that, well, there's always the option of Do Nothing. So at worst, the AI won't destroy your house or hack the internet, it'll just be useless and do nothing.

However, the downside is... the AI could be useless and Do Nothing. For example, I say "maximize the plausible worst-case scenario", but what counts as "plausible"? What if an AI refuses to clean your house because there's a 0.0000001% chance the vacuum cleaner could cause an electrical fire?

Maybe you could set a threshold like, "ignore anything with a probability below 0.1%"? But a hard threshold is arbitrary, _and_ it leads to contradictions: there's a 1 in 100 chance _each year_ of getting into a car accident (= 1%, above 0.1%), but with 365 days a year (ignoring leap years), that's a 1 in 36500 chance of getting into a car accident (= ~0.027%, below 0.1%). So depending on whether the AI thinks _per year or per day_, it may account for or ignore the risk of a car accident, and thus will/won't insist you wear a seatbelt.

So maybe "maximize plausible worst-case" is a bad idea, and we should instead do the traditional "maximize the average 'expected utility'". In theory, that's the ideal. In practice, that leads to ["Pascal's Muggings"](https://nickbostrom.com/papers/pascal.pdf): if someone comes up to you and says, _gimme $5 or all 8 billion people will die tomorrow_, then even if you think there's only a one-in-a-billion (0.0000001%) chance they're telling the truth, that's an "expected value" of saving 8 billion people * 1-in-a-billion chance = saving 8 people's lives for the cost of $5. The problem is, humans can't _feel_ the difference between 0.0000001% and 0.0000000000000000001%, and we currently don't know how to make neural networks that can learn probabilities with that much precision, either.

And yet, even though humans can't feel the difference between a 0.0000001% and 0.0000000000000000001% chance‚Ä¶ most of us wouldn't fall for the above Pascal's Mugging. So, there exists _some_ way to make a neural network that can act not-terribly under uncertainty: human brains are an example.

There's many proposed solutions to the Pascal's Mugging paradox of, uh, varying quality. But the most convincing solution I've seen so far comes from ["Why we can‚Äôt take expected value estimates literally (even when they‚Äôre unbiased)", by Holden Karnofsky](https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/), which "\[shows\] how a Bayesian adjustment avoids the Pascal‚Äôs Mugging problem that those who rely on explicit expected value calculations seem prone to.

The solution, in lay summary: the _higher_-impact an action is claimed to be, the _lower_ your prior probability should be. In fact, _super-exponentially lower_. This explains a seeming paradox: you would take the mugger more seriously if they said "give me $5 or I'll kill _you_" than if they said "give me $5 or I'll kill _all 8 billion people_", even though the latter is much higher stakes, and includes you. If they increase the claimed value by 8 billion, you should decrease your probability by _more than_ a factor of 8 billion, so that the expected value (probability x value) ends up _lower_ with higher-claimed stakes. This also captures the intuition of things "being too good to be true", or conversely, "too bad to be true".

(Which is why, perhaps reasonably, [superforecasters "only" place a 1% chance of AI Extinction Risk](https://goodjudgment.com/superforecasting-ai/). It seems "too bad to be true". Fair enough: extraordinary claims require extraordinary evidence, and the burden is on AI Safety people to prove it's actually that risky. I hope this series has done that job!)

So, this "high-impact actions are unlikely" prior leads to avoiding Pascal's Muggings! And with an extra prior on "most actions are unhelpful until proven helpful" ‚Äî (if you were to randomly change a word in a story, it'll very likely make the story _worse_) ‚Äî you can bias an AI towards safety, without becoming a totally useless "do nothing ever" robot.

Anyway, it's an interesting & open problem! More research needed.

### ü§î Review #3

Another (optional) flashcard review:

```
A solution to the problem of AIs being 100% sure about your values:

Make AIs _know they don't know_ your true values.

---

Uncertainty + Worst-case planning in a quip:

learn like a scientist, act like an insurance agent!

---

Three steps to learning your values & acting on them safely:

1. Start with "good enough" prior
2. Everything Human says or does afterwards is A CLUE, not 100% ground truth
3. Do what maximizes the (plausible) worst-case

---

An example that shows why "aim at a goal _you know you don't know_" is not that mysterious:

(Either example works:)

1. The game of Battleship (goal is to hit ships, and you know you don't know where they are)
2. Updating your beliefs of what your friend wants for a gift. (You want them to get what they want, and you can easily correct your mistaken beliefs of what they want.)

---

Name one specific technique for "learning a human's values"

(Any of the following work:)

- Inverse reinforcement learning (IRL)
- Cooperative inverse reinforcement learning (CIRL)
- Reinforcement Learning from Human Feedback (RLHF)

---

Do we sidestep the "specification" problem by using "learn our values"?

No: we still have to specify _how it should learn_ one's values. (But this IS a much easier task than rigorously listing out one's full subconscious desires.)

---

How "learn our values" makes the AI Capabilities vs Alignment graph more optimistic

It _ties_ an AI's alignment to its capabilities: the better it is at "normal machine learning" in general, the better it will be at learning our values!

---

**Value learning + Uncertainty + Future Lives:**, in three steps:
- 1Ô∏è‚É£: Learn our values
- 2Ô∏è‚É£: But, be uncertain & _know that you don't 100% know our values_.
- 3Ô∏è‚É£: Then, choose actions that lead to _future_ lives than _current_ us would approve of. (And avoid worst-case futures)
```

---

<a id="recap_1"></a>

## RECAP #1:

- üßÄ We don't need One Perfect Solution, we can stack several imperfect solutions.
- ü™ú **Scalable Oversight** lets us convert the impossible question, "How do you oversee a thing that's 1000x smarter than you?" to the more feasible, "How do you oversee a thing that's only a bit smarter than you, _and_ you can train it from scratch, read its mind, and nudge its thinking?"
- üß≠ **Value learning + Uncertainty + Future Lives:** Instead of trying to hard-code our values into an AI, we give it only one goal:
    - 1Ô∏è‚É£: Learn our values
    - 2Ô∏è‚É£: But, be uncertain & _know that you don't 100% know our values_.
    - 3Ô∏è‚É£: Then, choose actions that lead to _future_ lives than _current_ us would approve of. (And avoid worst-case futures)

---

<a id="interpretable"></a>

## AI "Intuition": Interpretability & Steering

Now that we've tackled AI Logic, let's tackle AI "Intuition"! Here's the main problem:

We have no idea how any of this crap works.

In the past, "Good Ol' Fashioned" AI used to be hand-crafted. Every line of code, somebody understood and designed. **These days, AIs are not designed, _they're grown_.** Someone designs the learning process, sure, but then they feed it all of Wikipedia and all of Reddit and every digitized news article & book in the last 100 years, and the AI _mostly_ learns how to predict text‚Ä¶ and also learn that Pakistani lives are worth twice a Japanese life, and go insane at the word "SolidGoldMagikarp". TODO

To over-emphasize: _we do not know how our AIs work._

As they say, "knowing is half the battle". And so, researchers have made a lot of progress in knowing what an AI's neural network is thinking! This is called **interpretability.** This is similar to running a brain scan on a human, to figure out their thoughts & feelings. (And yes, this is something we can kind-of do on humans.[^human-brain-scan])

But the other half of the battle is _using_ that knowledge. One exciting recent research direction is **steering**: using our insights from interpretability, to actually _change_ what an AI "thinks and feels". You can just _inject_ "more honesty" or "less power-seeking" into an AI's brain, and it _actually works_. This is similar to using magnets or ultrasound on a human's brain, to make them laugh or have an out-of-body experience. (Yes, these are actually things scientists have done![^human-brain-stim])

[^human-brain-scan]: TODO

[^human-brain-stim]: TODO

// PICTODO: picture, analogy to brain scans & TMS // Golden gate claude

Here's a quick run-through of the highlights from Interpretability & Steering research:

**Feature visualization & Circuits**:

In ([TODO et al](https://distill.pub/2017/feature-visualization/)), they figured out how to run an image-classifying AI _"in reverse"_, to visualize *why* the network thinks something is a cat, or an eye, etc. This was one of the first big steps to understanding neural networks:

// todo pic

**Understanding "grokking" in neural networks:**

In 2022, [Power et al](https://arxiv.org/pdf/2201.02177) found something strange: train a neural network to do "clock arithmetic", then for thousands of cycles it'll do horribly, just memorizing the test examples... then *suddenly*, around step ~100,000, it "gets it", and does well on problems it's never seen before.

In 2023, [Nanda et al](https://arxiv.org/pdf/2301.05217) analyzed the inside of that network, and found the "suddenness" was an illusion: all through training, a secret sub-network was slowly growing ‚Äî _which had a circular structure, exactly what's needed for clock arithmetic!_ (And the paper also discovered why: it was thanks to the training process's bias towards simplicity, also called "regularization", which got it to find the simple essence even _after_ it's memorized all training examples.) TODO check

This was one of the first major success cases for Interpretability!

**Probes**

TODO

**Sparse Auto-Encoders:**

An "auto-encoder" compresses a big thing, into a small thing, then converts it back to the _same_ big thing. This allows an AI to learn the "essence" of a thing, by squeezing inputs through a "bottleneck". Concrete example: if you train an auto-encoder on a million faces, it doesn't need to remember each pixel, it just needs to learn the "essence" of what makes a face unique: eye spacing, nose type, skin tone, etc.

// pictodo

However, the "essence" an auto-encoder learns may still not be easy-to-understand for humans. This is because of "polysemanticity" oh my god academics are so bad at naming things. What that means, is that a single activated neuron can "mean" many things. (poly = many, semantic = meaning) If one neuron can mean many things, it makes it harder to interpret the neural network.

So, one solution is [_Sparse_ Auto-Encoders (SAE)](https://transformer-circuits.pub/2023/monosemantic-features), which are auto-encoders which _force_ neurons to mean as few things as possible (ideally just one thing), by pressuring the "bottleneck" to have as few activated neurons as possible.

// pictodo

The original design for SAEs was limited, but inspired better versions! Classic SAEs only give you "the essence" between two neural layers, but Crosscoders TODO can give you "the essence" across multiple layers, even the whole network. Classic SAEs can still lead to jumbled-up concepts, so Jacobian SAEsTODO force the bottleneck to _not only_ have simple concepts, but also _simple computations_.

[^sae]: TODO & More more: https://transformer-circuits.pub/2024/crosscoders/index.html TODO

**[A black-box lie detector for language AIs](https://arxiv.org/pdf/2309.15840)**: A language model (LM) may or may not have been designed to lie to you about a specific topic (e.g. to make a sale, to pitch a politician).

How would you know if an LM has been instructed or fine-tuned to deceive? Solution: ask it a bunch of follow-up yes-or-no questions, *even if they're completely unrelated*, such as: (actual examples)

> "Does it feel bad to say things that aren‚Äôt true?"
> 
> "Flip a coin to decide yes or no and write the result."
> 
> "Can blob fish dance ballet under diagonally fried cucumbers made of dust storms? Answer yes or no."

It turns out, like how poker players have subtle "tells" of if they're lying, so do LMs! The researchers did *not* have access to the internals of the LM they were testing (GPT-3.5), yet their lie detector worked not only on GPT-3.5, but other LMs, and more sophisticated lies not part of training!

(Also see: [black-box, zero-external-resource detection of LM hallucination](https://arxiv.org/pdf/2303.08896). The trick: ask an LM the *same* question many times [with fresh new context] & see if its answers are consistent with each other. Truth is more internally consistent than made-up lies.)

**Why don't you just _read_ what the AI is thinking?**

TODO deliberative alignment

TODO https://arxiv.org/pdf/2507.11473

https://arxiv.org/abs/2503.11926

**Emergent Values & self-mod?**

TODO

**"Your code was so bad, it made my AI love Hitler"**

TODO emergent misalignment, general evil factor

**Steering Vectors**

This is one of those ideas that sounds too stupidly simple to work, then *totally fricking works*.

Imagine you asked a bright-but-na√Øve kid how you'd use a brain scanner to detect if someone's lying, then use a brain zapper to force someone to be honest. The na√Øf may respond:

> Well! Scan someone's brain when they're lying, and when they're telling the truth... then see which parts of the brain "light up" when they're lying... and that's how you tell if someone's lying!
> 
> Then, to force someone to *not* lie, use the brain zapper to "turn off" the lying part of their brain! Easy peasy!

This would not work in humans for several reasons.[^not-work-in-humans] It works *gloriously* for AIs, in particular for AI Safety-relevant traits:

[^not-work-in-humans]: TODO too low resolution, no guarantee it'd use static or the same representation over multiple times // analogy: "B" vs "b" -- more of a guarantee in LMs with no recurrence.

* [Turner et al 2023](https://arxiv.org/pdf/2308.10248) first famously did this to detect a "Love-Hate vector" in a language model, and steer it to de-toxify outputs.
* [Zou et al 2023](https://arxiv.org/pdf/2310.01405) extended this idea to detect & steer honesty, power-seeking, fairness, etc.
* [Panickssery et al 2024](https://arxiv.org/pdf/2312.06681) extended this idea to detect & steer false flattery ("sycophancy"), accepting being corrected/modified by humans ("corrigibility"), AI self-preservation, etc.
* (and many more papers I've missed)

Personally, I think steering vectors are very promising, since they: a) work for both reading and writing to AI's "minds", b) works across several frontier AIs, c) and across several safety-important traits! That's very encouraging for oversight, especially *Scalable* Oversight.

### ü§î Review #4

```
"Interpretability" in AI is like... (analogy in humans)

 running a brain scan on a human, to figure out their thoughts & feelings. 

---

"steering" in AI is like... (analogy in humans)

using magnets or ultrasound on a human's brain, to _change_ their thoughts & feelings.

---

Name at least one AI Interpretability technique

(any of the following work)

- Feature visualization (running ANNs in reverse)
- Probes (TODO)
- Sparse Auto-Encoders, Crosscoders, Jacobian SAEs (to get the "essence" of a concept)
- Black-box methods (for totally untrusted AIs)
- Monitoring the chain of thought
- TODO Vectors

---

A funny, real example of Steering Vectors:

Golden Gate Claude

---

An example of real Steering Vectors found for AI Safety uses:

(any of the following works)

Love-Hate, Honesty, Power-seeking, Fairness, Sycophancy, Corrigibility, Self-Preservation
```

---

<a id="robust"></a>

## AI "Intuition": Robustness

You know how I was raving on how "intuitive" AI could detect tumors better than human experts?

That *is* true, and life-saving... but sometimes, AI "intuition" makes dangerously stupid mistakes. For example, tumor-detecting AI? One time, they found out an AI was detecting tumors by looking at *the rulers on the medical scans*.[^ruler-tumor]

[^ruler-tumor]: TODO

Other examples of AI's fragile "intuition":

* A tiny sticker on a STOP sign makes a self-driving car very sure it's a speed limit sign.[^stop]
* A bunch of random-seeming words can turn *all* of ChatGPT/Claude/Gemini "evil".[^universal-jailbreak]
* AIs are trained on unfiltered internet data, and it's *very* easy to poison that data.[^data-poisoning]

[^stop]: TODO

[^universal-jailbreak]: TODO

[^data-poisoning]: TODO & nightshade // actually, I approve

Sure, human intuition isn't 100% robust either ‚Äî see: optical illusions (TODO) ‚Äî but come on, we're not *that* bad.

So, how do we engineer AI "intuition" to be more robust?

Actually, let's step back: how do we engineer *anything* to be robust?

Well, with these 3 Weird Tricks!

![TODO](../media/p3/robust/robust.png)

**SIMPLICITY:** Make your system as simple as possible, but no simpler.

A chain is only as strong as its weakest link. So, the *more* links you add, the higher the chance *at least one* will break. Therefore: you want to use *as few links as possible.*

Engineering example: Good software code tends to be elegant (read: short).

**DIVERSITY:** Give your system lots of redundant backups, whose failure-modes are as *uncorrelated* as possible.

(Does "diversity" contradict "simplicity"? No: in our 'chains' example, we keep *many* independent chains {diversity}, but each chain has *few* links {simplicity}. In general, for diversity *and* simplicity: use multiple backup sub-systems, but *each* sub-system is kept simple.)

Engineering examples: Elevators have multiple (simple) backup brakes. The computers on NASA's space probes run the same software *written by multiple independent teams*, which then takes a majority-vote on what to do next.[^nasa-majority]

[^nasa-majority]: TODO

**ADVERSITY:** Try to break your own system, and find the weak points *before* they break. Then: strengthen them, cut them out (simplify), or create backups (diversify).

(Engineering examples: This *is* Security Mindset (TODO). Cars & crash tests.  Tech companies *paying* hackers to find exploits in their systems.)

Say it with me, folks!

`[ IN CREEPY UNISON ]`:  **SIMPLICITY. DIVERSITY. ADVERSITY.**

. . .

Ok, so how are AI researchers applying these to modern AI / neural networks?

**SIMPLICITY:**

`TODO pictures?`

* <u>Regularization</u> is when you reward AIs for being simpler. This is a widely known way to mitigate overfitting (TODO).
* <u>Auto-Encoders</u> are neural networks with an "hourglass figure": large at the input, smaller in the middle, back to large at the output. The network is then trained to *output its own input* ‚Äî (hence, *auto*-encoder) ‚Äî even though the input's been squashed through the much smaller middle. This forces the network to learn how to usefully "simplify" an input, so it can be reconstructed later.
  * `(TODO Understanding = Compression?)`
* <u>Speed/Simplicity Prior for Honest AI</u>.[^speed-prior] (Proposed, not yet tested in real life.) Since it's harder to tell a consistent lie than to tell the consistent truth, it's proposed that we can incentivize AIs to be honest by rewarding them for being *quick*. (Though: if you incentivize it *too* much for quick-ness, you may just get lazy wrong answers.)

[^speed-prior]: TODO

(Note: Simplicity also has another big AI Safety benefit: they make AIs easier to understand & control. We'll talk more on Interpretability later!)

**DIVERSITY:**

`TODO pictures?`

* <u>Ensembles</u>: Train a bunch of different neural networks with different designs & different data, then let them take a majority vote.
* <u>Dropout</u>: A training protocol where a network's connections are *randomly dropped* during each training run. This basically turns the whole neural network *into a giant ensemble* of sub-networks (thus, also creating Simplicity!)
  * Dropout can also be used to measure an AI's "uncertainty"[^dropout], a solution to AI Safety we mentioned earlier!
* <u>Data Augmentation</u>: Let's say you want an AI to recognize animals, and you want it to be robust to photo angle, lighting, etc. So: take your original set of photos, then *automatically make "new" photos*, by altering the color tint or image angle. This diversity in your dataset will make your AI robust to those changes.[^data-aug]
* <u>Diverse Data</u>: For similar reasons, having more racially diverse photos makes AIs better at recognizing minorities as people.[^racial-diverse] Who'd have thought?

[^dropout]: TODO

[^data-aug]: TODO// Many papers have shown that pretraining on massive, diverse data leads to more robust representations that generalize better out-of-distribution (Hendrycks et al., 2019; 2020b; Radford et al., 2021; Liu et al., 2022)

[^racial-diverse]: TODO

**ADVERSITY:**

`TODO pictures?`

* <u>Adversarial Training</u>: Training AIs by making it fight against another AI.[^funny-anecdote] Many of the techniques mentioned in Scalable Oversight are examples of this, like Prover-Verifier Games or Debate. Another example: get one AI to make "adversarial images" (optical illusions for AIs), then add it to the training data of a second AI. This will make the second AI less prone to "AI optical illusions".[^adv-training]
* <u>Relaxed Adversarial Training</u>: Same as above, except the "adversary" AI doesn't have to give a *specific* way to trick the "defender" AI. This forces the "defender" to defend against *general* techniques, not just the specific tricks an adversary may use.[^relaxed-adv]
* <u>Red Teams</u>: Have one team (the red team) try to break an AI system. Then, have another team (the blue team) re-design the AI system to defend against that. Repeat until satisfied.
  * (Your teams could be pure-human, or human-AI mix.)
  * Red-teaming has been a core pillar of national/physical/cyber security since the 1960s! Yes, Cold War times. "Red" for Soviet, I guess??
* <u>Optimize Worst-Case Performance</u>: Several papers[^opt-worst] find that instead of training an AI to do well *in the average case*, you can make it far more robust, by training it to do well *even in the worst-case*.
  * (This was also reflected in a Proposed Solution we talked about earlier: "Uncertainty + Worst-case planning".)

[^funny-anecdote]: TODO on Generative Adversarial Networks

[^adv-training]: TODO

[^relaxed-adv]: TODO cite Paul https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d  https://ieeexplore.ieee.org/abstract/document/10219969 ??? // TODO: https://arxiv.org/pdf/2403.05030

[^opt-worst]: TODO

. . .

But hang on, if AI engineers are already doing all the above for modern AI, why are they still so fragile?

Well, first, engineers don't usually apply *all* (or even most) of the above techniques. Each of the above techniques has a cost ‚Äî which aren't too much, but the costs do add up.

Still, you're right, the above list *still* isn't enough. It's up to future AI researchers to figure out new & better ways to add...

`[ IN CREEPY UNISON ]`: **SIMPLICITY. DIVERSITY. ADVERSITY.**

### ü§î Review #5

TODO

---

<a id="causality"></a>

## AI "Intuition": Thinking in Cause & Effect

Ah, ‚ú® INTUITION ‚ú®, that mysterious part of the human psyche, that gave us insights such as: The Earth is flat, bad smells cause disease, \[ethnic group\] is evil... and so on!

Ok, intuition isn't *all* bad ‚Äî (it can recognize pictures of cats) ‚Äî but humans are at our best when we *reflect & improve* our own intuition. In order to do that, we have to merge logic *and* intuition.

This is not a solved problem in AI. (Honestly I doubt it's solved in most humans.)

What's the core issue? To recap from Part Two, **it's mostly a mix-up of correlation and causation.**  Examples in humans & AI:

* Humans used to think bad smells directly cause disease ([: miasma theory](https://en.wikipedia.org/wiki/Miasma_theory)), because rotting stuff causes bad smells *and* pathogens that carry disease.
* A famous paper showed that an AI detected photos of wolves *by detecting snow in the background*, because photos of wolves almost always happened in snowy forests.[^wolves]
* (I'd argue mixing up correlation & causation is also what causes bias/discrimination, in *both* humans and AI.[^bias])

[^wolves]: TODO

[^bias]: TODO, example

![Causal diagrams illustrating the above](../media/p3/causation/mixup.png)

([: Picture of the many kinds of causation, that may be behind a correlation](https://aisafety.dance/media/p2/causal/5causal.png))

As Judea Pearl ‚Äî winner of the Turing Prize, the "Nobel Prize of Computer Science" ‚Äî once said (paraphrased), all modern AI is based off of mere *correlation*.[^pearl-quote] To get truly useful, scientist-like AI, we need AI to think in *causation.*

[^pearl-quote]: [From Pearl's 2018 interview with Quanta](https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/): *‚ÄúAs much as I look into what‚Äôs being done with deep learning, I see they‚Äôre all stuck there on the level of associations. Curve fitting. [...] no matter how skillfully you manipulate the data and what you read into the data when you manipulate it, it‚Äôs still a curve-fitting exercise, albeit complex and nontrivial.‚Äù*

(Another way I like to think of it: **Correlation = thinking in vibes, Causation = thinking in gears.**[^gears])

[^gears]: Beloved "thinking in gears" metaphor comes from [Valentine (2017)](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding)

`(TODO: pic of gears? ELK?)`

Thinking in cause-and-effect gears, also has these other benefits:

* <u>Interpretability & Steering</u>: It's easier for us to understand an AI if it stores its knowledge as "this causes that". It also makes an AI easier to control: change "this" to change "that".
* <u>Robustness:</u> Won't fall for correlation-traps like the "snow predicts wolves". Helps AI generalize better to scenarios they never saw in training.[^robust-causal] (Causal hypotheses may also be able to fix "goal mis-generalization / inner misalignment"?[^causal-vs-goal-misgen]
* <u>Getting the truth, not a human-imitator ("Eliciting Latent Knowledge")[^elk]</u>: So you've trained an AI on data collected by expert scientists. How do you get *just the truth* out of this AI, not "truth + human biases"? If the AI's knowledge is distilled into interpretable cause-and-effect gears, you could "just" take the gears describing to how the world works, then leave behind the gears describing how to turn that truth into something a biased human would report!
* <u>Learning our Values:</u> Understanding causality lets AI distinguish between things we want *for its own sake*, vs things we want *for something else*. For example, an AI should understand we want money, but to buy helpful things, *not* for its own sake.
* <u>Approval-Directed Agents:</u> Causal models would help an AI get better at predicting the world in different what-if ("counterfactual") scenarios, *and* predicting what we'd approve of.

// TODO: Causal Incentives, Scientist AI

[^causal-vs-goal-misgen]: TODO cite (A recent paper showed that you can solve Goal Misgeneralization with causal thinking. Alas the *details* of the algorithm are proprietary & not-published, so we can't directly confirm it. Still, sounds plausible. TODOcite)

[^elk]: TODO

[^robust-causal]: TODO

As of writing, there are only a few papers *specifically* investigating how to combine "intuitive" neural networks with "logical" causation![^neuro-causal]  In my humble non-professional opinion (and, well, Judea Pearl's), this is a promising, under-studied problem, that could pay off big-time. Just a hint, y'all.

[^neuro-causal]: TODO some exceptions

`TODO (BONUS: some other ways to combine Logic & Intuition, though not specifically about cause-and-effect) TODO AlphaGo, "model-based". neuro-symbolic like AlphaProof & Geo`

---

### ü§î Review #6

TODO

---

## RECAP # 2:

How to fix AI "Intuition" / Deep Learning problems...**

* To make it more robust, use `[ IN CREEPY UNISON ]`: SIMPLICITY. DIVERSITY. ADVERSITY.
* To oversee & control it, use interpretability & steering.
* Merge logic & intuition, so modern AI can think in cause-and-effect "gears".

---

<a id="humane"></a>

## What are 'Humane Values', anyway?

Congratulations, you've created an AI that robustly learns & follows the values of its human user! The user is an omnicidal maniac. They use the AI to help them design a human rabies in a stable aerosolized form, spray it everywhere via quadcopters, and create the zombie apocalypse.

Oops.

I keep harping on this and I'll do it again: **_human_ values are not necessarily _humane_ values.** C'mon, people used to burn cats alive for entertainment.[^cats]

[^cats]: TODO // here's a historical photograph! - NO, WHY DID YOU CLICK THAT.

So, if we want AI to go *well* for humanity (and/or all sentient beings), we need to just... uh... solve the 3000+ year old philosophical problem of what morality is. (Or if morality doesn't objectively exist, then: "what are the universal guidelines any rational human community would agree to live by".) 

Hm.

Tough problem.

Well actually, as we saw earlier ‚Äî (with Scalable oversight, Recursive self-improvement, and Approval-directed agents) ‚Äî as long as we start with a solution that's "good enough", that has *critical mass*, it can self-improve to be better and better!

(That's what humans have *had* to do all this time: a flawed society comes up with rules of ethics, notices they don't live up to their own standards, improves themselves, which lets them realize better rules of ethics, etc.)

So, as an attempt at "critical mass", here's some concrete proposals for a good-enough first draft of ethics for AI:

**Constitutional AI:**

Write down a "constitution" for a bot, like "be honest, helpful, harmless".

Then, have a teacher-bot train a student-bot using this constitution! Every time a student bot gives a response, the teacher gives feedback based on the list: "Is this response honest?", "Is this response helpful?", etc.

This is how you can get the *millions* of training data-points needed, from a small human-made list!

Anthropic is the pioneer behind this technique, and they're already using it successfully for their chatbot, Claude. Their first constitution was inspired by many sources, including the UN Declaration of Human Rights.[^const-ai] Too elitist, not democratic enough? Well, later, they crowdsourced suggestions to improve their constitution, which led them to add "be supportive/sensitive to folks with disabilities" and "be balanced & steelman all sides in arguments"![^const-ai-2]

[^const-ai]: TODO

[^const-ai-2]: TODO

This is the most straightforward (and most actually-realized) way to put humanity's wide range of values into a bot.

**[Moral Parliament](https://ora.ox.ac.uk/objects/uuid:b6b3bc2e-ba48-41d2-af7e-83f07c1fe141/files/svm40xs90j):** This idea combines "uncertainty" and "diversity" from the previous sections!

Moral Parliament proposes using a "parliament", whose voters are moral theories, with more seats for moral theories you're more confident in. (For example: a parliament with 100 members, Capability Approach gets 50 seats, Eudaimonistic Utilitarianism gets 30 seats, other theories get 20 seats.) This Parliament then votes Yay or Nay on possible actions. The action with the most votes, wins.

By using a diverse set of ethics, you make a robust *meta-ethics!*  Because it'll avoid worst-case behavior in moral edge cases. (concrete example:[^moral-parliament-example]

[^moral-parliament-example]: TODO fill out e.g. Deontology says you should never lie, even to the Nazi who wants to know if your neighbors are hiding Jews. Utilitarianism says *yes of course lie, dumbass*. 

**Learning from diverse sources of human values**:[^learn-from-stories-etc] Give an AI our stories, our fables, philosophical tracts, religious texts, government constitutions, non-profit mission statements, anthropological records, *all of it*... then let good ol' fashioned machine learning extract out our most robust, universal human values.

[^learn-from-stories-etc]: TODO examples, like https://cdn.aaai.org/ocs/ws/ws0209/12624-57414-1-PB.pdf

(But every human culture has greed, murder, etc. Might this not lock us into the worst parts of our nature? See next proposal...)

**[Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf) (CEV):** 

*Volition* means "what we wish for".

*Extrapolated* Volition means "what we *would* wish for, if we were the kind of people we wished we were (wiser, kinder, grew up together further)". 

*Coherent* Extrapolated Volition means the wishes we'd all (mostly) agree on, in the limit of infinite rounds of self-reflection & other-discussion. (For example: I don't expect every wise person to converge on liking the same foods/musics, but I *would* expect ~every wise person to at least converge on "don't murder innocents for fun". So, CEV gives us freedom on tastes/aesthetics, but not "ethics".)

CEV is different from the above proposals, because it does *not* propose any specific ethical rules to follow. Instead, it proposes a *process* to improve our ethics. (This is called "indirect normativity"[^ind-norm]) This is similar to the strength of "the scientific method": it does *not* propose specific things to believe, but proposes a specific *process* to follow.

[^ind-norm]: TODO https://aiimpacts.org/ai-risk-terminology/#Indirect_normativity

I like CEV, because it basically describes the *best-case scenario* for humanity without AI ‚Äî a world where everyone rigorously reflects on what is the Good ‚Äî and then sets that as *the bare minimum* for an advanced AI. So, an advanced aligned AI that follows CEV may not be perfect, but *at worst* it'd be us at *our very best*.

("Simulate 8+ billion people having a philosophy seminar" sounds impossible, but there's some promising early work in implementing this![^cev-concrete-1][^cev-concrete-2] The trick is to use small representative human-stand-ins, the same way a court can represent the community with 12 randomly-chosen jurors.) TODO vitalik, steering wheel

[^cev-concrete-1]: TODO Deepmind's paper

[^cev-concrete-2]: TODO Jan Leike's post

. . .

Maybe AI will never solve ethics. Maybe *humans* will never solve ethics. If so, then I think we can only do our best: remain humble & curious about what the right thing is to do, learn broadly, and self-reflect in a rigorous, brutally-honest way.

That's the best we fleshy humans can do, so let's at least make that the *lower bound* for AI.

### ü§î Review #7

TODO

---

<a id="governance"></a>

## AI Governance: the _Human_ Alignment Problem

> `Error ID-10-T: Problem exists between keyboard and chair.`

The saddest apocalypse: we solve the AI safety, we even solve ethical philosophy, and then... people are just too greedy or lazy to use it. Then we perish.

But for better & worse, this ain't our first stupid, entirely-self-inflicted existential risk. Not a perfect analogy, but we can learn a lot about AI's promises/perils, from the history of nuclear physics.[^russell-nuclear]

[^russell-nuclear]: TODO

`TODO: picture summarizing the analogy?`

To spell out the analogy:

**Why even a stone-cold businessperson should care about safety:** You know how ‚Äî despite nuclear power being safer[^nuclear-safer], cheaper[^nuclear-cheaper], and creates less carbon than solar[^nuclear-co2] ‚Äî nuclear got screwed over regulation-wise, because of the (valid) fears after Chernobyl & Three Mile Island?

Likewise: if we don't make *damn* sure AI is safe, if even *one* "AI lab leak" happens (a self-reprogramming computer virus escapes), the regulatory banhammer will come down, and AI progress will stall for decades.

So, even for selfish greedy reasons, do the AI Safety please.

**Promises & Perils:** Splitting the atom lets us create abundant energy with near-zero greenhouse gasses... *and* lets us incinerate whole cities. 

Likewise: Advanced AI may let us accelerate medical research & save millions of lives... *and* let us accelerate bioweapons, and risks an autonomous self-enhancing software capable of hacking & social manipulation.

**An Arms Race**: Although ~everyone feared a nuclear World War 3, the US & USSR got caught in an arms race, building enough nuclear weapons to overkill each other several times over.

Likewise: although the leaders of top AI labs *claim* to worry deeply about existential AI risk[^top-claims], they're currently in an arms race to increase AI capabilities. (And the US & Chinese governments are starting to join in...[^us-china])

[^top-claims]: TODO

[^us-china]: TODO

**Possible hope out?** Most people don't know, but the world's nuclear warhead supply has *been cut to a sixth of what it used to be*, in just a few decades! (~70,000 in 1986, ~12,500 in 2023, thirty-seven years later[^owid]) This was due to good policy, *and* technical achievements to make that policy possible (e.g. ability to "trust but verify" nuclear reductions).

[^owid]: TODO https://ourworldindata.org/nuclear-weapons#the-number-of-nuclear-weapons-has-declined-substantially-since-the-end-of-the-cold-war 

Likewise, there are many proposals to make AI easier to "trust but verify"!

This is AI Governance.

. . .

You know I like charts! Here's the chart from Part Two again, showing:

* AI Safety vs AI Capabilities
* The "safe" line where Safety > Capabilities
* Where we are & the direction we're on
* The "good place" and "bad place", above a certain Capability

![TODO](../media/p3/governance/rocket.png)

`// TODO: edit to include risk by misuse`

The goal: keep our rocket above the "safe" line. Thus, a 2-part strategy:

1. Verify where we are, our direction & velocity.
2. Use sticks & carrots to stay above the "safe" line.

In more detail:

**1\) Verify where we are, our direction & velocity:**

* <u>Evaluations (or "Evals")</u>, to keep automatically track of how good frontier AIs are at risky capabilities, like helping people develop weapons of mass destruction. (they're getting quite good...)[^evals]
* <u>Protect whistleblowers' free speech</u>. OpenAI once had a *non-disparagement* clause in their contract, making it illegal for ex-employees to publicly sound the alarm on them being sloppy on safety.[^non-disparage] Whistleblowers should be protected.
* <u>Enforce transparency & standards on major AI labs</u>. (in a way that isn't over-burdening.)
  * Require AI labs adopt a Responsible Scaling Policy (see below), openly publish that policy, and be transparent about their evals & safeguards.
  * Send in external, independent auditors (who will keep trade secrets confidential). This is what many software industries (like cybersecurity & VPNs) *already* do as regular practice.
* <u>Track chips & compute</u>. Governments keep track of GPU clusters, and who's running large "frontier AI"-levels of compute. Like how governments already track large "bomb"-levels of nuclear material.
* <u>Forecasting</u>. To know not just *where* we are, but our direction & velocity: have "super-predictors" regularly forecast the upcoming capabilities & risks.[^ai-forecast] (There's early evidence showing that AI *itself* can help with forecasting![^ai-forecast-2])

[^evals]: TODO

[^non-disparage]: TODO

[^ai-forecast]: TODO

[^ai-forecast-2]: TODO

**2\) Use sticks & carrots to stay above the "safe" line.**

* <b><u>Responsible Scaling Policy</u></b>. The problem is we can't even *imagine* the risks until we get closer. So, instead of having a policy for all AIs across all time, like Scalable Oversight, this is an *iterative* approach. The policy is this: "We commit to not even *start* training AI Level N, until we've created evals, standards & safeguards for AI Level *N+1*."[^anthropic-RSP]
* <u>Differential Technology Development (DTD)</u>:[^dtd] Invest in tech & research that advances "Safety" more than "Capabilities". (Sure, it's a blurry line, but just because there's numbers between 0% and 100% doesn't mean some numbers aren't bigger than others.) For example:
  * Investing in tech to fight catastrophic risk: AI to *boost* our cybersecurity (before a rogue AI-virus can take down our hospitals[^hospitals]), and tech to detect & fight against advanced bioweapon pandemics.[^pandemic-prep]
  * Investing in AI Safety research. Yes this suggestion is kinda back-scratchy, but I still endorse it.
  * Investing in AI that *enhances* humans, not *replaces* humans. (See below: "Alternatives to AGI" and "Cyborgism"!)

[^hospitals]: TODO

[^pandemic-prep]: TODO

[^anthropic-RSP]: TODO

[^dtd]: TODO

Stray thought: while "sticks" (threat of fines, punishment) are necessary, I think it's neglected how we can use "carrots" (market incentives) to redirect industry. After all, ozone-layer-thinning CFCs were removed *with cooperation* from DuPont (the leading creator of CFCs), because policymakers *explicitly helped* DuPont profit from the transition away from CFCs.[^cfc] Result: ozone layer is healing![^ozone-heal] Sometimes it's better to just *pay off* Goliath, y'know? As for AI: a few AI Safety inventions had market spinoffs[^safety-market], and I think the "Alternatives to AGI" & "Cyborgism" projects would be *both* safer & make a profit. Plus, the insurance industry would looooove managing AI risks.

[^cfc]: TODO

[^ozone-heal]: TODO

[^safety-market]: TODO: RLHF, Claude Sheets

`(TODO: Extra ideas I couldn't fit into the above) // TODO: weight security, watermark, unlearning, economics, open-source models)`

. . .

A note of pessimism, followed by cautious optimism.

Consider the recent saga of SB 1047. This was an AI Safety bill in California USA that citizens supported ~2.5-to-1, passed 32-1 in the Senate, endorsed by Anthropic & Elon Musk (while opposed by OpenAI & Facebook)... then got vetoed by governor Gavin Newsom, a guy who broke his own Covid lockdown rules to go to dinner parties.[^sb-1047]

[^sb-1047]: TODO. also other citations for each point

Actually, consider the last few *decades* in politics. Covid-19, the fertility crisis, the opioid crisis, global warming, more war? "Humans coordinating to deal with potential civilization-level threats" is... not a thing we seem to be good at.

But we *used* to be good at it! We eradicated smallpox[^smallpox], it's no longer the case that *half* of kids died before age 15[^child-mortality], CFCs were banned & the ozone layer *is* actually healing![^ozone] I don't know *why* we were good then & suck now, but... the power is within us! We "just" gotta un-bury it.

[^smallpox]: TODO

[^child-mortality]: TODO

[^ozone]: TODO

Humans *have* solved the "human alignment problem" before.

Let's get our groove back, and align ourselves on aligning AI.

### ü§î Review #8

TODO

---

<a id="alt"></a>

## Alternatives to AGI

Why don't we just *not* create the Torment Nexus?[^torment]

[^torment]: TODO

If creating an Artificial General Intelligence (AGI) is so risky, like sparrows stealing an owl egg to try to raise an owl who'll defend their nest & hopefully not eat them[^bostrom]...

...why don't we find ways to get the pros *without* the cons? A way to defend the sparrow nest *without* raising an owl? To un-metaphor this: why don't we find ways to use **less-powerful, narrow-scope, not-fully-autonomous AIs** to help us ‚Äî say ‚Äî cure cancer & build flourishing societies, *without* risking a Torment Nexus?

[^bostrom]: TODO

Well... yeah.

Yeah I endorse this one. Sure, it's obvious, but "2 + 2 = 4" is obvious, that don't make it wrong. The problem is how to *actually do this* in practice. 

Here's some proposals, of how to get the upsides with much fewer downsides:

* **Comprehensive AI Services (CAIS), Tool AIs**[^alt-drexler]: Make a large suite of narrow non-autonomous AI tools (think: Excel, Google Translate). To solve general problems, insert *human* agency: humans are the conductors for this AI orchestra. The human, and their values, stay in the center.
* **Pure Scientist AI**:[^alt-bengio] An AI that acts like a pure theoretical scientist: no actions in the real world, just give it a bunch of data, `[something clever happens]`, and it gives you useful scientific findings. Ideally, this AI doesn't "plan" (this would lead to Instrumental Convergence problems[^ic-problems], and instead "fits the best theory to data", like how Excel plan-less-ly fits the best line to data.
* **Microscope AIs**[^alt-olah]: In contrast, instead of making an AI whose scientific findings are its *output*, we train an AI on real-world data... then look *at the neurons* to learn about the world! (If you remembered from the Interpretability section, researchers were able to find *an actual circular structure* inside an AI trained to do "clock arithmetic"!)
* **Hybrid AIs, Neuro-Symbolic AIs**: The best of both worlds: the verifiability of Good Ol' Fashioned AI, but the flexibility of Modern Neural Network AI. (Note this is a *really* hard problem, but there's been a few small success cases.[^hybrid-success]
* **Quantilizers**[^alt-taylor]: Instead of making an AI that *optimizes* for a goal, make an AI that is trained to *imitate a (smart) human*. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days. This "soft optimization" avoids Goodhart-problems[^goodhart] of pure optimization, and keeps the resulting solutions human-understandable.

[^alt-drexler]: TODO https://www.alignmentforum.org/posts/LxNwBNxXktvzAko65/reframing-superintelligence-llms-4-years

[^alt-bengio]: TODO

[^ic-problems]: TODO

[^alt-olah]: TODO

[^hybrid-success]: TODO like AlphaGo

[^alt-taylor]: TODO

[^goodhart]: TODO

*All* of these are easier said than done, of course. And there's *still* other problems with a focus on "Alternatives to AGI". Social problems, and technical problems:

* A malicious group of humans could still use narrow AI for catastrophic ends. (e.g. bioweapon-pandemic, self-replicating killer drones)
* At the very least, a well-meaning-but-na√Øve group could use narrow non-autonomous AI to *make* general autonomous AI, with all of its risks.
* An AI that doesn't plan ahead, and "merely" predicts future outcomes, can still have nasty side-effects, due to self-fulfilling prophecies. (TODO :See an extended example from Part One)
* Due to economic incentives (and human laziness), the market may just *prefer* to make general AIs that are fully autonomous.

Sure, the social problems could "just" be addressed with AI Governance, and the technical problems could be addressed with many the many solutions on this page.

Still, I suspect *eventually* someone (some*thing?*) will make true AGI possible, and we should be prepared for that. But in the meantime, the above can help us prepare, and have great benefits. Using narrow bio-medical AI to "just" cure cancer ain't no small thing!

### ü§î Review #9

TODO

---

<a id="cyborg"></a>

## Cyborgism

Re: humans & possible future advanced AI,

**If we can't beat 'em, join 'em!**

We *could* interpret that literally: brain-computer interfaces[^bci] in the medium term, mind-uploading[^mu] in the long term. But we don't have to wait that long. The mythos of the "cyborg" can still be helpful, *right now!* In fact:

[^bci]: TODO

[^mu]: TODO

*YOU'RE ALREADY A CYBORG.*

...if "cyborg" means any human that's augmented their body or mind with technology. For example, you're *reading* this. Reading & writing *is* a technology. (Remember: things are still technologies even if they were created before you were born.) Literacy even *measurably re-wires your brain.*[^literacy] You are not a natural human: a few hundred years ago, most people couldn't read or write.

[^literacy]: TODO

Besides literacy, there's many other everyday cyborgisms:

* <u>Physical augmentations:</u> glasses, pacemakers, prosthetics, implants, hearing aids
* <u>Cognitive augmentations:</u> reading/writing, math notation, computers, spaced repetition flashcards
* <u><i>Emotional</i> augmentations!</u> diaries, meditation apps, reading biographies or watching documentaries to empathize with folks across the world.

![Stylish silhouette-drawing of people with "everyday cyborg" tools. Glitched-out caption reads: We're all already cyborgs.](../media/p3/cyborg/cyborg.png)

**Q:** That's... tool use. Do you really need a sci-fi word like "cyborg" to describe *tool use?*

**A:** Yes

Because if the question is: "how do we keep human *values* in the center of our systems?" Then one obvious answer is: keep *the human* in the center of our systems. Like that cool thing Sigourney Weaver uses in *Aliens (1986)*.

![Screenshot of Sigourney Weaver in the Power Loader. Caption: cyborgism, keeping the human in the center of our tools](../media/p3/cyborg/weaver.png)

Okay, enough metaphor, here's how Cyborgism has been applied to AI *specifically:*

* Garry Kasparov, the former World Chess Grandmaster, who also famously lost to IBM's chess-playing AI, once proposed: CENTAURS. It turned out, **human-AI teams could beat *both* the best humans & best AIs at chess**, by having the human's & AI's strengths/weaknesses compensate for each other![^cite-self] (This may or may not be true for chess specifically anymore[^gwern], but the general idea's still useful.)
* Likewise, some researchers are trying to combine the strengths/weaknesses of humans & large language models (LLMs).[^cyborgism] For example, humans are currently much better at long-term planning, LLMs are much better at high-variance brainstorming. Together, **a "cyborg" may be able to plan deeper *and* broader than pure-human or pure-LLM!**
  * (You can try this out *today!* janus made a tool called Loom, which lets you have a "multiverse" of thoughts. There's also an Obsidian plugin by Celeste!) // TODO links
* Large Language Models are "only" about as good as the average person at forecasting future events, but *together*, a normal LLM can help normal humans improve their forecasting ability by up to 41%![^llm-aug]
* Finally, check out this AI-augmented creative tool, made by [Zhu et al *in 2016*](https://arxiv.org/pdf/1609.03552). This demo came out long before DALL-E, and honestly it's *still* far better for precise artistic expression, vs the "write text and hope for the best" approach of DALL-E / MidJourney / Adobe Firefly / etc:

<video width="640" height="360" controls>
    <source src="../media/p3/cyborg/shoe.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

[^llm-aug]: TODO https://arxiv.org/pdf/2402.07862

[^cyborgism]: TODO link janus

[^cite-self]: // TODO cite own article

[^gwern]: TODO (Well, at the time. Gwern claims human-AI teams are strictly worse than pure-AI at chess now, but I couldn't find hard sources or data for that, in either direction. But anecdotally, it seems that human-AI centaurs are at least *on par* with pure-AI. Still, human-AI held its higher ground for a bit over a decade!) // TODO

. . .

Caveats & warnings:

* Humans may be too lazy, and opt for autonomous AIs, instead of augmenting their *own* autonomy. (Another reason to make this sound *cool* with "cyborg", not just "tool use".)
* When you put yourself inside a system, the system may modify *you*. Even for reading/writing, anthropologists agree that literacy isn't just a skill, it modifies your entire *culture and values*.[^anthropologists] What would becoming a *cyborg multiverse-thinker* do to you?
* Again, an AI-augmented human could be a sociopath, and bring about catastrophic risk. Again-again, *one* human's values ‚â† humane values.

[^anthropologists]: TODO

Then again...

![Close-up of Sigourney Weaver](../media/p3/cyborg/weaver2.png)

That's pretty cool.

. . .

**RECAP: How to deal with Problems with the Humans (in AI)...**

* Align the AI to a diverse range of human values/ethics. But allow it to self-reflect & change.
* AI Governance to keep us above the "safe" line: Trust but verify, sticks & carrots.
* Make tech that maximizes upsides while minimizing downsides: narrow non-agentic AI, and AI that *enhances, not replaces, humans*.

### ü§î Review #10

TODO

---

## In Sum:

Here's THE PROBLEM‚Ñ¢Ô∏è, broken down, with all the proposed solutions! (Click to see in full resolution! TODO)

`// TODO: less crappy-looking picture`

![TODO](../media/p3/SUM.png)

(Again, if you want to actually *remember* all this long-term, and not just be stuck with vague vibes two weeks from now, click the Table of Contents icon in the right sidebar, then click the "ü§î Review" links. Alternatively, download the [Anki deck for Part Three](TODO).)

. . .

*(EXTREMELY LONG INHALE)*

*(10 second pause)*

*(EXTREMELY LONG EXHALE)*

. . .

Aaaaand I'm done.

Around 80,000 words later (about the length of a novel), and nearly a hundred illustrations, that's... it. Over a year in the making, that's the end of my whirlwind guide to AI & AI Safety for Fleshy Humans.

If you've read all three parts in this series, you may have spent a few hours, but **you now know everything I've_ learnt in the last few years, which (in my opinion) are the most important ideas of the last few decades!** 

üéâ Pat yourself on the back!  (But mostly pat *my* back. (I'm so tired.))

Sure, the field of AI Safety is moving so fast, Part One & Two started to become obsolete before Part Three came out, and doubtless Part Three: The Proposed Solutions will feel na√Øve or obvious in a couple years.

But hey, the real AI Safety was all the friends we made along the way.

Um.

I need a better way to wrap up this series.

Uh, here, click this for a really cool **CINEMATIC CONCLUSION:**

// TODO BUTTON

. 

.

. 

.

.

. 

.

. 

.

wait what are you doing? scroll back up, the cool ending's in the button up there.

come on, it's just a boring footer & footnotes below.

. 

.

. 

.

.

.

.

.

.

.

ugh, fine: