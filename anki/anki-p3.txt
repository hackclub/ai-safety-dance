"<p>We don't need One Perfect Solution, we can...</p>";"<p>...stack several imperfect solutions! (Swiss Cheese Model)</p><img src='aisffs-cheese.png'>"
"<p>What is Scalable Oversight?</p>";"<p>Instead of trying to oversee an AI much more capable than you, you oversee a slightly-more capable AI, which oversees a slightly-more capable AI, repeat!</p><img src='aisffs-so.png'>"
"<p>How can you make scalable oversight more robust?</p>";"<p>By adding parallel chains of overseers (whose failure modes are as <em>uncorrelated</em> as possible)</p><img src='aisffs-robust-chains.png'>"
"<p>Scalable Oversight lets us convert the impossible question, “How do you oversee a thing that's 1000x smarter than you?” to the more feasible question...</p>";"<p>“How do you oversee a thing that's only a bit smarter than you, <em>and</em> you can train it from scratch, read its mind, and nudge its thinking?” (you don't have to remember this exactly, just the gist of it)</p>"
"<p>Name at least 2 specific implementations of Scalable Oversight:</p>";"<p>(any 2 of the following work:) Recursive Reward Modeling, Prover-Verifier Games, Weak-to-Strong Generalization, Debate, Superfiltering, Iterated Distillation &amp; Amplification</p>"
"<p>The pattern in AI Safety <s>paranoia</s> Security Mindset:</p>";"<p>Step 1: Imagine giving an AI an innocent-seeming goal. Step 2: Think of a bad way it could <em>technically</em> achieve that goal.</p>"
"<p>The 'Future Lives' approach is like having an AI apply _____ against itself</p>";"<p><strong>Security Mindset</strong> (“Think of the worst that could happen, in advance. Then fix it.”)</p>"
"<p>The 'Future Lives' algorithm, in sum:</p>";"<p>1] Robot predicts the future results of possible actions, and how <em>current you</em> would react to <em>those futures</em>. 2] Robot picks the action whose future you'd most approve of, and <em>not</em> the futures that current-you would disapprove of.</p>"
"<p>The 'Future Lives' approach, as described by Stuart Russell: (you don't need to remember the quote exactly, just paraphrase it)</p>";"<p>[Imagine] if you were somehow able to <strong>watch two movies</strong>, each describing in sufficient detail and breadth <strong>a future life</strong> you might lead [as well as the consequences outside of &amp; after your life.] <strong>You could say which you prefer, or express indifference.</strong></p>"
"<p>What does 'indirect normativity' mean?</p>";"<p>An approach where, instead of directly specifying our values, we specify <em>how to learn</em> our values.</p>"
"<p>Why may we NOT need a perfect alignment algorithm to start with?</p>";"<p>Like recursive self-improvement in AI Capabilities, above a certain <strong>'critical mass'</strong>, the alignment algorithm <em>can improve itself</em> to fix its flaws. (Let it go meta!)</p>"
"<p>One limitation of the 'self-improving AI Alignment' idea</p>";"<p>It requires risky levels of AI Capabilities to work in the first place. (so, this needs to be stacked with other security measures)</p>"
"<p>A solution to the problem of AIs being 100% sure about your values:</p>";"<p>Make AIs <em>know they don't know</em> your true values. (and so, learn &amp; serve them cautiously)</p>"
"<p>Three steps to learning your values &amp; acting on them safely:</p>";"<p>1] Start with a good-enough 'prior'. 2] Everything Human says/does is a <em>clue</em> to true values, not 100% ground truth. 3] Maximize worst(-ish) case</p>"
"<p>Three main ways to maximize uncertain value</p>";"<p>Maximize average-case (most common), worst-case (safest), or best-case (riskiest). (extra note: it's also possible to do any in-between, like 'maximize the 5%th-worst percentile'.)</p>"
"<p>An example that shows why 'aim at a goal <em>you know you don't know</em>' is not that mysterious:</p>";"<p>(Either example works:) 1] The game of Battleship (goal is to hit ships of unknown position). 2] Wanting to get a good gift for your friend, but not knowing what they'd want.</p>"
"<p>Name one specific technique for “learning a human's values”</p>";"<p>(Any of the following work:) Inverse reinforcement learning (IRL), Cooperative inverse reinforcement learning (CIRL), Reinforcement Learning from Human Feedback (RLHF)</p>"
"<p>Do we sidestep the 'specification' problem by using the 'learn our values' approach?</p>";"<p>NO: we still have to specify <em>how an AI should learn</em> one's values. (Which is tricky, but much easier than rigorously listing out one's full subconscious desires. It's like how it takes years to teach French, but 'only' hours to teach <em>how to teach oneself</em> French.)</p>"
"<p><strong>Value learning + Uncertainty + Future Lives:</strong>, in three steps:</p>";"<p>1️⃣ Learn our values 2️⃣ But, be uncertain &amp; <em>know that you don't 100% know our values</em>. 3️⃣ Then, choose actions that lead to <em>future</em> lives than <em>current</em> us would approve of.</p>"
"<p>How 'learn our values' makes the AI Capabilities vs Alignment graph more optimistic</p>";"<p>It <em>ties</em> an AI's alignment to its capabilities: the better it is at 'normal machine learning' in general, the better it will be at learning our values!</p><img src='aisffs-floor.png'>"
"<p>'Interpretability' in AI is like... (analogy in humans)</p>";"<p>running a brain scan on a human, to figure out their thoughts &amp; feelings.</p>"
"<p>'steering' in AI is like... (analogy in humans)</p>";"<p>using magnets or ultrasound on a human's brain, to <em>change</em> their thoughts &amp; feelings. (and yes, scientists have done this)</p>"
"<p>Name at least <em>three</em> AI Interpretability techniques:</p>";"<p>(any 3 of the following work) Feature visualization, Probes, Sparse Auto-Encoders, Sparse Crosscoders, Jacobian SAEs, Black-box detectors for lying &amp; hallucination, Monitoring chain of thought, Steering Vectors</p>"
"<p>Roughly speaking, how do you do Feature Visualization (figuring out what a neuron 'does')?</p>";"<p>Generate an input that maximizes the activation of that neuron.</p><img src='aisffs-feature-viz.png'>"
"<p>Roughly speaking, how does a Probing Classifier work?</p>";"<p>Train a <em>simple</em> AI, to understand a single layer of a <em>complex</em> AI. <em>(Yo dawg, I heard you like AIs, so I trained an AI on your AI, so you can predict your predictor.)</em></p><img src='aisffs-probe.png'>"
"<p>Roughly speaking, what do Auto-encoders do?</p>";"<p>An 'auto-encoder' compresses a big thing, into a small thing, then converts it back to the <em>same</em> big thing. This allows an AI to learn the 'essence' of a thing, by squeezing inputs through a small bottleneck. (and the bottleneck can be: fewer neurons, fewer meanings, simpler computations, etc)</p><img src='aisffs-autoencoder.png'>"
"<p>Name a black-box method for interpreting AIs:</p>";"<p>(either works:) 1] Figure out if an LLM is lying by asking it a set of 'nonsensical' questions. 2] Figure out if an LLM is hallucinating by asking it the same question over and over. Truth is consistent, hallucinations aren't.</p>"
"<p>'Deliberative Alignment' is:</p>";"<p>getting an AI to be more aligned to its policy, by simply prompting it to remember its policy &amp; reason about it.</p>"
"<p>'Chain-of-thought monitoring' is:</p>";"<p>Monitoring, well, an LLM's chain-of-thought (reasoning) to make sure it's not being suspicious or harmful.</p>"
"<p>How do you create &amp; apply a Steering Vector?</p>";"<p>Create: take the <em>difference</em> between multiple examples of the ANN in state X vs not-X // Apply: add this difference <em>back</em> to an ANN at run-time.</p><img src='aisffs-steering.png'>"
"<p>Name 1 example of real-life Steering Vectors found for AI Safety:</p>";"<p>(any of the following works:) Love-Hate, Honesty, Power-seeking, Fairness, Sycophancy, Corrigibility, Self-Preservation, Jailbreaking.</p>"
"<p>Example of a funny &amp; weirdly specific steering vector found in the Claude LLM</p>";"<p>The 'Golden Gate' vector, that forces Claude to think about the Golden Gate Bridge in San Francisco</p><img src='aisffs-bridge.png'>"
"<p>THE THREE KEYS TO ROBUSTNESS</p>";"<p><strong>SIMPLICITY / DIVERSITY / ADVERSITY</strong></p><img src='aisffs-robust.png'>"
"<p>In the chain metaphor, how does <strong>Simplicity</strong> reduce the chance of failure?</p>";"<p>If a single link breaks in a chain, the whole chain breaks. Therefore, <em>minimize the number of necessary links in any chain.</em></p>"
"<p>In the chain metaphor, how does <strong>Diversity</strong> reduce the chance of failure?</p>";"<p>If one chain breaks, it's good to have 'redundant' backups. Therefore, <em>maximize the number of independent chains</em>. (note: the chains should be as different/independent from each other as possible, to lower the correlation between their failures.)</p>"
"<p>In the chain metaphor, how does <strong>Adversity</strong> reduce the chance of failure?</p>";"<p>Hunt down the weakest links, the weakest chains. Strengthen them, or replace them with something stronger.</p>"
"<p>Name at least 1 way to do <strong>Simplicity</strong> for Robust AI:</p>";"<p>(any of the following works) Regularization, Impact regularization, Auto-encoders (bottleneck for 'essence'), Speed/Simplicity Prior for Honest AI</p>"
"<p>Name at least 1 way to do <strong>Diversity</strong> for Robust AI:</p>";"<p>(any of the following works) Ensembles, Dropout, Data Augmentation, Diverse Data, Kalman filters, Shard theory</p>"
"<p>Name at least 1 way to do <strong>Adversity</strong> for Robust AI:</p>";"<p>(any of the following works) Adversarial Training, Relaxed / Latent Adversarial Training, Red Teams, Best Worst-Case Performance</p>"
"<p>List at least 1 surprising thing <strong>LLMs are great at</strong> (as of December 2025)</p>";"<p>(any of the following works:) International Math Olympiad, passed the Turing Test, humans <em>prefer AI over humans</em> in poetry, therapy &amp; short fiction</p>"
"<p>List at least 1 surprising thing <strong>LLMs still suck at</strong> (as of December 2025)</p>";"<p>(any of the following works:) running the business of a vending machine, can't play Pokémon Red, can't solve simple 'rule-discovery' games, can't generalize on Tower of Hanoi or other classic puzzles.</p>"
"<p>As of 2025, how do the frontier LLMs perform on the Tower of Hanoi problem?</p>";"<p>Really well, up to 7 disks... then <em>completely collapses</em> at 8+ disks. (This is like a human being able to add two 7-digit numbers on pen &amp; paper, then <em>utterly bomb</em> on two 8-digit numbers. Very strange.)</p><img src='aisffs-hanoi.png'>"
"<p>Modern AIs think in ____. They do not think in ____.</p>";"<p>They 'think in vibes'. They do not 'think in gears'. (To be precise: they do not form &amp; use <em>robust mental models</em>.)</p>"
"<p>Good Ol' Fashioned AI (GOFAI): _____ but _____ Modern AI: _____, but _____.</p>";"<p>Good Ol' Fashioned AI (GOFAI): Robust but inflexible. Modern AI: Flexible, but not robust. <em>(note: as of Dec 2025, we don't know how to make an AI that can do both: flexible discover &amp; use robust mental models.)</em></p>"
"<p>According to the Embers of Autoregression paper, what are 3 factors that predict when an LLM will rock/suck at a task?</p>";"<p>The probability of 1] the task, 2] the target output, and 3] the provided input. (Where 'probability' ~= 'how common was it in the training data'.)</p>"
"<p>What's 1 task LLMs are getting exponentially better at, and 1 where they're exponentially bad at?</p>";"<p>Common coding tasks (length of task doubling every 7 months), vs the ARC-AGI 'rule-discovery' games (cost scales worse-than-exponentially)</p><img src='aisffs-arc-agi-cost.png'>"
"<p>Name at least 1 research direction that's (trying) to make AI both be flexible <em>and</em> have robust mental models:</p>";"<p>(any of the following works:) Neuro-Symbolic AIs, Hybrid AIs, ANNs that infer causal diagrams, code interpreters, program search &amp; program synthesis, Theory-Based Reinforcement Learning, Free energy principle</p>"
"<p>Name at least 2 benefits from having AI being able to think in cause-and-effect gears: (other than better capabilties)</p>";"<p>(any 2 of the following works:) Interpretability &amp; Steering, Trustworthiness &amp; Accountability, Robustness, Learning our Values, The 'Future Lives' Algorithm, Getting the truth not just a human-imitator, Non-agentic Scientist AIs, Causal Incentives</p>"
"<p>How does Constitutional AI work?</p>";"<p>1] Human creates a list of principles. 2] Teacher-bot uses this list to train a student-bot. (This is how you can get millions of training data-points, from a small human-made list, which can also be democratically crowdsourced.)</p>"
"<p>What is Moral Parliament?</p>";"<p>A 'parliament' of moral theories, with more seats for theories you're more confident in. The parliament votes on decisions.</p>"
"<p>One way to distill human judgment using AI (not limited to LLMs specifically)</p>";"<p>1] ask human jury to answer lots of questions. 2] people can submit AIs to predict their answers. 3] use an ensemble (for robustness) of the best AIs as an Oracle for “does this fit humanity's values”</p>"
"<p>What does 'Coherent Extrapolated Volition' (CEV) mean?</p>";"<p>What wishes (Volition) we'd have in common (Coherent) if we had a lot of time to reflect &amp; discuss with each other (Extrapolated)</p>"
"<p>What does 'Coherent <em>Blended</em> Volition' (CBV) mean?</p>";"<p>Same as CEV, except that the AIs <em>actually help us reflect &amp; grow together</em>, instead of just simulating us doing that. (Success case: Taiwan's digital “citizen's assembly”, when Uber wanted to enter Taiwan)</p>"
"<p>All our work on the AI alignment problem means nothing if we can't solve...</p>";"<p>...the <em>human</em> alignment problem: how do we get fallible fleshy humans to _actually coordinate on safe, humane AI?</p>"
"<p>The 2 core pillars of AI Governance:</p>";"<p>1] Verify where we are. 2] Stay above the 'safe' line.</p><img src='aisffs-gov-rocket.png'>"
"<p>Name at least 1 way to 'verify where we are' in AI Governance:</p>";"<p>(any of the following work:) evaluations (ideally crowdsourced), protect whistleblowers, enforce transparency &amp; standards on AI labs, track chips &amp; compute, forecasting (maybe AI-aided)</p>"
"<p>Responsible Scaling Policy, in a nutshell:</p>";"<p>'We commit to not even <em>start</em> training an AI of Level N, until we have safeguards and evaluations <em>up to Level N+1</em>.'</p>"
"<p>Differential Technological Development in a nutshell:</p>";"<p>Invest in tech/research that boosts Alignment <em>more than</em> Capabilities. (or at least Safe Capabilities <em>more than</em> Risky Capabilities)</p>"
"<p>Name at least 1 way to 'stay above the safe line':</p>";"<p>(any of the following work:) Responsible Scaling Policy, Differentially invest in Alignment &gt; Capabilities, Tech to fight catastrophic risk, Data filtering/machine unlearning so that AI doesn't know how to make bombs/superviruses, AI that enhances not replaces humans ('Cyborgism').</p>"
"<p>If you want your AI Alignment &amp; Governance ideas to be <em>actually implemented in real life, not just in academic papers</em>...</p>";"<p>You need your ideas to “pay dividends” right now, in the short term. (“Nobody cares about the bomb that didn't go off.”)</p>"
"<p>Name at least 1 time humanity successfully coordinated to solve a global problem:</p>";"<p>(any of the following works:) We eradicated smallpox, it's no longer the case that <em>half</em> of kids died before age 15, the ozone layer <em>is</em> actually healing!</p>"
"<p>Name at least 1 way to get the benefits of AI without making a powerful, general, autonomous AI</p>";"<p>(Any of the following works:) Comprehensive AI Services, Tool AIs, Pure Scientist AIs, Microscope AIs, Quantilizers, Cyborgism</p>"
"<p>What's a 'Scientist AI'?</p>";"<p>An AI that “just” takes in observations, and spits out theories. (The same way that Google Translate can take English as input, and give Mandarin as output.)</p>"
"<p>What's Microscope AI?</p>";"<p>Instead of making an AI whose scientific findings are its <em>output</em>, we train an AI to predict real-world data, then look <em>at the neural connections themselves</em> to learn about that data! (using Interpretability techniques)</p>"
"<p>What's a 'Quantilizer'?</p>";"<p>Instead of making an AI that <em>optimizes</em> for a goal, make an AI that is trained to <em>imitate a (smart) human</em>. Then to solve a problem, run this human-imitator e.g. 20 times, and pick the best solution. This will be equivalent to getting a smart human on the best 5% of their days.</p>"
"<p>“We're all already cyborgs”. Give 3 examples?</p>";"<p>(any 3 of the following works): <strong>Physical augmentations:</strong> glasses, pacemakers, prosthetics, implants, hearing aids, continuous glucose monitors. <strong>Cognitive augmentations:</strong> reading/writing, math notation, computers, spaced repetition flashcards. <strong>Emotional augmentations:</strong> diaries, meditation apps, biographies &amp; documentaries as empathy-enhancers</p><img src='aisffs-cyborg.png'>"
"<p>Cyborgism: “keeping the _____ in the _____”</p>";"<p>keeping the human in the center of our tools</p>"
"<p>Think of 1 thing that Humans are relatively better at than LLMs, and vice versa:</p>";"<p>(any of the following in this puzzle-piece diagram:)</p><img src='aisffs-cyborg-puzzle.png'>"
"<p>Name a real-world example where Human+AI team <em>proved</em> to better than Human or AI alone</p>";"<p>Chess, Go, Forecasting future events. (although 'cyborgs/centaurs' in chess stopped having an advantage since 2017 -- there's a time limit to the 'sweet spot' for cyborgs!)</p>"
"<p>Visualize the sweet spot for Cyborgism, as 3 Venn diagrams:</p>";"<img src='aisffs-cyborg-venn.png'>"